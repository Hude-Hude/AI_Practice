---
title: "Solving a Dynamic Discrete Choice Model"
subtitle: "MDP with Type-I Extreme Value Shocks"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Model Setup

We consider a dynamic discrete choice model with:

- **State**: $s \in \mathbb{R}_{\geq 0}$ (continuous, non-negative)
- **Action**: $a \in \{0, 1\}$ (binary choice)
- **Mean reward**: $u(s, a) = \beta \log(1 + s) - a$
- **State transition**: $s' = (1 - \gamma) s + a$ (deterministic)
- **Discount factor**: $\delta$
- **Shocks**: $\varepsilon_a \sim$ Type-I Extreme Value (iid across actions and time)

### Interpretation

- Higher state $s$ yields higher flow payoff, but with **diminishing marginal returns**
- The $\log(1 + s)$ specification ensures:
  - Marginal utility $\frac{\partial u}{\partial s} = \frac{\beta}{1 + s} > 0$ (increasing in $s$)
  - Diminishing: $\frac{\partial^2 u}{\partial s^2} = -\frac{\beta}{(1 + s)^2} < 0$ (concave)
  - Well-defined at $s = 0$: $u(0, a) = -a$
- Action $a = 1$ costs 1 unit but boosts next period's state by 1
- Action $a = 0$ is free but state decays toward zero at rate $\gamma$

## Optimal Markov Decision Policy

### Bellman Equation with Extreme Value Shocks

Each period, the agent observes state $s$ and receives action-specific shocks $(\varepsilon_0, \varepsilon_1)$ drawn iid from the Type-I Extreme Value (Gumbel) distribution:
$$
F(\varepsilon) = \exp(-\exp(-\varepsilon)), \quad f(\varepsilon) = \exp(-\varepsilon - \exp(-\varepsilon))
$$

The agent chooses action $a \in \{0, 1\}$ to maximize:
$$
u(s, a) + \varepsilon_a + \delta V(s')
$$
where $s' = (1 - \gamma)s + a$ is the next state.

The **Bellman equation** is:
$$
V(s, \varepsilon_0, \varepsilon_1) = \max_{a \in \{0,1\}} \left\{ u(s, a) + \varepsilon_a + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon_0', \varepsilon_1') \right] \right\}
$$

### Choice-Specific Value Functions

Define the **choice-specific value function** (the deterministic part, excluding the current shock):
$$
v(s, a) = u(s, a) + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon') \right]
$$

Substituting the reward and transition:
$$
v(s, a) = \beta \log(1 + s) - a + \delta \bar{V}\bigl((1 - \gamma) s + a\bigr)
$$
where $\bar{V}(s) = \mathbb{E}_\varepsilon[V(s, \varepsilon)]$ is the **integrated (ex-ante) value function**.

### Integrated Value Function

The **integrated value function** is the expected value before observing shocks:
$$
\bar{V}(s) = \mathbb{E}_{\varepsilon_0, \varepsilon_1} \left[ \max_{a \in \{0,1\}} \left\{ v(s, a) + \varepsilon_a \right\} \right]
$$

With Type-I EV shocks, this has a closed-form (the "log-sum-exp" formula):
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right) + \gamma_E
$$
where $\gamma_E \approx 0.5772$ is Euler's constant. Since $\gamma_E$ is a constant, it does not affect the policy and is often omitted.

### Optimal Policy and Choice Probabilities

The optimal decision rule is:
$$
a^*(s, \varepsilon_0, \varepsilon_1) = \mathbf{1}\left\{ v(s, 1) + \varepsilon_1 > v(s, 0) + \varepsilon_0 \right\}
$$

The probability of choosing action $a = 1$ given state $s$ follows the **logit formula**:
$$
P(a = 1 | s) = \frac{\exp(v(s, 1))}{\exp(v(s, 0)) + \exp(v(s, 1))} = \frac{1}{1 + \exp(v(s, 0) - v(s, 1))}
$$

The optimal policy is a **stochastic Markov policy**: the choice probability depends only on the current state $s$ through the value difference $v(s, 1) - v(s, 0)$.

## Solution Method: Neural Network Value Iteration

### Bellman Fixed Point

The integrated value function $\bar{V}(s)$ satisfies the **fixed point equation**:
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right)
$$
where each choice-specific value depends on $\bar{V}$:
$$
v(s, a) = \beta \log(1 + s) - a + \delta \bar{V}((1 - \gamma)s + a)
$$

By Blackwell's sufficient conditions, this defines a contraction mapping with modulus $\delta < 1$, guaranteeing a unique fixed point $\bar{V}^*$.

### Monotonic Neural Network Architecture

We approximate each action's choice-specific value function $v(s, a)$ using a **monotonic neural network**. Since higher states yield higher rewards ($\beta > 0$), we expect $v(s, a)$ to be increasing in $s$. Enforcing monotonicity as an architectural constraint improves extrapolation and ensures economically sensible behavior.

A feed-forward neural network is **monotonically increasing** in its input if:

1. All weights are **non-negative**
2. All activation functions are **monotonically increasing**

For a network with $L$ hidden layers:
$$
v_\theta(s) = W_L \cdot \sigma\left( W_{L-1} \cdot \sigma\left( \cdots \sigma(W_1 \cdot s + b_1) \cdots \right) + b_{L-1} \right) + b_L
$$
where $W_\ell \geq 0$ for all layers and $\sigma$ is a monotonically increasing activation function.

### Softplus Activation

We use the **softplus** activation function:
$$
\sigma(x) = \log(1 + e^x)
$$

Properties:

- **Monotonically increasing**: $\sigma'(x) = \frac{e^x}{1 + e^x} = \text{sigmoid}(x) > 0$
- **Smooth approximation to ReLU**: $\sigma(x) \approx \max(0, x)$ for large $|x|$
- **Differentiable everywhere** (unlike ReLU)

### Weight Parameterization

To enforce non-negative weights during optimization, we parameterize the weights using softplus of unconstrained parameters:
$$
W = \text{softplus}(\tilde{W}) = \log(1 + e^{\tilde{W}})
$$
where $\tilde{W} \in \mathbb{R}$ is the learnable parameter. This ensures $W > 0$ while allowing standard gradient-based optimization.

### Value Function Approximator

We use **separate monotonic networks** for each action:

- $v_{\theta_0}(s)$: Monotonic network for action $a = 0$
- $v_{\theta_1}(s)$: Monotonic network for action $a = 1$

The integrated value function is computed as:
$$
\bar{V}_\theta(s) = \log\left( \exp(v_{\theta_0}(s)) + \exp(v_{\theta_1}(s)) \right)
$$

Choice probabilities follow the logit formula:
$$
P_\theta(a = 1 | s) = \frac{1}{1 + \exp(v_{\theta_0}(s) - v_{\theta_1}(s))}
$$

### Training Objective

The networks are trained to satisfy the Bellman equation by minimizing the **Bellman residual**:
$$
\mathcal{L}(\theta) = \mathbb{E}_s\left[ \sum_{a \in \{0,1\}} \left( v_{\theta_a}(s) - u(s, a) - \delta \bar{V}_\theta(s'_a) \right)^2 \right]
$$
where $s'_a = (1 - \gamma)s + a$ is the next state under action $a$.

### Type Definitions

```
TYPE DEFINITIONS
────────────────
Scalar      = Float                      # Single real number
Vector[n]   = Array[Float, n]            # 1D array of n floats
Matrix[m,n] = Array[Float, m, n]         # 2D array of shape (m, n)
Action      = Int ∈ {0, 1}               # Binary action
Network     = MonotonicNeuralNetwork     # Neural network with non-negative weights
```

### Algorithm

```
ALGORITHM: SOLVE_VALUE_FUNCTION
───────────────────────────────
INPUT:
  β            : Scalar              # Reward coefficient
  γ            : Scalar              # State decay rate
  δ            : Scalar              # Discount factor
  s_min        : Scalar              # Minimum state
  s_max        : Scalar              # Maximum state
  hidden_sizes : List[Int]           # Hidden layer sizes
  α            : Scalar              # Learning rate
  N            : Int                 # Batch size
  ε            : Scalar              # Convergence tolerance
  K            : Int                 # Maximum iterations

OUTPUT:
  v₀_net       : Network             # Trained network for action 0
  v₁_net       : Network             # Trained network for action 1

PROCEDURE:
  v₀_net : Network, v₁_net : Network ← INITIALIZE_NETWORKS(hidden_sizes)
  
  FOR k : Int = 1 TO K:
    s_batch : Vector[N] ← SAMPLE_STATES(N, s_min, s_max)
    
    v₀_pred : Vector[N] ← EVALUATE_NETWORK(v₀_net, s_batch)
    v₁_pred : Vector[N] ← EVALUATE_NETWORK(v₁_net, s_batch)
    
    target₀ : Vector[N], target₁ : Vector[N] ← COMPUTE_BELLMAN_TARGETS(
      v₀_net, v₁_net, s_batch, β, γ, δ, s_min, s_max
    )
    
    loss : Scalar ← COMPUTE_BELLMAN_LOSS(v₀_pred, v₁_pred, target₀, target₁)
    
    UPDATE_NETWORK_WEIGHTS(v₀_net, v₁_net, loss, α)
    
    IF CHECK_CONVERGENCE(loss, ε):
      RETURN (v₀_net, v₁_net)
  
  RETURN (v₀_net, v₁_net)
```

### Subroutines

```
SUBROUTINE: INITIALIZE_NETWORKS
───────────────────────────────
INPUT:
  hidden_sizes : List[Int]           # Hidden layer sizes
OUTPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1

  v₀_net : Network ← BUILD_MONOTONIC_NETWORK(hidden_sizes)
  v₁_net : Network ← BUILD_MONOTONIC_NETWORK(hidden_sizes)
  RETURN (v₀_net, v₁_net)
```

```
SUBROUTINE: BUILD_MONOTONIC_NETWORK
───────────────────────────────────
INPUT:
  hidden_sizes : List[Int]           # Hidden layer sizes
OUTPUT:
  network : Network                  # Monotonic neural network

  layers : List[Layer] ← empty list
  in_features : Int ← 1
  
  FOR i : Int = 0 TO length(hidden_sizes) - 1:
    out_features : Int ← hidden_sizes[i]
    
    # Create monotonic linear layer
    W_raw : Matrix[out_features, in_features] ← random_normal(0, 0.1)
    b : Vector[out_features] ← zeros(out_features)
    layers.append(MonotonicLinear(W_raw, b))
    
    # Add softplus activation
    layers.append(SoftplusActivation())
    
    in_features ← out_features
  
  # Output layer (no activation)
  W_raw_out : Matrix[1, in_features] ← random_normal(0, 0.1)
  b_out : Vector[1] ← zeros(1)
  layers.append(MonotonicLinear(W_raw_out, b_out))
  
  RETURN Network(layers)
```

```
SUBROUTINE: APPLY_MONOTONIC_LINEAR
──────────────────────────────────
INPUT:
  W_raw : Matrix[m, n]               # Unconstrained weight parameters
  b     : Vector[m]                  # Bias vector
  x     : Vector[n]                  # Input vector
OUTPUT:
  y     : Vector[m]                  # Output vector

  # Enforce non-negative weights via softplus
  W : Matrix[m, n] ← log(1 + exp(W_raw))
  
  # Linear transformation
  y : Vector[m] ← W @ x + b
  RETURN y
```

```
SUBROUTINE: APPLY_SOFTPLUS
──────────────────────────
INPUT:
  x : Vector[n]                      # Input vector
OUTPUT:
  y : Vector[n]                      # Output vector

  FOR i : Int = 0 TO n - 1:
    y[i] : Scalar ← log(1 + exp(x[i]))
  RETURN y
```

```
SUBROUTINE: EVALUATE_NETWORK
────────────────────────────
INPUT:
  network : Network                  # Monotonic neural network
  s       : Vector[N]                # Input states
OUTPUT:
  values  : Vector[N]                # Output values

  x : Matrix[N, 1] ← reshape(s, (N, 1))
  
  FOR layer IN network.layers:
    IF layer IS MonotonicLinear:
      x ← APPLY_MONOTONIC_LINEAR(layer.W_raw, layer.b, x)
    ELSE IF layer IS SoftplusActivation:
      x ← APPLY_SOFTPLUS(x)
  
  values : Vector[N] ← reshape(x, (N,))
  RETURN values
```

```
SUBROUTINE: SAMPLE_STATES
─────────────────────────
INPUT:
  N     : Int                        # Number of samples
  s_min : Scalar                     # Minimum state
  s_max : Scalar                     # Maximum state
OUTPUT:
  s     : Vector[N]                  # Sampled states

  FOR i : Int = 0 TO N - 1:
    u : Scalar ← random_uniform(0, 1)
    s[i] : Scalar ← s_min + u * (s_max - s_min)
  RETURN s
```

```
SUBROUTINE: COMPUTE_REWARD
──────────────────────────
INPUT:
  s      : Vector[N]                 # States
  action : Action                    # Action (0 or 1)
  β      : Scalar                    # Reward coefficient
OUTPUT:
  r      : Vector[N]                 # Rewards

  FOR i : Int = 0 TO N - 1:
    r[i] : Scalar ← β * log(1 + s[i]) - action
  RETURN r
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
──────────────────────────────
INPUT:
  s      : Vector[N]                 # Current states
  action : Action                    # Action (0 or 1)
  γ      : Scalar                    # Decay rate
OUTPUT:
  s_next : Vector[N]                 # Next states

  FOR i : Int = 0 TO N - 1:
    s_next[i] : Scalar ← (1 - γ) * s[i] + action
  RETURN s_next
```

```
SUBROUTINE: CLAMP_STATES
────────────────────────
INPUT:
  s     : Vector[N]                  # States
  s_min : Scalar                     # Minimum bound
  s_max : Scalar                     # Maximum bound
OUTPUT:
  s_clamped : Vector[N]              # Clamped states

  FOR i : Int = 0 TO N - 1:
    IF s[i] < s_min:
      s_clamped[i] : Scalar ← s_min
    ELSE IF s[i] > s_max:
      s_clamped[i] : Scalar ← s_max
    ELSE:
      s_clamped[i] : Scalar ← s[i]
  RETURN s_clamped
```

```
SUBROUTINE: COMPUTE_INTEGRATED_VALUE
────────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # States
OUTPUT:
  V_bar  : Vector[N]                 # Integrated value

  v₀ : Vector[N] ← EVALUATE_NETWORK(v₀_net, s)
  v₁ : Vector[N] ← EVALUATE_NETWORK(v₁_net, s)
  
  FOR i : Int = 0 TO N - 1:
    v_max : Scalar ← max(v₀[i], v₁[i])
    V_bar[i] : Scalar ← v_max + log(exp(v₀[i] - v_max) + exp(v₁[i] - v_max))
  RETURN V_bar
```

```
SUBROUTINE: COMPUTE_BELLMAN_TARGETS
───────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # Current states
  β      : Scalar                    # Reward coefficient
  γ      : Scalar                    # Decay rate
  δ      : Scalar                    # Discount factor
  s_min  : Scalar                    # Minimum state bound
  s_max  : Scalar                    # Maximum state bound
OUTPUT:
  target₀ : Vector[N]                # Target for action 0
  target₁ : Vector[N]                # Target for action 1

  # Compute targets for action 0
  r₀ : Vector[N] ← COMPUTE_REWARD(s, action=0, β)
  s_next₀ : Vector[N] ← COMPUTE_NEXT_STATE(s, action=0, γ)
  s_next₀ ← CLAMP_STATES(s_next₀, s_min, s_max)
  V_next₀ : Vector[N] ← COMPUTE_INTEGRATED_VALUE(v₀_net, v₁_net, s_next₀)
  
  FOR i : Int = 0 TO N - 1:
    target₀[i] : Scalar ← r₀[i] + δ * V_next₀[i]
  
  # Compute targets for action 1
  r₁ : Vector[N] ← COMPUTE_REWARD(s, action=1, β)
  s_next₁ : Vector[N] ← COMPUTE_NEXT_STATE(s, action=1, γ)
  s_next₁ ← CLAMP_STATES(s_next₁, s_min, s_max)
  V_next₁ : Vector[N] ← COMPUTE_INTEGRATED_VALUE(v₀_net, v₁_net, s_next₁)
  
  FOR i : Int = 0 TO N - 1:
    target₁[i] : Scalar ← r₁[i] + δ * V_next₁[i]
  
  RETURN (target₀, target₁)
```

```
SUBROUTINE: COMPUTE_BELLMAN_LOSS
────────────────────────────────
INPUT:
  v₀_pred : Vector[N]                # Predicted values for action 0
  v₁_pred : Vector[N]                # Predicted values for action 1
  target₀ : Vector[N]                # Target values for action 0
  target₁ : Vector[N]                # Target values for action 1
OUTPUT:
  loss    : Scalar                   # Mean squared Bellman residual

  sum_sq : Scalar ← 0.0
  
  FOR i : Int = 0 TO N - 1:
    error₀ : Scalar ← v₀_pred[i] - target₀[i]
    error₁ : Scalar ← v₁_pred[i] - target₁[i]
    sum_sq ← sum_sq + error₀ * error₀ + error₁ * error₁
  
  loss : Scalar ← sum_sq / (2 * N)
  RETURN loss
```

```
SUBROUTINE: UPDATE_NETWORK_WEIGHTS
──────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  loss   : Scalar                    # Loss value
  α      : Scalar                    # Learning rate
OUTPUT:
  (none, updates networks in place)

  # Compute gradients via backpropagation
  grad₀ : List[Matrix] ← BACKPROPAGATE(v₀_net, loss)
  grad₁ : List[Matrix] ← BACKPROPAGATE(v₁_net, loss)
  
  # Update weights for network 0
  FOR j : Int = 0 TO length(v₀_net.layers) - 1:
    IF v₀_net.layers[j] IS MonotonicLinear:
      v₀_net.layers[j].W_raw ← v₀_net.layers[j].W_raw - α * grad₀[j].W
      v₀_net.layers[j].b ← v₀_net.layers[j].b - α * grad₀[j].b
  
  # Update weights for network 1
  FOR j : Int = 0 TO length(v₁_net.layers) - 1:
    IF v₁_net.layers[j] IS MonotonicLinear:
      v₁_net.layers[j].W_raw ← v₁_net.layers[j].W_raw - α * grad₁[j].W
      v₁_net.layers[j].b ← v₁_net.layers[j].b - α * grad₁[j].b
```

```
SUBROUTINE: CHECK_CONVERGENCE
─────────────────────────────
INPUT:
  loss : Scalar                      # Current loss
  ε    : Scalar                      # Tolerance
OUTPUT:
  converged : Bool                   # True if converged

  rmse : Scalar ← sqrt(loss)
  converged : Bool ← (rmse < ε)
  RETURN converged
```

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
──────────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # States
OUTPUT:
  prob   : Vector[N]                 # P(a=1|s)

  v₀ : Vector[N] ← EVALUATE_NETWORK(v₀_net, s)
  v₁ : Vector[N] ← EVALUATE_NETWORK(v₁_net, s)
  
  FOR i : Int = 0 TO N - 1:
    diff : Scalar ← v₁[i] - v₀[i]
    prob[i] : Scalar ← 1 / (1 + exp(-diff))
  RETURN prob
```

### Convergence

The algorithm minimizes the Bellman residual, driving the neural network approximation toward the true fixed point. Convergence is monitored via the root mean squared Bellman error $\sqrt{\mathcal{L}^{(k)}}$.

Key properties:

- **Continuous approximation**: No grid discretization or interpolation required
- **Scalability**: Extends naturally to high-dimensional state spaces
- **Economic structure**: Monotonicity enforced architecturally via non-negative weights

---
title: "Monte Carlo Simulation for MDP"
subtitle: "Simulator and Structural Parameter Estimation"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Setup

```{python}
#| label: setup

import sys
from pathlib import Path

import numpy as np
import torch

# Add project paths
sys.path.insert(0, "../../../src")
sys.path.insert(0, "../config")
sys.path.insert(0, "../../utils")

# Import MDP solver
from mdp.solver import (
    build_monotonic_network,
    evaluate_network,
    compute_choice_probability,
    compute_integrated_value,
)

# Import MDP simulator
from mdp.simulator import simulate_mdp_panel

# Import configuration
import config

# Import plotting utilities
from plotting import (
    plot_convergence,
    plot_value_functions,
    plot_choice_probabilities,
    plot_value_difference,
    plot_bellman_residuals,
    plot_state_evolution,
    plot_trajectories,
    plot_choice_validation,
    plot_calibration,
    plot_state_transitions,
    plot_state_distribution,
    plot_reward_distribution,
)

print("Modules loaded successfully")
```

## Load Configuration and Trained Model

```{python}
#| label: load-model

# Load parameters from central config
beta = config.beta
gamma = config.gamma
delta = config.delta
s_min = config.s_min
s_max = config.s_max
hidden_sizes = config.hidden_sizes

print("Model parameters (from scripts/config_mdp/config.py):")
print(f"  beta = {beta}")
print(f"  gamma = {gamma}")
print(f"  delta = {delta}")
print(f"  s_min = {s_min}")
print(f"  s_max = {s_max}")
print(f"  hidden_sizes = {hidden_sizes}")

# Load trained networks from solver output
output_dir = Path("../../../output/mdp/solve")

v0_net = build_monotonic_network(hidden_sizes=hidden_sizes)
v1_net = build_monotonic_network(hidden_sizes=hidden_sizes)

v0_net.load_state_dict(torch.load(output_dir / "v0_net.pt", weights_only=True))
v1_net.load_state_dict(torch.load(output_dir / "v1_net.pt", weights_only=True))

v0_net.eval()
v1_net.eval()

# Load training losses
losses = np.load(output_dir / "losses.npy")

print(f"\nLoaded trained networks from {output_dir.resolve()}")
print(f"  Training iterations: {len(losses)}")
print(f"  Final RMSE: {losses[-1]**0.5:.2e}")
```

## Simulation Algorithm

### Overview

The Monte Carlo simulator generates panel data by forward-simulating agents who follow the optimal policy derived from the trained value functions. Each period:

1. Agent observes current state $s_t$
2. Agent draws action from the closed-form logit choice probability:
   $$a_t \sim \text{Bernoulli}\left( P(a=1|s_t) \right) \quad \text{where} \quad P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
3. State transitions: $s_{t+1} = (1 - \gamma) s_t + a_t$

Since Type-I Extreme Value shocks yield closed-form logit probabilities, we use the analytical choice probabilities directly rather than drawing shocks.

### Type Definitions

```
TYPE DEFINITIONS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Scalar      = Float                      # Single real number
Vector[n]   = Array[Float, n]            # 1D array of n floats
Matrix[m,n] = Array[Float, m, n]         # 2D array of shape (m, n)
Action      = Int ‚àà {0, 1}               # Binary action
Network     = MonotonicNeuralNetwork     # Trained value function network
```

### Main Algorithm

```
ALGORITHM: SIMULATE_MDP_PANEL
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
INPUT:
  v‚ÇÄ_net       : Network       # Trained network for action 0
  v‚ÇÅ_net       : Network       # Trained network for action 1
  n_agents     : Int           # Number of agents
  n_periods    : Int           # Number of time periods
  s_init       : Vector[n] | Scalar  # Initial states
  Œ≤            : Scalar        # Reward coefficient
  Œ≥            : Scalar        # State decay rate
  seed         : Int           # Random seed for reproducibility

OUTPUT:
  states       : Matrix[n, T]  # State history for each agent
  actions      : Matrix[n, T]  # Action history for each agent
  rewards      : Matrix[n, T]  # Reward history for each agent

PROCEDURE:
  SET_SEED(seed)
  
  # Initialize storage matrices
  states  ‚Üê Matrix[n_agents, n_periods]
  actions ‚Üê Matrix[n_agents, n_periods]
  rewards ‚Üê Matrix[n_agents, n_periods]
  
  # Set initial states
  states[*, 0] ‚Üê s_init
  
  FOR t = 0 TO n_periods - 1:
    # Get current states for all agents
    s_t ‚Üê states[*, t]
    
    # Compute choice probabilities using closed-form logit formula
    _, p‚ÇÅ ‚Üê COMPUTE_CHOICE_PROBABILITY(v‚ÇÄ_net, v‚ÇÅ_net, s_t)
    
    # Draw actions from Bernoulli distribution
    u ‚Üê UNIFORM(0, 1, n_agents)
    actions[*, t] ‚Üê (u < p‚ÇÅ) ? 1 : 0
    
    # Compute flow rewards
    rewards[*, t] ‚Üê COMPUTE_REWARD(s_t, actions[*, t], Œ≤)
    
    # State transition (if not last period)
    IF t < n_periods - 1:
      states[*, t+1] ‚Üê COMPUTE_NEXT_STATE(s_t, actions[*, t], Œ≥)
  
  RETURN (states, actions, rewards)
```

### Subroutines

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
INPUT:
  v‚ÇÄ_net : Network           # Trained network for action 0
  v‚ÇÅ_net : Network           # Trained network for action 1
  s      : Vector[n]         # States
OUTPUT:
  p‚ÇÄ     : Vector[n]         # P(a=0|s)
  p‚ÇÅ     : Vector[n]         # P(a=1|s)

  # Evaluate value functions
  v‚ÇÄ ‚Üê EVALUATE_NETWORK(v‚ÇÄ_net, s)
  v‚ÇÅ ‚Üê EVALUATE_NETWORK(v‚ÇÅ_net, s)
  
  # Closed-form logit probabilities (from Type-I EV distribution)
  # Using numerically stable softmax
  v_max ‚Üê MAX(v‚ÇÄ, v‚ÇÅ)
  exp‚ÇÄ ‚Üê exp(v‚ÇÄ - v_max)
  exp‚ÇÅ ‚Üê exp(v‚ÇÅ - v_max)
  sum_exp ‚Üê exp‚ÇÄ + exp‚ÇÅ
  
  p‚ÇÄ ‚Üê exp‚ÇÄ / sum_exp
  p‚ÇÅ ‚Üê exp‚ÇÅ / sum_exp
  
  RETURN (p‚ÇÄ, p‚ÇÅ)
```

```
SUBROUTINE: COMPUTE_REWARD
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
INPUT:
  s      : Vector[n]         # States
  a      : Vector[n]         # Actions (0 or 1)
  Œ≤      : Scalar            # Reward coefficient
OUTPUT:
  r      : Vector[n]         # Flow rewards

  FOR i = 0 TO n - 1:
    r[i] ‚Üê Œ≤ * log(1 + s[i]) - a[i]
  RETURN r
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
INPUT:
  s      : Vector[n]         # Current states
  a      : Vector[n]         # Actions (0 or 1)
  Œ≥      : Scalar            # Decay rate
OUTPUT:
  s'     : Vector[n]         # Next states

  FOR i = 0 TO n - 1:
    s'[i] ‚Üê (1 - Œ≥) * s[i] + a[i]
  RETURN s'
```

### Properties

The simulation algorithm satisfies the following properties:

1. **Exactness**: Actions are drawn directly from the closed-form logit probabilities:
   $$P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
   This uses the analytical result from Type-I Extreme Value shocks rather than simulating the shocks.

2. **Ergodicity**: Under the optimal policy, the state process converges to a stationary distribution (given $0 < \gamma < 1$).

3. **Reproducibility**: Given the same seed, the algorithm produces identical panel data.

## Run Simulation

```{python}
#| label: run-simulation

# Simulation parameters
n_agents = 100       # Number of agents (paths)
n_periods = 100      # Number of time periods
simulation_seed = 42 # Random seed

# Randomize initial states: draw from Uniform[s_min, s_max]
np.random.seed(simulation_seed)
s_init = np.random.uniform(s_min, s_max, size=n_agents)

print(f"Initial state distribution:")
print(f"  Distribution: Uniform[{s_min}, {s_max}]")
print(f"  Mean: {s_init.mean():.2f}")
print(f"  Std:  {s_init.std():.2f}")
print(f"  Min:  {s_init.min():.2f}")
print(f"  Max:  {s_init.max():.2f}")

# Run simulation using the implemented algorithm
panel = simulate_mdp_panel(
    v0_net=v0_net,
    v1_net=v1_net,
    n_agents=n_agents,
    n_periods=n_periods,
    s_init=s_init,  # Vector of different initial states per agent
    beta=beta,
    gamma=gamma,
    seed=simulation_seed,
)

print(f"\nSimulation complete:")
print(f"  Agents: {panel.n_agents}")
print(f"  Periods: {panel.n_periods}")
print(f"  Total observations: {panel.n_agents * panel.n_periods:,}")

# Save simulated data for estimation
import json

sim_output_dir = Path("../../../output/mdp/simulate")
sim_output_dir.mkdir(parents=True, exist_ok=True)

# Save panel data arrays
np.save(sim_output_dir / "states.npy", panel.states)
np.save(sim_output_dir / "actions.npy", panel.actions)
np.save(sim_output_dir / "rewards.npy", panel.rewards)

# Save simulation configuration
sim_config = {
    "n_agents": n_agents,
    "n_periods": n_periods,
    "seed": simulation_seed,
    "s_init_distribution": f"Uniform[{s_min}, {s_max}]",
    "beta": beta,
    "gamma": gamma,
    "delta": delta,
}
with open(sim_output_dir / "config.json", "w") as f:
    json.dump(sim_config, f, indent=2)

print(f"\nSaved simulation data to {sim_output_dir.resolve()}:")
print(f"  states.npy:  {panel.states.shape}")
print(f"  actions.npy: {panel.actions.shape}")
print(f"  rewards.npy: {panel.rewards.shape}")
print(f"  config.json: simulation parameters")
```

## Simulation Results

### Summary Statistics

```{python}
#| label: summary-stats

print("=" * 60)
print("PANEL DATA SUMMARY STATISTICS")
print("=" * 60)

# State statistics
print("\nState Distribution:")
print(f"  Mean:   {panel.states.mean():.3f}")
print(f"  Std:    {panel.states.std():.3f}")
print(f"  Min:    {panel.states.min():.3f}")
print(f"  Max:    {panel.states.max():.3f}")

# Action statistics
action_rate = panel.actions.mean()
print(f"\nAction Statistics:")
print(f"  P(a=1): {action_rate:.3f} (investment rate)")
print(f"  P(a=0): {1 - action_rate:.3f} (no investment rate)")

# Reward statistics
print(f"\nReward Distribution:")
print(f"  Mean:   {panel.rewards.mean():.3f}")
print(f"  Std:    {panel.rewards.std():.3f}")
print(f"  Min:    {panel.rewards.min():.3f}")
print(f"  Max:    {panel.rewards.max():.3f}")

# Steady state analysis
print(f"\nSteady State Analysis (last 10 periods):")
late_states = panel.states[:, -10:]
late_actions = panel.actions[:, -10:]
print(f"  Mean state:  {late_states.mean():.3f}")
print(f"  P(a=1):      {late_actions.mean():.3f}")

# Note: Total rewards and final states statistics are reported 
# in dedicated sections below with histograms
```

### State Evolution

```{python}
#| label: plot-state-evolution
#| fig-cap: "Evolution of state distribution over time"

plot_state_evolution(panel.states, panel.actions, s_init.mean())
```

### Sample Trajectories

```{python}
#| label: plot-trajectories
#| fig-cap: "Sample agent trajectories showing state evolution"

plot_trajectories(panel.states, s_init.mean(), n_samples=10)
```

### Choice Probability Validation

```{python}
#| label: plot-choice-validation
#| fig-cap: "Comparison of simulated choice frequencies with theoretical choice probabilities"

# Flatten data for plotting
state_flat = panel.states.flatten()
action_flat = panel.actions.flatten()

# Compute theoretical probabilities
s_theory = torch.linspace(state_flat.min(), state_flat.max(), 200)
with torch.no_grad():
    _, prob_a1_theory = compute_choice_probability(v0_net, v1_net, s_theory)
    prob_a1_theory = prob_a1_theory.numpy()

rmse = plot_choice_validation(
    state_flat, action_flat, s_theory.numpy(), prob_a1_theory
)
```

### Choice Probability Calibration

```{python}
#| label: plot-calibration
#| fig-cap: "Calibration plot: empirical vs theoretical choice probabilities"

# Compute theoretical P(a=1|s) for all observations
s_all = torch.tensor(state_flat, dtype=torch.float32)
with torch.no_grad():
    _, p1_theory_all = compute_choice_probability(v0_net, v1_net, s_all)
    p1_theory_all = p1_theory_all.numpy()

cal_rmse = plot_calibration(action_flat, p1_theory_all, n_bins=50)
```

### State Transition Dynamics

```{python}
#| label: plot-transitions
#| fig-cap: "State transition dynamics: current state vs next state by action"

plot_state_transitions(panel.states, panel.actions, gamma)
```

### State Distribution (Initial vs Final)

```{python}
#| label: plot-state-distribution
#| fig-cap: "Distribution of states: initial (t=0) vs final (t=T-1)"

final_states = panel.states[:, -1]
plot_state_distribution(s_init, final_states, s_min, s_max, n_periods)
```

### Reward Distribution (Cumulative vs Discounted)

```{python}
#| label: plot-reward-distribution
#| fig-cap: "Distribution of total rewards: cumulative vs discounted"

cumulative_rewards, discounted_rewards = plot_reward_distribution(panel.rewards, delta)
```

## Summary

```{python}
#| label: summary

print("=" * 60)
print("SIMULATION SUMMARY")
print("=" * 60)

print(f"\nModel Parameters:")
print(f"  Œ≤ = {beta} (reward coefficient)")
print(f"  Œ≥ = {gamma} (state decay rate)")
print(f"  Œ¥ = {delta} (discount factor)")

print(f"\nSimulation Setup:")
print(f"  Agents: {panel.n_agents}")
print(f"  Periods: {panel.n_periods}")
print(f"  Initial states: Uniform[{s_min}, {s_max}], mean={s_init.mean():.1f}")
print(f"  Random seed: {simulation_seed}")

print(f"\nKey Results:")
print(f"  Overall investment rate: {panel.actions.mean():.1%}")
print(f"  Stationary mean state: {panel.states[:, -20:].mean():.2f}")
print(f"  Choice probability RMSE: {rmse:.4f}")

print(f"\nThe simulation validates that:")
print(f"  ‚úì State transitions follow s' = (1-Œ≥)s + a")
print(f"  ‚úì Empirical choice frequencies match theoretical logit probabilities")
print(f"  ‚úì State distribution converges to a stationary distribution")
```

## Diagnostics

```{python}
#| label: diagnostics

print("=" * 60)
print("SIMULATION DIAGNOSTICS")
print("=" * 60)

# -----------------------------------------------------------------------------
# 1. Choice Probability Validation
# -----------------------------------------------------------------------------
print("\n1. CHOICE PROBABILITY VALIDATION")
print("-" * 40)

if rmse < 0.01:
    prob_status = "‚úì EXCELLENT"
    prob_msg = "Empirical choices closely match theoretical probabilities"
elif rmse < 0.05:
    prob_status = "‚úì GOOD"
    prob_msg = "Minor deviations, likely due to sampling variance"
elif rmse < 0.10:
    prob_status = "‚ö† WARNING"
    prob_msg = "Moderate deviations detected - consider larger sample"
else:
    prob_status = "‚úó POOR"
    prob_msg = "Large deviations - potential implementation issue"

print(f"   RMSE: {rmse:.4f}")
print(f"   Status: {prob_status}")
print(f"   {prob_msg}")

# -----------------------------------------------------------------------------
# 2. Calibration Quality
# -----------------------------------------------------------------------------
print("\n2. CALIBRATION QUALITY")
print("-" * 40)

if cal_rmse < 0.01:
    cal_status = "‚úì EXCELLENT"
    cal_msg = "Perfect calibration - simulation draws from correct distribution"
elif cal_rmse < 0.03:
    cal_status = "‚úì GOOD"
    cal_msg = "Well-calibrated with minor sampling noise"
elif cal_rmse < 0.05:
    cal_status = "‚ö† WARNING"
    cal_msg = "Some miscalibration - check binning or sample size"
else:
    cal_status = "‚úó POOR"
    cal_msg = "Significant miscalibration - verify action drawing mechanism"

print(f"   Calibration RMSE: {cal_rmse:.4f}")
print(f"   Status: {cal_status}")
print(f"   {cal_msg}")

# -----------------------------------------------------------------------------
# 3. State Convergence Analysis
# -----------------------------------------------------------------------------
print("\n3. STATE CONVERGENCE ANALYSIS")
print("-" * 40)

# Check if states have converged by comparing early vs late periods
early_mean = panel.states[:, :20].mean()
late_mean = panel.states[:, -20:].mean()
convergence_diff = abs(late_mean - early_mean)

# Compute steady-state theoretical value (approximate)
# At steady state: E[s'] = E[s] => (1-Œ≥)E[s] + E[a] = E[s] => E[s] = E[a]/Œ≥
late_action_rate = panel.actions[:, -20:].mean()
theoretical_steady_state = late_action_rate / gamma

steady_state_error = abs(late_mean - theoretical_steady_state)

if convergence_diff < 0.5 and steady_state_error < 0.5:
    conv_status = "‚úì CONVERGED"
    conv_msg = "State distribution has reached stationarity"
elif convergence_diff < 1.0:
    conv_status = "‚ö† NEARLY CONVERGED"
    conv_msg = "Close to stationary - consider more periods"
else:
    conv_status = "‚úó NOT CONVERGED"
    conv_msg = "Still transitioning - increase simulation length"

print(f"   Early mean (t<20):  {early_mean:.2f}")
print(f"   Late mean (t‚â•{n_periods-20}):   {late_mean:.2f}")
print(f"   Theoretical steady state: {theoretical_steady_state:.2f}")
print(f"   Status: {conv_status}")
print(f"   {conv_msg}")

# -----------------------------------------------------------------------------
# 4. State Transition Verification
# -----------------------------------------------------------------------------
print("\n4. STATE TRANSITION VERIFICATION")
print("-" * 40)

# Verify transitions match model
s_current = panel.states[:, :-1].flatten()
s_next_actual = panel.states[:, 1:].flatten()
a_taken = panel.actions[:, :-1].flatten()

# Expected next state
s_next_expected = (1 - gamma) * s_current + a_taken

# Compute transition error
transition_error = np.abs(s_next_actual - s_next_expected).max()

if transition_error < 1e-6:
    trans_status = "‚úì EXACT"
    trans_msg = "State transitions perfectly follow s' = (1-Œ≥)s + a"
elif transition_error < 1e-4:
    trans_status = "‚úì GOOD"
    trans_msg = "Minor floating point differences only"
else:
    trans_status = "‚úó ERROR"
    trans_msg = f"Transition mismatch detected (max error: {transition_error:.2e})"

print(f"   Max transition error: {transition_error:.2e}")
print(f"   Status: {trans_status}")
print(f"   {trans_msg}")

# -----------------------------------------------------------------------------
# 5. Reward Consistency Check
# -----------------------------------------------------------------------------
print("\n5. REWARD CONSISTENCY CHECK")
print("-" * 40)

# Verify rewards match model: r = Œ≤*log(1+s) - a
states_flat = panel.states.flatten()
actions_flat = panel.actions.flatten()
rewards_flat = panel.rewards.flatten()

expected_rewards = beta * np.log(1 + states_flat) - actions_flat
reward_error = np.abs(rewards_flat - expected_rewards).max()

if reward_error < 1e-6:
    reward_status = "‚úì EXACT"
    reward_msg = "Rewards perfectly match r = Œ≤¬∑log(1+s) - a"
elif reward_error < 1e-4:
    reward_status = "‚úì GOOD"
    reward_msg = "Minor floating point differences only"
else:
    reward_status = "‚úó ERROR"
    reward_msg = f"Reward mismatch detected (max error: {reward_error:.2e})"

print(f"   Max reward error: {reward_error:.2e}")
print(f"   Status: {reward_status}")
print(f"   {reward_msg}")

# -----------------------------------------------------------------------------
# Overall Assessment
# -----------------------------------------------------------------------------
print("\n" + "=" * 60)
print("OVERALL ASSESSMENT")
print("=" * 60)

all_checks = [
    ("Choice Probability", rmse < 0.05),
    ("Calibration", cal_rmse < 0.05),
    ("Convergence", convergence_diff < 1.0),
    ("Transitions", transition_error < 1e-4),
    ("Rewards", reward_error < 1e-4),
]

passed = sum(1 for _, ok in all_checks if ok)
total = len(all_checks)

print(f"\nChecks Passed: {passed}/{total}")
for name, ok in all_checks:
    status = "‚úì" if ok else "‚úó"
    print(f"   {status} {name}")

if passed == total:
    print(f"\nüéâ SIMULATION VALIDATED SUCCESSFULLY")
    print(f"   The Monte Carlo simulation correctly implements the MDP model.")
    print(f"   Simulated data can be used for structural estimation.")
elif passed >= total - 1:
    print(f"\n‚ö†Ô∏è  SIMULATION MOSTLY VALIDATED")
    print(f"   Minor issues detected - review warnings above.")
else:
    print(f"\n‚ùå SIMULATION REQUIRES ATTENTION")
    print(f"   Multiple issues detected - review diagnostics above.")
```

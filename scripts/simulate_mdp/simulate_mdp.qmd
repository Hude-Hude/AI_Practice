---
title: "Monte Carlo Simulation for MDP"
subtitle: "Simulator and Structural Parameter Estimation"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Setup

```{python}
#| label: setup

import sys
from pathlib import Path

import numpy as np
import torch

# Add project paths
sys.path.insert(0, "../../src")
sys.path.insert(0, "../config_mdp")
sys.path.insert(0, "../mdp_utils")

# Import MDP solver
from mdp_solver import (
    build_monotonic_network,
    evaluate_network,
    compute_choice_probability,
    compute_integrated_value,
)

# Import configuration
import config

# Import plotting utilities
from plotting import (
    plot_convergence,
    plot_value_functions,
    plot_choice_probabilities,
    plot_value_difference,
    plot_bellman_residuals,
)

print("Modules loaded successfully")
```

## Load Configuration and Trained Model

```{python}
#| label: load-model

# Load parameters from central config
beta = config.beta
gamma = config.gamma
delta = config.delta
s_min = config.s_min
s_max = config.s_max
hidden_sizes = config.hidden_sizes

print("Model parameters (from scripts/config_mdp/config.py):")
print(f"  beta = {beta}")
print(f"  gamma = {gamma}")
print(f"  delta = {delta}")
print(f"  s_min = {s_min}")
print(f"  s_max = {s_max}")
print(f"  hidden_sizes = {hidden_sizes}")

# Load trained networks from solver output
output_dir = Path("../../output/solve_mdp")

v0_net = build_monotonic_network(hidden_sizes=hidden_sizes)
v1_net = build_monotonic_network(hidden_sizes=hidden_sizes)

v0_net.load_state_dict(torch.load(output_dir / "v0_net.pt", weights_only=True))
v1_net.load_state_dict(torch.load(output_dir / "v1_net.pt", weights_only=True))

v0_net.eval()
v1_net.eval()

# Load training losses
losses = np.load(output_dir / "losses.npy")

print(f"\nLoaded trained networks from {output_dir.resolve()}")
print(f"  Training iterations: {len(losses)}")
print(f"  Final RMSE: {losses[-1]**0.5:.2e}")
```

## Simulation Algorithm

### Overview

The Monte Carlo simulator generates panel data by forward-simulating agents who follow the optimal policy derived from the trained value functions. Each period:

1. Agent observes current state $s_t$
2. Agent draws action from the closed-form logit choice probability:
   $$a_t \sim \text{Bernoulli}\left( P(a=1|s_t) \right) \quad \text{where} \quad P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
3. State transitions: $s_{t+1} = (1 - \gamma) s_t + a_t$

Since Type-I Extreme Value shocks yield closed-form logit probabilities, we use the analytical choice probabilities directly rather than drawing shocks.

### Type Definitions

```
TYPE DEFINITIONS
────────────────
Scalar      = Float                      # Single real number
Vector[n]   = Array[Float, n]            # 1D array of n floats
Matrix[m,n] = Array[Float, m, n]         # 2D array of shape (m, n)
Action      = Int ∈ {0, 1}               # Binary action
Network     = MonotonicNeuralNetwork     # Trained value function network
```

### Main Algorithm

```
ALGORITHM: SIMULATE_MDP_PANEL
─────────────────────────────
INPUT:
  v₀_net       : Network       # Trained network for action 0
  v₁_net       : Network       # Trained network for action 1
  n_agents     : Int           # Number of agents
  n_periods    : Int           # Number of time periods
  s_init       : Vector[n] | Scalar  # Initial states
  β            : Scalar        # Reward coefficient
  γ            : Scalar        # State decay rate
  seed         : Int           # Random seed for reproducibility

OUTPUT:
  states       : Matrix[n, T]  # State history for each agent
  actions      : Matrix[n, T]  # Action history for each agent
  rewards      : Matrix[n, T]  # Reward history for each agent

PROCEDURE:
  SET_SEED(seed)
  
  # Initialize storage matrices
  states  ← Matrix[n_agents, n_periods]
  actions ← Matrix[n_agents, n_periods]
  rewards ← Matrix[n_agents, n_periods]
  
  # Set initial states
  states[*, 0] ← s_init
  
  FOR t = 0 TO n_periods - 1:
    # Get current states for all agents
    s_t ← states[*, t]
    
    # Compute choice probabilities using closed-form logit formula
    _, p₁ ← COMPUTE_CHOICE_PROBABILITY(v₀_net, v₁_net, s_t)
    
    # Draw actions from Bernoulli distribution
    u ← UNIFORM(0, 1, n_agents)
    actions[*, t] ← (u < p₁) ? 1 : 0
    
    # Compute flow rewards
    rewards[*, t] ← COMPUTE_REWARD(s_t, actions[*, t], β)
    
    # State transition (if not last period)
    IF t < n_periods - 1:
      states[*, t+1] ← COMPUTE_NEXT_STATE(s_t, actions[*, t], γ)
  
  RETURN (states, actions, rewards)
```

### Subroutines

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
──────────────────────────────────────
INPUT:
  v₀_net : Network           # Trained network for action 0
  v₁_net : Network           # Trained network for action 1
  s      : Vector[n]         # States
OUTPUT:
  p₀     : Vector[n]         # P(a=0|s)
  p₁     : Vector[n]         # P(a=1|s)

  # Evaluate value functions
  v₀ ← EVALUATE_NETWORK(v₀_net, s)
  v₁ ← EVALUATE_NETWORK(v₁_net, s)
  
  # Closed-form logit probabilities (from Type-I EV distribution)
  # Using numerically stable softmax
  v_max ← MAX(v₀, v₁)
  exp₀ ← exp(v₀ - v_max)
  exp₁ ← exp(v₁ - v_max)
  sum_exp ← exp₀ + exp₁
  
  p₀ ← exp₀ / sum_exp
  p₁ ← exp₁ / sum_exp
  
  RETURN (p₀, p₁)
```

```
SUBROUTINE: COMPUTE_REWARD
──────────────────────────
INPUT:
  s      : Vector[n]         # States
  a      : Vector[n]         # Actions (0 or 1)
  β      : Scalar            # Reward coefficient
OUTPUT:
  r      : Vector[n]         # Flow rewards

  FOR i = 0 TO n - 1:
    r[i] ← β * log(1 + s[i]) - a[i]
  RETURN r
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
──────────────────────────────
INPUT:
  s      : Vector[n]         # Current states
  a      : Vector[n]         # Actions (0 or 1)
  γ      : Scalar            # Decay rate
OUTPUT:
  s'     : Vector[n]         # Next states

  FOR i = 0 TO n - 1:
    s'[i] ← (1 - γ) * s[i] + a[i]
  RETURN s'
```

### Properties

The simulation algorithm satisfies the following properties:

1. **Exactness**: Actions are drawn directly from the closed-form logit probabilities:
   $$P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
   This uses the analytical result from Type-I Extreme Value shocks rather than simulating the shocks.

2. **Ergodicity**: Under the optimal policy, the state process converges to a stationary distribution (given $0 < \gamma < 1$).

3. **Reproducibility**: Given the same seed, the algorithm produces identical panel data.

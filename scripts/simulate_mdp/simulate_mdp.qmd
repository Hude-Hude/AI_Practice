---
title: "Monte Carlo Simulation for MDP"
subtitle: "Simulator and Structural Parameter Estimation"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Setup

```{python}
#| label: setup

import sys
from pathlib import Path

import numpy as np
import torch

# Add project paths
sys.path.insert(0, "../../src")
sys.path.insert(0, "../config_mdp")
sys.path.insert(0, "../mdp_utils")

# Import MDP solver
from mdp_solver import (
    build_monotonic_network,
    evaluate_network,
    compute_choice_probability,
    compute_integrated_value,
)

# Import MDP simulator
from mdp_simulator import simulate_mdp_panel

# Import configuration
import config

# Import plotting utilities
from plotting import (
    plot_convergence,
    plot_value_functions,
    plot_choice_probabilities,
    plot_value_difference,
    plot_bellman_residuals,
)

print("Modules loaded successfully")
```

## Load Configuration and Trained Model

```{python}
#| label: load-model

# Load parameters from central config
beta = config.beta
gamma = config.gamma
delta = config.delta
s_min = config.s_min
s_max = config.s_max
hidden_sizes = config.hidden_sizes

print("Model parameters (from scripts/config_mdp/config.py):")
print(f"  beta = {beta}")
print(f"  gamma = {gamma}")
print(f"  delta = {delta}")
print(f"  s_min = {s_min}")
print(f"  s_max = {s_max}")
print(f"  hidden_sizes = {hidden_sizes}")

# Load trained networks from solver output
output_dir = Path("../../output/solve_mdp")

v0_net = build_monotonic_network(hidden_sizes=hidden_sizes)
v1_net = build_monotonic_network(hidden_sizes=hidden_sizes)

v0_net.load_state_dict(torch.load(output_dir / "v0_net.pt", weights_only=True))
v1_net.load_state_dict(torch.load(output_dir / "v1_net.pt", weights_only=True))

v0_net.eval()
v1_net.eval()

# Load training losses
losses = np.load(output_dir / "losses.npy")

print(f"\nLoaded trained networks from {output_dir.resolve()}")
print(f"  Training iterations: {len(losses)}")
print(f"  Final RMSE: {losses[-1]**0.5:.2e}")
```

## Simulation Algorithm

### Overview

The Monte Carlo simulator generates panel data by forward-simulating agents who follow the optimal policy derived from the trained value functions. Each period:

1. Agent observes current state $s_t$
2. Agent draws action from the closed-form logit choice probability:
   $$a_t \sim \text{Bernoulli}\left( P(a=1|s_t) \right) \quad \text{where} \quad P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
3. State transitions: $s_{t+1} = (1 - \gamma) s_t + a_t$

Since Type-I Extreme Value shocks yield closed-form logit probabilities, we use the analytical choice probabilities directly rather than drawing shocks.

### Type Definitions

```
TYPE DEFINITIONS
────────────────
Scalar      = Float                      # Single real number
Vector[n]   = Array[Float, n]            # 1D array of n floats
Matrix[m,n] = Array[Float, m, n]         # 2D array of shape (m, n)
Action      = Int ∈ {0, 1}               # Binary action
Network     = MonotonicNeuralNetwork     # Trained value function network
```

### Main Algorithm

```
ALGORITHM: SIMULATE_MDP_PANEL
─────────────────────────────
INPUT:
  v₀_net       : Network       # Trained network for action 0
  v₁_net       : Network       # Trained network for action 1
  n_agents     : Int           # Number of agents
  n_periods    : Int           # Number of time periods
  s_init       : Vector[n] | Scalar  # Initial states
  β            : Scalar        # Reward coefficient
  γ            : Scalar        # State decay rate
  seed         : Int           # Random seed for reproducibility

OUTPUT:
  states       : Matrix[n, T]  # State history for each agent
  actions      : Matrix[n, T]  # Action history for each agent
  rewards      : Matrix[n, T]  # Reward history for each agent

PROCEDURE:
  SET_SEED(seed)
  
  # Initialize storage matrices
  states  ← Matrix[n_agents, n_periods]
  actions ← Matrix[n_agents, n_periods]
  rewards ← Matrix[n_agents, n_periods]
  
  # Set initial states
  states[*, 0] ← s_init
  
  FOR t = 0 TO n_periods - 1:
    # Get current states for all agents
    s_t ← states[*, t]
    
    # Compute choice probabilities using closed-form logit formula
    _, p₁ ← COMPUTE_CHOICE_PROBABILITY(v₀_net, v₁_net, s_t)
    
    # Draw actions from Bernoulli distribution
    u ← UNIFORM(0, 1, n_agents)
    actions[*, t] ← (u < p₁) ? 1 : 0
    
    # Compute flow rewards
    rewards[*, t] ← COMPUTE_REWARD(s_t, actions[*, t], β)
    
    # State transition (if not last period)
    IF t < n_periods - 1:
      states[*, t+1] ← COMPUTE_NEXT_STATE(s_t, actions[*, t], γ)
  
  RETURN (states, actions, rewards)
```

### Subroutines

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
──────────────────────────────────────
INPUT:
  v₀_net : Network           # Trained network for action 0
  v₁_net : Network           # Trained network for action 1
  s      : Vector[n]         # States
OUTPUT:
  p₀     : Vector[n]         # P(a=0|s)
  p₁     : Vector[n]         # P(a=1|s)

  # Evaluate value functions
  v₀ ← EVALUATE_NETWORK(v₀_net, s)
  v₁ ← EVALUATE_NETWORK(v₁_net, s)
  
  # Closed-form logit probabilities (from Type-I EV distribution)
  # Using numerically stable softmax
  v_max ← MAX(v₀, v₁)
  exp₀ ← exp(v₀ - v_max)
  exp₁ ← exp(v₁ - v_max)
  sum_exp ← exp₀ + exp₁
  
  p₀ ← exp₀ / sum_exp
  p₁ ← exp₁ / sum_exp
  
  RETURN (p₀, p₁)
```

```
SUBROUTINE: COMPUTE_REWARD
──────────────────────────
INPUT:
  s      : Vector[n]         # States
  a      : Vector[n]         # Actions (0 or 1)
  β      : Scalar            # Reward coefficient
OUTPUT:
  r      : Vector[n]         # Flow rewards

  FOR i = 0 TO n - 1:
    r[i] ← β * log(1 + s[i]) - a[i]
  RETURN r
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
──────────────────────────────
INPUT:
  s      : Vector[n]         # Current states
  a      : Vector[n]         # Actions (0 or 1)
  γ      : Scalar            # Decay rate
OUTPUT:
  s'     : Vector[n]         # Next states

  FOR i = 0 TO n - 1:
    s'[i] ← (1 - γ) * s[i] + a[i]
  RETURN s'
```

### Properties

The simulation algorithm satisfies the following properties:

1. **Exactness**: Actions are drawn directly from the closed-form logit probabilities:
   $$P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
   This uses the analytical result from Type-I Extreme Value shocks rather than simulating the shocks.

2. **Ergodicity**: Under the optimal policy, the state process converges to a stationary distribution (given $0 < \gamma < 1$).

3. **Reproducibility**: Given the same seed, the algorithm produces identical panel data.

## Run Simulation

```{python}
#| label: run-simulation

# Simulation parameters
n_agents = 100       # Number of agents (paths)
n_periods = 100      # Number of time periods
s_init = 5.0         # Initial state for all agents
simulation_seed = 42 # Random seed

# Run simulation using the implemented algorithm
panel = simulate_mdp_panel(
    v0_net=v0_net,
    v1_net=v1_net,
    n_agents=n_agents,
    n_periods=n_periods,
    s_init=s_init,
    beta=beta,
    gamma=gamma,
    seed=simulation_seed,
)

print(f"Simulation complete:")
print(f"  Agents: {panel.n_agents}")
print(f"  Periods: {panel.n_periods}")
print(f"  Initial state: {s_init}")
print(f"  Total observations: {panel.n_agents * panel.n_periods:,}")
```

## Simulation Results

### Summary Statistics

```{python}
#| label: summary-stats

print("=" * 60)
print("PANEL DATA SUMMARY STATISTICS")
print("=" * 60)

# State statistics
print("\nState Distribution:")
print(f"  Mean:   {panel.states.mean():.3f}")
print(f"  Std:    {panel.states.std():.3f}")
print(f"  Min:    {panel.states.min():.3f}")
print(f"  Max:    {panel.states.max():.3f}")

# Action statistics
action_rate = panel.actions.mean()
print(f"\nAction Statistics:")
print(f"  P(a=1): {action_rate:.3f} (investment rate)")
print(f"  P(a=0): {1 - action_rate:.3f} (no investment rate)")

# Reward statistics
print(f"\nReward Distribution:")
print(f"  Mean:   {panel.rewards.mean():.3f}")
print(f"  Std:    {panel.rewards.std():.3f}")
print(f"  Min:    {panel.rewards.min():.3f}")
print(f"  Max:    {panel.rewards.max():.3f}")

# Steady state analysis
print(f"\nSteady State Analysis (last 10 periods):")
late_states = panel.states[:, -10:]
late_actions = panel.actions[:, -10:]
print(f"  Mean state:  {late_states.mean():.3f}")
print(f"  P(a=1):      {late_actions.mean():.3f}")

# Note: Total rewards and final states statistics are reported 
# in dedicated sections below with histograms
```

### State Evolution

```{python}
#| label: plot-state-evolution
#| fig-cap: "Evolution of state distribution over time"

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Mean state over time with confidence band
ax0 = axes[0]
mean_state = panel.states.mean(axis=0)
std_state = panel.states.std(axis=0)
periods = np.arange(n_periods)

ax0.plot(periods, mean_state, 'b-', linewidth=2, label='Mean state')
ax0.fill_between(
    periods, 
    mean_state - std_state, 
    mean_state + std_state, 
    alpha=0.3, 
    color='blue',
    label='±1 std'
)
ax0.axhline(y=s_init, color='gray', linestyle='--', alpha=0.5, label=f'Initial: {s_init}')
ax0.set_xlabel('Period')
ax0.set_ylabel('State')
ax0.set_title('Mean State Over Time')
ax0.legend()
ax0.grid(True, alpha=0.3)

# Right: Action rate over time
ax1 = axes[1]
action_rate_by_period = panel.actions.mean(axis=0)
ax1.plot(periods, action_rate_by_period, 'r-', linewidth=2)
ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
ax1.set_xlabel('Period')
ax1.set_ylabel('P(a=1|t)')
ax1.set_title('Investment Rate Over Time')
ax1.set_ylim(0, 1)
ax1.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Sample Trajectories

```{python}
#| label: plot-trajectories
#| fig-cap: "Sample agent trajectories showing state evolution"

# Plot sample trajectories
n_samples = 10
fig, ax = plt.subplots(figsize=(12, 5))

for i in range(n_samples):
    ax.plot(periods, panel.states[i, :], alpha=0.6, linewidth=1)

ax.axhline(y=s_init, color='black', linestyle='--', alpha=0.5, label=f'Initial: {s_init}')
ax.set_xlabel('Period')
ax.set_ylabel('State')
ax.set_title(f'Sample Trajectories ({n_samples} agents)')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Choice Probability Validation

```{python}
#| label: plot-choice-validation
#| fig-cap: "Comparison of simulated choice frequencies with theoretical choice probabilities"

# Bin states and compute empirical action frequencies
n_bins = 20
state_flat = panel.states.flatten()
action_flat = panel.actions.flatten()

# Create bins
bin_edges = np.linspace(state_flat.min(), state_flat.max(), n_bins + 1)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# Compute empirical P(a=1|s) for each bin
empirical_prob = np.zeros(n_bins)
bin_counts = np.zeros(n_bins)

for i in range(n_bins):
    mask = (state_flat >= bin_edges[i]) & (state_flat < bin_edges[i + 1])
    if mask.sum() > 0:
        empirical_prob[i] = action_flat[mask].mean()
        bin_counts[i] = mask.sum()

# Compute theoretical P(a=1|s) from value functions
s_theory = torch.linspace(state_flat.min(), state_flat.max(), 200)
with torch.no_grad():
    _, prob_a1_theory = compute_choice_probability(v0_net, v1_net, s_theory)
    prob_a1_theory = prob_a1_theory.numpy()

# Plot comparison
fig, ax = plt.subplots(figsize=(10, 5))

# Theoretical
ax.plot(s_theory.numpy(), prob_a1_theory, 'r-', linewidth=2, label='Theoretical P(a=1|s)')

# Empirical (only bins with enough data)
valid_bins = bin_counts > 50
ax.scatter(
    bin_centers[valid_bins], 
    empirical_prob[valid_bins], 
    s=50, 
    c='blue', 
    alpha=0.7, 
    label='Simulated (binned)',
    zorder=5
)

ax.set_xlabel('State s')
ax.set_ylabel('P(a=1|s)')
ax.set_title('Choice Probability: Theoretical vs Simulated')
ax.set_ylim(-0.05, 1.05)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Compute goodness of fit
valid_theory = np.interp(bin_centers[valid_bins], s_theory.numpy(), prob_a1_theory)
rmse = np.sqrt(np.mean((empirical_prob[valid_bins] - valid_theory) ** 2))
print(f"RMSE between theoretical and simulated choice probabilities: {rmse:.4f}")
```

### State Transition Dynamics

```{python}
#| label: plot-transitions
#| fig-cap: "State transition dynamics: current state vs next state by action"

# Extract transitions (excluding last period)
s_current = panel.states[:, :-1].flatten()
s_next = panel.states[:, 1:].flatten()
a_current = panel.actions[:, :-1].flatten()

# Separate by action
mask_a0 = a_current == 0
mask_a1 = a_current == 1

fig, ax = plt.subplots(figsize=(10, 8))

# Scatter plot (subsample for visibility)
subsample = 5000
np.random.seed(42)
idx = np.random.choice(len(s_current), size=min(subsample, len(s_current)), replace=False)

ax.scatter(
    s_current[idx][mask_a0[idx]], 
    s_next[idx][mask_a0[idx]], 
    alpha=0.3, 
    s=10, 
    c='blue', 
    label='a=0 (no investment)'
)
ax.scatter(
    s_current[idx][mask_a1[idx]], 
    s_next[idx][mask_a1[idx]], 
    alpha=0.3, 
    s=10, 
    c='red', 
    label='a=1 (investment)'
)

# Theoretical transition lines
s_line = np.linspace(0, panel.states.max(), 100)
ax.plot(s_line, (1 - gamma) * s_line, 'b--', linewidth=2, label=f"s' = {1-gamma:.1f}s (a=0)")
ax.plot(s_line, (1 - gamma) * s_line + 1, 'r--', linewidth=2, label=f"s' = {1-gamma:.1f}s + 1 (a=1)")

ax.plot([0, panel.states.max()], [0, panel.states.max()], 'k:', alpha=0.3, label='45° line')

ax.set_xlabel('Current State s')
ax.set_ylabel('Next State s\'')
ax.set_title('State Transitions by Action')
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_aspect('equal', adjustable='box')

plt.tight_layout()
plt.show()
```

### State Distribution (Pooled Over Time)

```{python}
#| label: plot-state-distribution
#| fig-cap: "Distribution of states pooled across time (not final states)"

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Histogram of ALL states (all agents × all periods)
ax0 = axes[0]
ax0.hist(state_flat, bins=30, density=True, alpha=0.7, color='steelblue', edgecolor='white')
ax0.axvline(x=state_flat.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {state_flat.mean():.2f}')
ax0.set_xlabel('State s')
ax0.set_ylabel('Density')
ax0.set_title(f'All States Pooled\n({n_agents} agents × {n_periods} periods = {n_agents*n_periods} obs)')
ax0.legend()
ax0.grid(True, alpha=0.3)

# Right: Histogram of late-period states (approximating stationary distribution)
ax1 = axes[1]
late_states_flat = panel.states[:, -20:].flatten()
ax1.hist(late_states_flat, bins=20, density=True, alpha=0.7, color='darkgreen', edgecolor='white')
ax1.axvline(x=late_states_flat.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {late_states_flat.mean():.2f}')
ax1.set_xlabel('State s')
ax1.set_ylabel('Density')
ax1.set_title(f'Late-Period States (t={n_periods-20} to t={n_periods-1})\n(Approx. Stationary Distribution)')
ax1.legend()
ax1.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("Note: These are NOT final-period-only distributions.")
print("      - Left: All observations pooled across agents and time")
print("      - Right: Last 20 periods pooled (stationary approximation)")
print("      - For final states, see 'Total Rewards Distribution' section below")
```

### Total Rewards Distribution

```{python}
#| label: plot-total-rewards
#| fig-cap: "Distribution of cumulative and discounted total rewards across agents"

# Compute cumulative rewards (simple sum)
cumulative_rewards = panel.rewards.sum(axis=1)  # Shape: (n_agents,)

# Compute discounted rewards: sum_{t=0}^{T-1} delta^t * r_t
discount_factors = delta ** np.arange(n_periods)  # [1, delta, delta^2, ..., delta^{T-1}]
discounted_rewards = (panel.rewards * discount_factors).sum(axis=1)  # Shape: (n_agents,)

print("Total Rewards Statistics:")
print("\nCumulative (undiscounted) Rewards:")
print(f"  Sum: Σ r_t")
print(f"  Mean:   {cumulative_rewards.mean():.2f}")
print(f"  Std:    {cumulative_rewards.std():.2f}")
print(f"  Min:    {cumulative_rewards.min():.2f}")
print(f"  Max:    {cumulative_rewards.max():.2f}")

print(f"\nDiscounted Rewards (δ = {delta}):")
print(f"  Sum: Σ δ^t r_t")
print(f"  Mean:   {discounted_rewards.mean():.2f}")
print(f"  Std:    {discounted_rewards.std():.2f}")
print(f"  Min:    {discounted_rewards.min():.2f}")
print(f"  Max:    {discounted_rewards.max():.2f}")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Cumulative rewards histogram
ax0 = axes[0]
ax0.hist(cumulative_rewards, bins=20, density=True, alpha=0.7, color='purple', edgecolor='white')
ax0.axvline(x=cumulative_rewards.mean(), color='red', linestyle='--', linewidth=2, 
            label=f'Mean: {cumulative_rewards.mean():.1f}')
ax0.set_xlabel('Cumulative Reward (Σ rₜ)')
ax0.set_ylabel('Density')
ax0.set_title('Cumulative Rewards (Undiscounted)')
ax0.legend()
ax0.grid(True, alpha=0.3)

# Right: Discounted rewards histogram
ax1 = axes[1]
ax1.hist(discounted_rewards, bins=20, density=True, alpha=0.7, color='darkgreen', edgecolor='white')
ax1.axvline(x=discounted_rewards.mean(), color='red', linestyle='--', linewidth=2,
            label=f'Mean: {discounted_rewards.mean():.1f}')
ax1.set_xlabel(f'Discounted Reward (Σ δᵗrₜ, δ={delta})')
ax1.set_ylabel('Density')
ax1.set_title('Discounted Rewards')
ax1.legend()
ax1.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Final States Distribution

```{python}
#| label: plot-final-states
#| fig-cap: "Distribution of final states (last period only)"

final_states = panel.states[:, -1]

fig, ax = plt.subplots(figsize=(8, 5))
ax.hist(final_states, bins=20, density=True, alpha=0.7, color='teal', edgecolor='white')
ax.axvline(x=final_states.mean(), color='red', linestyle='--', linewidth=2,
           label=f'Mean: {final_states.mean():.2f}')
ax.axvline(x=np.median(final_states), color='orange', linestyle='--', linewidth=2,
           label=f'Median: {np.median(final_states):.2f}')
ax.set_xlabel('Final State')
ax.set_ylabel('Density')
ax.set_title(f'Distribution of Final States (t={n_periods-1})')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Final States (t={n_periods-1}):")
print(f"  Mean:   {final_states.mean():.2f}")
print(f"  Std:    {final_states.std():.2f}")
print(f"  Min:    {final_states.min():.2f}")
print(f"  Max:    {final_states.max():.2f}")
```

## Summary

```{python}
#| label: summary

print("=" * 60)
print("SIMULATION SUMMARY")
print("=" * 60)

print(f"\nModel Parameters:")
print(f"  β = {beta} (reward coefficient)")
print(f"  γ = {gamma} (state decay rate)")
print(f"  δ = {delta} (discount factor)")

print(f"\nSimulation Setup:")
print(f"  Agents: {panel.n_agents}")
print(f"  Periods: {panel.n_periods}")
print(f"  Initial state: {s_init}")
print(f"  Random seed: {simulation_seed}")

print(f"\nKey Results:")
print(f"  Overall investment rate: {panel.actions.mean():.1%}")
print(f"  Stationary mean state: {panel.states[:, -20:].mean():.2f}")
print(f"  Choice probability RMSE: {rmse:.4f}")

print(f"\nThe simulation validates that:")
print(f"  ✓ State transitions follow s' = (1-γ)s + a")
print(f"  ✓ Empirical choice frequencies match theoretical logit probabilities")
print(f"  ✓ State distribution converges to a stationary distribution")
```

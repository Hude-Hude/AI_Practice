{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Structural Estimation for MDP\"\n",
        "subtitle: \"Parameter Recovery from Simulated Panel Data\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "---\n",
        "\n",
        "## Estimation Problem\n",
        "\n",
        "### Objective\n",
        "\n",
        "Given panel data $\\{(s_{it}, a_{it})\\}_{i=1,\\ldots,N; t=0,\\ldots,T-1}$ of states and actions, we seek to recover the structural parameters $\\theta = (\\beta, \\gamma, \\delta)$ that generated the data.\n",
        "\n",
        "### Model Specification\n",
        "\n",
        "The data-generating process follows the dynamic discrete choice model:\n",
        "\n",
        "- **State transition**: $s_{t+1} = (1 - \\gamma) s_t + a_t$ (deterministic)\n",
        "- **Flow reward**: $u(s, a) = \\beta \\log(1 + s) - a$\n",
        "- **Choice-specific value function**:\n",
        "$$\n",
        "v(s, a; \\theta) = u(s, a) + \\delta \\bar{V}((1-\\gamma)s + a; \\theta)\n",
        "$$\n",
        "- **Integrated value function** (log-sum-exp):\n",
        "$$\n",
        "\\bar{V}(s; \\theta) = \\log\\left( \\exp(v(s, 0; \\theta)) + \\exp(v(s, 1; \\theta)) \\right) + \\gamma_E\n",
        "$$\n",
        "where $\\gamma_E \\approx 0.5772$ is Euler's constant.\n",
        "\n",
        "### Choice Probability\n",
        "\n",
        "Under Type-I Extreme Value shocks, the probability of choosing action $a = 1$ given state $s$ is:\n",
        "$$\n",
        "P(a = 1 | s; \\theta) = \\frac{\\exp(v(s, 1; \\theta))}{\\exp(v(s, 0; \\theta)) + \\exp(v(s, 1; \\theta))} = \\frac{1}{1 + \\exp(v(s, 0; \\theta) - v(s, 1; \\theta))}\n",
        "$$\n",
        "\n",
        "This is the **logit formula** with choice probability determined by the value difference.\n",
        "\n",
        "## Maximum Likelihood Estimation\n",
        "\n",
        "### Likelihood Function\n",
        "\n",
        "The log-likelihood of observing the panel data is:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} \\log P(a_{it} | s_{it}; \\theta)\n",
        "$$\n",
        "\n",
        "Expanding the choice probability:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} \\left[ a_{it} \\cdot v(s_{it}, 1; \\theta) + (1 - a_{it}) \\cdot v(s_{it}, 0; \\theta) - \\log\\left( \\exp(v(s_{it}, 0; \\theta)) + \\exp(v(s_{it}, 1; \\theta)) \\right) \\right]\n",
        "$$\n",
        "\n",
        "Or equivalently:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} \\left[ a_{it} \\cdot \\Delta v(s_{it}; \\theta) - \\log\\left( 1 + \\exp(\\Delta v(s_{it}; \\theta)) \\right) \\right]\n",
        "$$\n",
        "where $\\Delta v(s; \\theta) = v(s, 1; \\theta) - v(s, 0; \\theta)$ is the value difference.\n",
        "\n",
        "### MLE Estimator\n",
        "\n",
        "The maximum likelihood estimator is:\n",
        "$$\n",
        "\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} \\mathcal{L}(\\theta)\n",
        "$$\n",
        "\n",
        "### Computational Challenge\n",
        "\n",
        "The key challenge is that evaluating $\\mathcal{L}(\\theta)$ requires computing the value functions $v(s, a; \\theta)$, which themselves depend on $\\theta$ through the Bellman equation. This creates a **nested optimization problem**.\n",
        "\n",
        "## Our Approach: NFXP with Existing Solver\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "We have already implemented a neural network-based value function solver in `mdp_solver`. This solver can be directly invoked for any candidate parameters $\\theta = (\\beta, \\gamma, \\delta)$. The estimation problem becomes straightforward:\n",
        "\n",
        "- **Inner loop**: Call our existing `solve_value_function(beta, gamma, delta, ...)` to obtain converged value networks\n",
        "- **Outer loop**: Use the value networks to compute choice probabilities and optimize the likelihood\n",
        "\n",
        "### Available Infrastructure\n",
        "\n",
        "From `src/mdp_solver`, we have:\n",
        "\n",
        "| Function | Purpose |\n",
        "|----------|---------|\n",
        "| `solve_value_function(beta, gamma, delta, ...)` | Solves Bellman iteration for given parameters |\n",
        "| `compute_choice_probability(v0_net, v1_net, s)` | Computes $P(a=1|s)$ from value networks |\n",
        "| `build_monotonic_network(hidden_sizes)` | Creates network architecture |\n",
        "| `evaluate_network(net, s)` | Evaluates value function at states |\n",
        "\n",
        "### Algorithm: Two-Step NFXP-NN Estimator\n",
        "\n",
        "Due to weak identification between parameters when estimating all three simultaneously, we adopt a **two-step approach**:\n",
        "\n",
        "1. **Step 1**: Estimate $\\gamma$ directly from transition data (OLS, no optimization needed)\n",
        "2. **Step 2**: Estimate $(\\beta, \\delta)$ via grid search with $\\gamma$ fixed\n",
        "\n",
        "This exploits the fact that $\\gamma$ enters only the deterministic transition equation, while $(\\beta, \\delta)$ enter the value function through the Bellman equation.\n",
        "\n",
        "#### Pseudocode: Step 1 - Estimate γ from Transitions (OLS)\n",
        "\n",
        "```\n",
        "FUNCTION ESTIMATE_GAMMA_OLS(data):\n",
        "    \"\"\"\n",
        "    Estimate γ directly from observed state transitions using OLS.\n",
        "    \n",
        "    The transition equation is deterministic:\n",
        "        s_{t+1} = (1 - γ) * s_t + a_t\n",
        "    \n",
        "    Rearranging:\n",
        "        (s_{t+1} - a_t) = (1 - γ) * s_t\n",
        "    \n",
        "    This is a regression through the origin:\n",
        "        Y = (1 - γ) * X\n",
        "    where Y = s_{t+1} - a_t, X = s_t\n",
        "    \n",
        "    Inputs:\n",
        "        data: PanelData object containing:\n",
        "            - states: np.ndarray of shape (n_agents, n_periods)\n",
        "            - actions: np.ndarray of shape (n_agents, n_periods)\n",
        "    \n",
        "    Outputs:\n",
        "        gamma_hat: float, estimated decay rate\n",
        "        std_error: float, standard error of estimate\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract current states, next states, and actions\n",
        "    s_current = data.states[:, :-1].flatten()    # s_t (exclude last period)\n",
        "    s_next = data.states[:, 1:].flatten()        # s_{t+1} (exclude first period)\n",
        "    a_current = data.actions[:, :-1].flatten()   # a_t (exclude last period)\n",
        "    \n",
        "    # Construct regression variables\n",
        "    Y = s_next - a_current    # Dependent variable\n",
        "    X = s_current             # Independent variable (regressor)\n",
        "    \n",
        "    # Filter out observations where s_t = 0 (division issue)\n",
        "    valid = X > 1e-6\n",
        "    Y = Y[valid]\n",
        "    X = X[valid]\n",
        "    \n",
        "    # OLS through origin: β = (X'X)^{-1} X'Y = Σ(X*Y) / Σ(X²)\n",
        "    coef = np.sum(X * Y) / np.sum(X ** 2)  # This estimates (1 - γ)\n",
        "    gamma_hat = 1 - coef\n",
        "    \n",
        "    # Compute standard error\n",
        "    residuals = Y - coef * X\n",
        "    n = len(X)\n",
        "    mse = np.sum(residuals ** 2) / (n - 1)  # Mean squared error\n",
        "    var_coef = mse / np.sum(X ** 2)         # Variance of coefficient\n",
        "    std_error = np.sqrt(var_coef)            # SE for (1-γ), same as SE for γ\n",
        "    \n",
        "    RETURN gamma_hat, std_error\n",
        "```\n",
        "\n",
        "#### Pseudocode: Step 2 - Grid Search for (β, δ) with γ Fixed\n",
        "\n",
        "```\n",
        "FUNCTION ESTIMATE_BETA_DELTA_GRID(data, gamma_fixed, solver_params, bounds, n_points):\n",
        "    \"\"\"\n",
        "    Estimate (β, δ) via 2D grid search with γ fixed.\n",
        "    \n",
        "    Inputs:\n",
        "        data: PanelData object\n",
        "        gamma_fixed: float, pre-estimated γ from Step 1\n",
        "        solver_params: dict with solver hyperparameters\n",
        "        bounds: dict with {'beta': (lo, hi), 'delta': (lo, hi)}\n",
        "        n_points: int, grid points per dimension\n",
        "    \n",
        "    Outputs:\n",
        "        beta_hat: float, estimated reward coefficient\n",
        "        delta_hat: float, estimated discount factor\n",
        "        log_likelihood: float, maximized log-likelihood\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create 2D grid\n",
        "    beta_grid = np.linspace(bounds['beta'][0], bounds['beta'][1], n_points)\n",
        "    delta_grid = np.linspace(bounds['delta'][0], bounds['delta'][1], n_points)\n",
        "    \n",
        "    best_ll = -np.inf\n",
        "    best_beta = None\n",
        "    best_delta = None\n",
        "    \n",
        "    # Search over grid\n",
        "    FOR beta IN beta_grid:\n",
        "        FOR delta IN delta_grid:\n",
        "            theta = (beta, gamma_fixed, delta)\n",
        "            ll = COMPUTE_LOG_LIKELIHOOD(theta, data, solver_params)\n",
        "            \n",
        "            IF ll > best_ll:\n",
        "                best_ll = ll\n",
        "                best_beta = beta\n",
        "                best_delta = delta\n",
        "    \n",
        "    RETURN best_beta, best_delta, best_ll\n",
        "```\n",
        "\n",
        "#### Pseudocode: Main Two-Step Estimation Routine\n",
        "\n",
        "```\n",
        "FUNCTION ESTIMATE_TWO_STEP(data, solver_params, bounds, n_points):\n",
        "    \"\"\"\n",
        "    Two-step structural estimation.\n",
        "    \n",
        "    Step 1: Estimate γ from transition data (OLS)\n",
        "    Step 2: Estimate (β, δ) via grid search with γ fixed\n",
        "    \n",
        "    Inputs:\n",
        "        data: PanelData object\n",
        "        solver_params: dict with solver hyperparameters\n",
        "        bounds: dict with parameter bounds\n",
        "        n_points: int, grid points per dimension for (β, δ)\n",
        "    \n",
        "    Outputs:\n",
        "        theta_hat: tuple (beta_hat, gamma_hat, delta_hat)\n",
        "        results: dict with estimation details\n",
        "    \"\"\"\n",
        "    \n",
        "    # Step 1: Estimate γ from transitions\n",
        "    gamma_hat, gamma_se = ESTIMATE_GAMMA_OLS(data)\n",
        "    PRINT \"Step 1: γ_hat =\", gamma_hat, \"± \", gamma_se\n",
        "    \n",
        "    # Step 2: Estimate (β, δ) with γ fixed\n",
        "    beta_hat, delta_hat, log_lik = ESTIMATE_BETA_DELTA_GRID(\n",
        "        data, gamma_hat, solver_params, bounds, n_points\n",
        "    )\n",
        "    PRINT \"Step 2: β_hat =\", beta_hat, \", δ_hat =\", delta_hat\n",
        "    \n",
        "    theta_hat = (beta_hat, gamma_hat, delta_hat)\n",
        "    RETURN theta_hat, {'gamma_se': gamma_se, 'log_likelihood': log_lik}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Alternative: Joint 3-Parameter Estimation (For Reference)\n",
        "\n",
        "The following pseudocode shows joint estimation of all three parameters, which suffers from identification issues as demonstrated above.\n",
        "\n",
        "#### Pseudocode: Joint Estimation (Not Recommended)\n",
        "\n",
        "```\n",
        "FUNCTION ESTIMATE_MLE(data, theta_init, solver_params, bounds):\n",
        "    \"\"\"\n",
        "    Estimate structural parameters via Maximum Likelihood.\n",
        "    \n",
        "    Inputs:\n",
        "        data: PanelData object containing:\n",
        "            - states: np.ndarray of shape (n_agents, n_periods)\n",
        "            - actions: np.ndarray of shape (n_agents, n_periods)\n",
        "        theta_init: tuple (beta_init, gamma_init, delta_init)\n",
        "        solver_params: dict containing:\n",
        "            - s_min: float, minimum state value\n",
        "            - s_max: float, maximum state value\n",
        "            - hidden_sizes: list[int], network architecture\n",
        "            - learning_rate: float\n",
        "            - batch_size: int\n",
        "            - tolerance: float, convergence tolerance for inner loop\n",
        "            - max_iterations: int, max iterations for inner loop\n",
        "            - target_update_freq: int\n",
        "        bounds: list of tuples [(beta_lo, beta_hi), (gamma_lo, gamma_hi), (delta_lo, delta_hi)]\n",
        "    \n",
        "    Outputs:\n",
        "        theta_hat: tuple (beta_hat, gamma_hat, delta_hat)\n",
        "        result: scipy.optimize.OptimizeResult object\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define objective function (negative log-likelihood)\n",
        "    FUNCTION neg_log_likelihood(theta):\n",
        "        log_lik = COMPUTE_LOG_LIKELIHOOD(theta, data, solver_params)\n",
        "        RETURN -log_lik\n",
        "    \n",
        "    # Run Nelder-Mead optimization\n",
        "    result = scipy.optimize.minimize(\n",
        "        fun=neg_log_likelihood,\n",
        "        x0=theta_init,\n",
        "        method='Nelder-Mead',\n",
        "        options={\n",
        "            'maxiter': 200,\n",
        "            'xatol': 1e-4,      # Parameter tolerance\n",
        "            'fatol': 1e-4,      # Function value tolerance\n",
        "            'disp': True,\n",
        "            'adaptive': True    # Adapt simplex to parameter scales\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    theta_hat = result.x\n",
        "    RETURN theta_hat, result\n",
        "```\n",
        "\n",
        "#### Pseudocode: Log-Likelihood Computation (Inner Loop)\n",
        "\n",
        "```\n",
        "FUNCTION COMPUTE_LOG_LIKELIHOOD(theta, data, solver_params):\n",
        "    \"\"\"\n",
        "    Compute log-likelihood for candidate parameters.\n",
        "    This is called once per outer loop iteration.\n",
        "    \n",
        "    Inputs:\n",
        "        theta: tuple (beta, gamma, delta)\n",
        "        data: PanelData object\n",
        "        solver_params: dict with solver hyperparameters\n",
        "    \n",
        "    Outputs:\n",
        "        log_lik: float, total log-likelihood\n",
        "    \"\"\"\n",
        "    \n",
        "    # Unpack parameters\n",
        "    beta, gamma, delta = theta\n",
        "    \n",
        "    # Validate parameter bounds (return -inf for invalid)\n",
        "    IF beta <= 0 OR gamma <= 0 OR gamma >= 1 OR delta <= 0 OR delta >= 1:\n",
        "        RETURN -np.inf\n",
        "    \n",
        "    # === INNER LOOP: Solve value functions for this theta ===\n",
        "    v0_net, v1_net, losses, n_iter = solve_value_function(\n",
        "        beta=beta,\n",
        "        gamma=gamma,\n",
        "        delta=delta,\n",
        "        s_min=solver_params['s_min'],\n",
        "        s_max=solver_params['s_max'],\n",
        "        hidden_sizes=solver_params['hidden_sizes'],\n",
        "        learning_rate=solver_params['learning_rate'],\n",
        "        batch_size=solver_params['batch_size'],\n",
        "        tolerance=solver_params['tolerance'],\n",
        "        max_iterations=solver_params['max_iterations'],\n",
        "        target_update_freq=solver_params['target_update_freq']\n",
        "    )\n",
        "    \n",
        "    # Check convergence (optional: warn if not converged)\n",
        "    IF n_iter == solver_params['max_iterations']:\n",
        "        PRINT warning: \"Inner loop did not converge for theta =\", theta\n",
        "    \n",
        "    # === Compute choice probabilities at all observed states ===\n",
        "    # Flatten panel data for batch computation\n",
        "    states_flat = data.states.flatten()  # shape: (N * T,)\n",
        "    actions_flat = data.actions.flatten()  # shape: (N * T,)\n",
        "    \n",
        "    # Convert to tensor\n",
        "    s_tensor = torch.tensor(states_flat, dtype=torch.float32)\n",
        "    \n",
        "    # Compute P(a=1 | s) for all observations\n",
        "    WITH torch.no_grad():\n",
        "        p1 = compute_choice_probability(v0_net, v1_net, s_tensor)\n",
        "        p1 = p1.numpy()\n",
        "    \n",
        "    # Clip probabilities for numerical stability\n",
        "    eps = 1e-10\n",
        "    p1 = np.clip(p1, eps, 1 - eps)\n",
        "    \n",
        "    # === Compute log-likelihood ===\n",
        "    # L = Σ [a * log(p1) + (1-a) * log(1-p1)]\n",
        "    log_lik = np.sum(\n",
        "        actions_flat * np.log(p1) + \n",
        "        (1 - actions_flat) * np.log(1 - p1)\n",
        "    )\n",
        "    \n",
        "    RETURN log_lik\n",
        "```\n",
        "\n",
        "#### Pseudocode: Standard Error Computation\n",
        "\n",
        "```\n",
        "FUNCTION COMPUTE_STANDARD_ERRORS(theta_hat, data, solver_params, eps=1e-4):\n",
        "    \"\"\"\n",
        "    Compute standard errors via numerical Hessian at the MLE.\n",
        "    \n",
        "    Inputs:\n",
        "        theta_hat: tuple (beta_hat, gamma_hat, delta_hat), MLE estimates\n",
        "        data: PanelData object\n",
        "        solver_params: dict\n",
        "        eps: float, step size for finite differences\n",
        "    \n",
        "    Outputs:\n",
        "        std_errors: np.ndarray of shape (3,)\n",
        "        cov_matrix: np.ndarray of shape (3, 3)\n",
        "    \"\"\"\n",
        "    \n",
        "    n_params = 3\n",
        "    theta_hat = np.array(theta_hat)\n",
        "    \n",
        "    # Compute numerical Hessian via central differences\n",
        "    hessian = np.zeros((n_params, n_params))\n",
        "    \n",
        "    FOR i IN range(n_params):\n",
        "        FOR j IN range(n_params):\n",
        "            # f(θ + ei*eps + ej*eps)\n",
        "            theta_pp = theta_hat.copy()\n",
        "            theta_pp[i] += eps\n",
        "            theta_pp[j] += eps\n",
        "            f_pp = COMPUTE_LOG_LIKELIHOOD(theta_pp, data, solver_params)\n",
        "            \n",
        "            # f(θ + ei*eps - ej*eps)\n",
        "            theta_pm = theta_hat.copy()\n",
        "            theta_pm[i] += eps\n",
        "            theta_pm[j] -= eps\n",
        "            f_pm = COMPUTE_LOG_LIKELIHOOD(theta_pm, data, solver_params)\n",
        "            \n",
        "            # f(θ - ei*eps + ej*eps)\n",
        "            theta_mp = theta_hat.copy()\n",
        "            theta_mp[i] -= eps\n",
        "            theta_mp[j] += eps\n",
        "            f_mp = COMPUTE_LOG_LIKELIHOOD(theta_mp, data, solver_params)\n",
        "            \n",
        "            # f(θ - ei*eps - ej*eps)\n",
        "            theta_mm = theta_hat.copy()\n",
        "            theta_mm[i] -= eps\n",
        "            theta_mm[j] -= eps\n",
        "            f_mm = COMPUTE_LOG_LIKELIHOOD(theta_mm, data, solver_params)\n",
        "            \n",
        "            # Second derivative approximation\n",
        "            hessian[i, j] = (f_pp - f_pm - f_mp + f_mm) / (4 * eps * eps)\n",
        "    \n",
        "    # Covariance matrix = inverse of negative Hessian (information matrix)\n",
        "    info_matrix = -hessian\n",
        "    cov_matrix = np.linalg.inv(info_matrix)\n",
        "    \n",
        "    # Standard errors = sqrt of diagonal\n",
        "    std_errors = np.sqrt(np.diag(cov_matrix))\n",
        "    \n",
        "    RETURN std_errors, cov_matrix\n",
        "```\n",
        "\n",
        "### Why Nelder-Mead\n",
        "\n",
        "For our 3-parameter problem, Nelder-Mead is the practical choice:\n",
        "\n",
        "1. **No gradients required**: Computing $\\partial \\mathcal{L}/\\partial \\theta$ would require differentiating through thousands of inner loop iterations — complex and numerically unstable.\n",
        "\n",
        "2. **Robust**: Nelder-Mead handles non-smooth objective functions and doesn't get stuck on saddle points.\n",
        "\n",
        "3. **Sufficient for low dimensions**: With only 3 parameters, Nelder-Mead converges in reasonable time (~50-200 function evaluations).\n",
        "\n",
        "4. **Each evaluation is expensive**: Our inner loop (solving the DP) dominates computation time. Nelder-Mead minimizes the number of evaluations.\n",
        "\n",
        "### Computational Cost\n",
        "\n",
        "Each outer loop iteration requires:\n",
        "- 1 call to `solve_value_function` (inner loop): ~1000-10000 iterations of neural network training\n",
        "- 1 batch evaluation of choice probabilities: O(N × T) forward passes\n",
        "\n",
        "Total estimation: ~50-200 outer iterations × inner loop cost\n",
        "\n",
        "## Identification\n",
        "\n",
        "### Identification Conditions\n",
        "\n",
        "For the parameters $(\\beta, \\gamma, \\delta)$ to be identified from choice data:\n",
        "\n",
        "1. **Reward normalization**: One parameter must be normalized. Typically, the coefficient on the action cost is set to 1, so the reward is $u(s,a) = \\beta \\log(1+s) - a$ (the $-a$ term is normalized to have coefficient -1).\n",
        "\n",
        "2. **Variation in states**: The data must contain sufficient variation in states $s$ to identify $\\beta$ from the state-dependent reward component $\\beta \\log(1+s)$.\n",
        "\n",
        "3. **Discount factor**: $\\delta$ is identified from the forward-looking behavior. Higher $\\delta$ means agents put more weight on future states, affecting current choices.\n",
        "\n",
        "4. **Transition parameter**: $\\gamma$ is directly identified from observed state transitions:\n",
        "$$\n",
        "\\gamma = 1 - \\frac{s_{t+1} - a_t}{s_t} \\quad \\text{(when } s_t \\neq 0 \\text{)}\n",
        "$$\n",
        "\n",
        "### Exclusion Restrictions\n",
        "\n",
        "In our model:\n",
        "- $\\gamma$ enters only the transition equation, not the flow reward\n",
        "- $\\beta$ enters only the flow reward, not the transition\n",
        "- $\\delta$ appears only in the discounting of future values\n",
        "\n",
        "This separation aids identification.\n",
        "\n",
        "### Special Case: Direct $\\gamma$ Estimation\n",
        "\n",
        "Since state transitions are deterministic, $\\gamma$ can be estimated directly from observed transitions without solving the DP problem:\n",
        "$$\n",
        "\\hat{\\gamma} = 1 - \\frac{1}{|\\mathcal{D}|} \\sum_{(s_t, a_t, s_{t+1}) \\in \\mathcal{D}} \\frac{s_{t+1} - a_t}{s_t}\n",
        "$$\n",
        "\n",
        "This reduces the estimation problem to finding $(\\beta, \\delta)$ given known $\\gamma$.\n",
        "\n",
        "## Score and Hessian\n",
        "\n",
        "### Score Function\n",
        "\n",
        "The score (gradient of log-likelihood) with respect to $\\theta$ is:\n",
        "$$\n",
        "S(\\theta) = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\sum_{i,t} \\left( a_{it} - P(a=1|s_{it}; \\theta) \\right) \\cdot \\frac{\\partial \\Delta v(s_{it}; \\theta)}{\\partial \\theta}\n",
        "$$\n",
        "\n",
        "This has the intuitive form: prediction error $\\times$ sensitivity of value difference to parameters.\n",
        "\n",
        "### Information Matrix\n",
        "\n",
        "The expected (Fisher) information matrix is:\n",
        "$$\n",
        "\\mathcal{I}(\\theta) = -\\mathbb{E}\\left[ \\frac{\\partial^2 \\mathcal{L}}{\\partial \\theta \\partial \\theta'} \\right] = \\sum_{i,t} P(1-P) \\cdot \\frac{\\partial \\Delta v}{\\partial \\theta} \\cdot \\frac{\\partial \\Delta v}{\\partial \\theta'}\n",
        "$$\n",
        "where $P = P(a=1|s_{it}; \\theta)$.\n",
        "\n",
        "### Asymptotic Distribution\n",
        "\n",
        "Under regularity conditions, the MLE is asymptotically normal:\n",
        "$$\n",
        "\\sqrt{NT}(\\hat{\\theta} - \\theta_0) \\xrightarrow{d} \\mathcal{N}(0, \\mathcal{I}(\\theta_0)^{-1})\n",
        "$$\n",
        "\n",
        "Standard errors can be computed from the inverse of the estimated information matrix.\n",
        "\n",
        "### Numerical Standard Errors\n",
        "\n",
        "For derivative-free optimization, standard errors can be computed via:\n",
        "1. **Numerical Hessian**: Finite differences around $\\hat{\\theta}$\n",
        "2. **Bootstrap**: Resample the panel data and re-estimate\n",
        "3. **Outer product of gradients (OPG)**: If numerical gradients are available\n",
        "\n",
        "## Alternative Approaches (For Reference)\n",
        "\n",
        "### Alternative 1: Hotz-Miller CCP Inversion\n",
        "\n",
        "Instead of solving the full dynamic problem, Hotz and Miller (1993) showed that value differences can be expressed in terms of observable CCPs:\n",
        "$$\n",
        "v(s, 1) - v(s, 0) = \\log\\left( \\frac{P(a=1|s)}{P(a=0|s)} \\right)\n",
        "$$\n",
        "\n",
        "**Two-Step Estimation**:\n",
        "\n",
        "1. **First stage**: Estimate CCPs non-parametrically from the data:\n",
        "$$\n",
        "\\hat{P}(a=1|s) = \\frac{\\sum_{i,t} \\mathbf{1}[a_{it}=1, s_{it} \\approx s]}{\\sum_{i,t} \\mathbf{1}[s_{it} \\approx s]}\n",
        "$$\n",
        "\n",
        "2. **Second stage**: Use the inverted CCPs to form moment conditions and estimate structural parameters.\n",
        "\n",
        "**Pros**: Avoids solving the Bellman equation repeatedly\n",
        "**Cons**: Requires accurate first-stage CCP estimates, which introduces noise — particularly problematic with limited data or sparse state coverage\n",
        "\n",
        "### Alternative 2: Simulation-Based Estimation (Indirect Inference)\n",
        "\n",
        "Simulate data from the model at candidate parameters and match moments:\n",
        "$$\n",
        "\\hat{\\theta} = \\arg\\min_{\\theta} \\left( m(\\text{data}) - m(\\text{simulated}(\\theta)) \\right)' W \\left( m(\\text{data}) - m(\\text{simulated}(\\theta)) \\right)\n",
        "$$\n",
        "\n",
        "**Pros**: Flexible, can match any computable moments\n",
        "**Cons**: Requires many simulations, choice of moments affects efficiency\n",
        "\n",
        "### Alternative 3: Bayesian Estimation\n",
        "\n",
        "Place priors on $\\theta$ and compute posterior:\n",
        "$$\n",
        "p(\\theta | \\text{data}) \\propto \\mathcal{L}(\\theta) \\cdot p(\\theta)\n",
        "$$\n",
        "\n",
        "**Pros**: Quantifies parameter uncertainty, regularizes estimation\n",
        "**Cons**: Computationally intensive (MCMC), requires prior specification\n",
        "\n",
        "## Summary of Estimation Approaches\n",
        "\n",
        "| Approach | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| **NFXP-NN (Our Approach)** | Reuses existing solver, exact probabilities, no CCP noise | Requires solving DP per evaluation |\n",
        "| **Hotz-Miller CCP** | Avoids repeated DP | Noisy CCP estimates, binning artifacts |\n",
        "| **Indirect Inference** | Flexible moment matching | Many simulations needed |\n",
        "| **Bayesian** | Full uncertainty quantification | MCMC computational cost |\n",
        "\n",
        "## Implementation Roadmap\n",
        "\n",
        "### Module Structure\n",
        "\n",
        "```\n",
        "src/mdp_estimator/\n",
        "├── __init__.py\n",
        "├── mdp_estimator.py      # Core estimation functions\n",
        "└── utils.py              # Helper functions (numerical Hessian, etc.)\n",
        "\n",
        "test/mdp_estimator/\n",
        "└── test_estimator.py     # Unit and integration tests\n",
        "```\n",
        "\n",
        "### Step 1: Create Estimator Module\n",
        "\n",
        "File: `src/mdp_estimator/mdp_estimator.py`\n",
        "\n",
        "```python\n",
        "\"\"\"MDP Structural Parameter Estimator using NFXP.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import optimize\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, Any, Optional\n",
        "\n",
        "from mdp_solver import solve_value_function, compute_choice_probability\n",
        "from mdp_simulator import PanelData\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EstimationResult:\n",
        "    \"\"\"Container for estimation results.\"\"\"\n",
        "    theta_hat: np.ndarray          # (beta, gamma, delta)\n",
        "    std_errors: np.ndarray         # Standard errors\n",
        "    cov_matrix: np.ndarray         # Covariance matrix\n",
        "    log_likelihood: float          # Log-likelihood at optimum\n",
        "    n_iterations: int              # Outer loop iterations\n",
        "    converged: bool                # Whether optimizer converged\n",
        "    optimization_result: Any       # Full scipy result object\n",
        "\n",
        "\n",
        "def compute_log_likelihood(\n",
        "    theta: np.ndarray,\n",
        "    data: PanelData,\n",
        "    solver_params: Dict[str, Any],\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute log-likelihood for candidate parameters.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    theta : np.ndarray\n",
        "        Parameters (beta, gamma, delta)\n",
        "    data : PanelData\n",
        "        Panel data with states and actions\n",
        "    solver_params : dict\n",
        "        Solver hyperparameters (s_min, s_max, hidden_sizes, etc.)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Log-likelihood value\n",
        "    \"\"\"\n",
        "    beta, gamma, delta = theta\n",
        "    \n",
        "    # Validate bounds\n",
        "    if beta <= 0 or gamma <= 0 or gamma >= 1 or delta <= 0 or delta >= 1:\n",
        "        return -np.inf\n",
        "    \n",
        "    # Inner loop: solve for value functions\n",
        "    v0_net, v1_net, losses, n_iter = solve_value_function(\n",
        "        beta=beta,\n",
        "        gamma=gamma,\n",
        "        delta=delta,\n",
        "        s_min=solver_params['s_min'],\n",
        "        s_max=solver_params['s_max'],\n",
        "        hidden_sizes=solver_params['hidden_sizes'],\n",
        "        learning_rate=solver_params['learning_rate'],\n",
        "        batch_size=solver_params['batch_size'],\n",
        "        tolerance=solver_params['tolerance'],\n",
        "        max_iterations=solver_params['max_iterations'],\n",
        "        target_update_freq=solver_params.get('target_update_freq', 100),\n",
        "    )\n",
        "    \n",
        "    # Compute choice probabilities\n",
        "    states_flat = data.states.flatten()\n",
        "    actions_flat = data.actions.flatten()\n",
        "    s_tensor = torch.tensor(states_flat, dtype=torch.float32)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        p1 = compute_choice_probability(v0_net, v1_net, s_tensor).numpy()\n",
        "    \n",
        "    # Clip for numerical stability\n",
        "    eps = 1e-10\n",
        "    p1 = np.clip(p1, eps, 1 - eps)\n",
        "    \n",
        "    # Log-likelihood\n",
        "    log_lik = np.sum(\n",
        "        actions_flat * np.log(p1) + \n",
        "        (1 - actions_flat) * np.log(1 - p1)\n",
        "    )\n",
        "    \n",
        "    return log_lik\n",
        "\n",
        "\n",
        "def estimate_mle(\n",
        "    data: PanelData,\n",
        "    theta_init: Tuple[float, float, float],\n",
        "    solver_params: Dict[str, Any],\n",
        "    maxiter: int = 200,\n",
        "    verbose: bool = True,\n",
        ") -> EstimationResult:\n",
        "    \"\"\"\n",
        "    Estimate structural parameters via MLE using NFXP.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : PanelData\n",
        "        Panel data with states and actions\n",
        "    theta_init : tuple\n",
        "        Initial guess (beta, gamma, delta)\n",
        "    solver_params : dict\n",
        "        Solver hyperparameters\n",
        "    maxiter : int\n",
        "        Maximum outer loop iterations\n",
        "    verbose : bool\n",
        "        Print progress\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    EstimationResult\n",
        "        Estimation results including estimates and standard errors\n",
        "    \"\"\"\n",
        "    theta_init = np.array(theta_init)\n",
        "    \n",
        "    # Track evaluations\n",
        "    eval_count = [0]\n",
        "    \n",
        "    def neg_log_lik(theta):\n",
        "        eval_count[0] += 1\n",
        "        ll = compute_log_likelihood(theta, data, solver_params)\n",
        "        if verbose and eval_count[0] % 10 == 0:\n",
        "            print(f\"Eval {eval_count[0]}: theta={theta}, LL={ll:.2f}\")\n",
        "        return -ll\n",
        "    \n",
        "    # Run optimization\n",
        "    result = optimize.minimize(\n",
        "        neg_log_lik,\n",
        "        theta_init,\n",
        "        method='Nelder-Mead',\n",
        "        options={\n",
        "            'maxiter': maxiter,\n",
        "            'xatol': 1e-4,\n",
        "            'fatol': 1e-4,\n",
        "            'disp': verbose,\n",
        "            'adaptive': True,\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    theta_hat = result.x\n",
        "    log_lik_hat = -result.fun\n",
        "    \n",
        "    # Compute standard errors\n",
        "    if verbose:\n",
        "        print(\"Computing standard errors...\")\n",
        "    std_errors, cov_matrix = compute_standard_errors(\n",
        "        theta_hat, data, solver_params\n",
        "    )\n",
        "    \n",
        "    return EstimationResult(\n",
        "        theta_hat=theta_hat,\n",
        "        std_errors=std_errors,\n",
        "        cov_matrix=cov_matrix,\n",
        "        log_likelihood=log_lik_hat,\n",
        "        n_iterations=result.nit,\n",
        "        converged=result.success,\n",
        "        optimization_result=result,\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_standard_errors(\n",
        "    theta_hat: np.ndarray,\n",
        "    data: PanelData,\n",
        "    solver_params: Dict[str, Any],\n",
        "    eps: float = 1e-4,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute standard errors via numerical Hessian.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    theta_hat : np.ndarray\n",
        "        MLE estimates\n",
        "    data : PanelData\n",
        "        Panel data\n",
        "    solver_params : dict\n",
        "        Solver hyperparameters\n",
        "    eps : float\n",
        "        Step size for finite differences\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    std_errors : np.ndarray\n",
        "        Standard errors for each parameter\n",
        "    cov_matrix : np.ndarray\n",
        "        Variance-covariance matrix\n",
        "    \"\"\"\n",
        "    n_params = len(theta_hat)\n",
        "    hessian = np.zeros((n_params, n_params))\n",
        "    \n",
        "    for i in range(n_params):\n",
        "        for j in range(i, n_params):  # Exploit symmetry\n",
        "            theta_pp = theta_hat.copy()\n",
        "            theta_pp[i] += eps\n",
        "            theta_pp[j] += eps\n",
        "            \n",
        "            theta_pm = theta_hat.copy()\n",
        "            theta_pm[i] += eps\n",
        "            theta_pm[j] -= eps\n",
        "            \n",
        "            theta_mp = theta_hat.copy()\n",
        "            theta_mp[i] -= eps\n",
        "            theta_mp[j] += eps\n",
        "            \n",
        "            theta_mm = theta_hat.copy()\n",
        "            theta_mm[i] -= eps\n",
        "            theta_mm[j] -= eps\n",
        "            \n",
        "            f_pp = compute_log_likelihood(theta_pp, data, solver_params)\n",
        "            f_pm = compute_log_likelihood(theta_pm, data, solver_params)\n",
        "            f_mp = compute_log_likelihood(theta_mp, data, solver_params)\n",
        "            f_mm = compute_log_likelihood(theta_mm, data, solver_params)\n",
        "            \n",
        "            hessian[i, j] = (f_pp - f_pm - f_mp + f_mm) / (4 * eps * eps)\n",
        "            hessian[j, i] = hessian[i, j]  # Symmetric\n",
        "    \n",
        "    # Information matrix = -Hessian\n",
        "    info_matrix = -hessian\n",
        "    \n",
        "    # Covariance = inverse of information matrix\n",
        "    try:\n",
        "        cov_matrix = np.linalg.inv(info_matrix)\n",
        "        std_errors = np.sqrt(np.diag(cov_matrix))\n",
        "    except np.linalg.LinAlgError:\n",
        "        # Singular matrix - return NaN\n",
        "        cov_matrix = np.full((n_params, n_params), np.nan)\n",
        "        std_errors = np.full(n_params, np.nan)\n",
        "    \n",
        "    return std_errors, cov_matrix\n",
        "```\n",
        "\n",
        "### Step 2: Add Tests\n",
        "\n",
        "File: `test/mdp_estimator/test_estimator.py`\n",
        "\n",
        "Key tests to implement:\n",
        "1. **Unit test**: `compute_log_likelihood` returns correct sign and magnitude\n",
        "2. **Unit test**: Standard errors are positive and reasonable\n",
        "3. **Integration test**: Recover true parameters from simulated data\n",
        "4. **Convergence test**: Optimizer converges for well-specified problems\n",
        "\n",
        "### Step 3: Integration in Quarto Report\n",
        "\n",
        "```python\n",
        "# In estimate_mdp.qmd\n",
        "\n",
        "# Load simulated data\n",
        "from mdp_simulator import simulate_mdp_panel\n",
        "from mdp_estimator import estimate_mle, EstimationResult\n",
        "\n",
        "# Simulate data at true parameters\n",
        "data = simulate_mdp_panel(...)\n",
        "\n",
        "# Estimate with perturbed initial values\n",
        "theta_init = (beta * 1.2, gamma * 0.8, delta * 0.95)  # Perturbed\n",
        "result = estimate_mle(data, theta_init, solver_params)\n",
        "\n",
        "# Display results\n",
        "print(f\"True:      β={beta:.3f}, γ={gamma:.3f}, δ={delta:.3f}\")\n",
        "print(f\"Estimated: β={result.theta_hat[0]:.3f}, γ={result.theta_hat[1]:.3f}, δ={result.theta_hat[2]:.3f}\")\n",
        "print(f\"Std Err:   ({result.std_errors[0]:.3f}, {result.std_errors[1]:.3f}, {result.std_errors[2]:.3f})\")\n",
        "```\n",
        "\n",
        "## Implementation\n",
        "\n",
        "### Setup"
      ],
      "id": "3d8b0163"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "#| code-fold: true\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Add paths for local modules\n",
        "sys.path.insert(0, '../../src')\n",
        "sys.path.insert(0, '../config_mdp')\n",
        "\n",
        "# Import configuration\n",
        "from config import (\n",
        "    beta as true_beta,\n",
        "    gamma as true_gamma,\n",
        "    delta as true_delta,\n",
        "    s_min, s_max,\n",
        "    hidden_sizes,\n",
        "    learning_rate,\n",
        "    batch_size,\n",
        "    tolerance,\n",
        "    max_iterations,\n",
        "    target_update_freq,\n",
        ")\n",
        "\n",
        "# Import estimator\n",
        "from mdp_estimator import grid_search_mle, EstimationResult\n",
        "from mdp_simulator import PanelData\n",
        "\n",
        "# Paths\n",
        "SIMULATE_OUTPUT_PATH = '../../output/simulate_mdp'\n",
        "SOLVER_OUTPUT_PATH = '../../output/solve_mdp'\n",
        "\n",
        "print(\"Setup complete.\")\n",
        "print(f\"True parameters: β={true_beta}, γ={true_gamma}, δ={true_delta}\")"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Simulated Data"
      ],
      "id": "ff80a3a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-data\n",
        "\n",
        "# Load simulated panel data\n",
        "states = np.load(f'{SIMULATE_OUTPUT_PATH}/states.npy')\n",
        "actions = np.load(f'{SIMULATE_OUTPUT_PATH}/actions.npy')\n",
        "rewards = np.load(f'{SIMULATE_OUTPUT_PATH}/rewards.npy')\n",
        "\n",
        "# Load simulation config\n",
        "with open(f'{SIMULATE_OUTPUT_PATH}/config.json', 'r') as f:\n",
        "    sim_config = json.load(f)\n",
        "\n",
        "n_agents = states.shape[0]\n",
        "n_periods = states.shape[1]\n",
        "\n",
        "# Create PanelData object\n",
        "panel_data = PanelData(\n",
        "    states=states,\n",
        "    actions=actions,\n",
        "    rewards=rewards,\n",
        "    n_agents=n_agents,\n",
        "    n_periods=n_periods,\n",
        ")\n",
        "\n",
        "print(f\"Loaded panel data:\")\n",
        "print(f\"  N agents:  {n_agents}\")\n",
        "print(f\"  T periods: {n_periods}\")\n",
        "print(f\"  Total observations: {n_agents * n_periods}\")"
      ],
      "id": "load-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### True Parameters (Ground Truth)"
      ],
      "id": "8777868f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: true-params\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"TRUE PARAMETERS (Data-Generating Process)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  β (reward coefficient):  {true_beta}\")\n",
        "print(f\"  γ (state decay rate):    {true_gamma}\")\n",
        "print(f\"  δ (discount factor):     {true_delta}\")\n",
        "print(\"=\" * 50)"
      ],
      "id": "true-params",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grid Search Estimation"
      ],
      "id": "b54dd39d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: grid-search\n",
        "\n",
        "# Solver parameters for inner loop\n",
        "solver_params = {\n",
        "    's_min': s_min,\n",
        "    's_max': s_max,\n",
        "    'hidden_sizes': hidden_sizes,\n",
        "    'learning_rate': learning_rate,\n",
        "    'batch_size': batch_size,\n",
        "    'tolerance': tolerance,\n",
        "    'max_iterations': max_iterations,\n",
        "    'target_update_freq': target_update_freq,\n",
        "}\n",
        "\n",
        "# Grid search bounds (centered around true values with reasonable range)\n",
        "bounds = {\n",
        "    'beta': (0.5, 1.5),      # True: 1.0\n",
        "    'gamma': (0.05, 0.20),   # True: 0.1\n",
        "    'delta': (0.90, 0.99),   # True: 0.95\n",
        "}\n",
        "\n",
        "# Number of grid points per dimension\n",
        "n_points = 5  # 5^3 = 125 evaluations\n",
        "\n",
        "print(\"Starting grid search estimation...\")\n",
        "print(f\"Grid: {n_points} points per dimension = {n_points**3} total evaluations\")\n",
        "print(f\"Using warm-start from: {SOLVER_OUTPUT_PATH}\")\n",
        "print()\n",
        "\n",
        "# Run grid search with warm-start\n",
        "result = grid_search_mle(\n",
        "    data=panel_data,\n",
        "    bounds=bounds,\n",
        "    n_points=n_points,\n",
        "    solver_params=solver_params,\n",
        "    verbose=True,\n",
        "    compute_se=True,\n",
        "    pretrained_path=SOLVER_OUTPUT_PATH,\n",
        ")\n",
        "\n",
        "print(\"\\nGrid search completed!\")"
      ],
      "id": "grid-search",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimation Results"
      ],
      "id": "30861b56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: results\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Extract results\n",
        "beta_hat, gamma_hat, delta_hat = result.theta_hat\n",
        "se_beta, se_gamma, se_delta = result.std_errors\n",
        "\n",
        "# Create results table\n",
        "results_df = pd.DataFrame({\n",
        "    'Parameter': ['β (reward)', 'γ (decay)', 'δ (discount)'],\n",
        "    'True': [true_beta, true_gamma, true_delta],\n",
        "    'Estimated': [beta_hat, gamma_hat, delta_hat],\n",
        "    'Std. Error': [se_beta, se_gamma, se_delta],\n",
        "    'Bias': [beta_hat - true_beta, gamma_hat - true_gamma, delta_hat - true_delta],\n",
        "    'Bias (%)': [\n",
        "        100 * (beta_hat - true_beta) / true_beta,\n",
        "        100 * (gamma_hat - true_gamma) / true_gamma,\n",
        "        100 * (delta_hat - true_delta) / true_delta,\n",
        "    ],\n",
        "})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ESTIMATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nLog-likelihood at optimum: {result.log_likelihood:.2f}\")\n",
        "print(f\"Number of evaluations: {result.n_iterations}\")"
      ],
      "id": "results",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Likelihood at True Parameters"
      ],
      "id": "4dde79dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: likelihood-check\n",
        "\n",
        "from mdp_estimator import compute_log_likelihood\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load pre-trained networks for warm-start\n",
        "v0_init_state = torch.load(f'{SOLVER_OUTPUT_PATH}/v0_net.pt', weights_only=True)\n",
        "v1_init_state = torch.load(f'{SOLVER_OUTPUT_PATH}/v1_net.pt', weights_only=True)\n",
        "\n",
        "# Compute log-likelihood at TRUE parameters\n",
        "theta_true = np.array([true_beta, true_gamma, true_delta])\n",
        "\n",
        "print(f\"Evaluating log-likelihood at true parameters: θ = {theta_true}\")\n",
        "ll_true = compute_log_likelihood(\n",
        "    theta=theta_true,\n",
        "    data=panel_data,\n",
        "    solver_params=solver_params,\n",
        "    v0_init_state=v0_init_state,\n",
        "    v1_init_state=v1_init_state,\n",
        ")\n",
        "\n",
        "print(f\"\\nLog-likelihood comparison:\")\n",
        "print(f\"  At TRUE parameters (β={true_beta}, γ={true_gamma}, δ={true_delta}):  LL = {ll_true:.2f}\")\n",
        "print(f\"  At ESTIMATED parameters (β={beta_hat:.2f}, γ={gamma_hat:.2f}, δ={delta_hat:.2f}): LL = {result.log_likelihood:.2f}\")\n",
        "print(f\"  Difference: {ll_true - result.log_likelihood:.2f}\")\n",
        "\n",
        "if ll_true > result.log_likelihood:\n",
        "    print(\"\\n⚠ TRUE parameters have HIGHER likelihood!\")\n",
        "    print(\"  → Grid search missed the true maximum (grid too coarse)\")\n",
        "elif abs(ll_true - result.log_likelihood) < 10:\n",
        "    print(\"\\n⚠ Likelihoods are SIMILAR (within 10 log-units)\")\n",
        "    print(\"  → Likelihood surface is FLAT - identification issues\")\n",
        "else:\n",
        "    print(\"\\n✓ Estimated parameters have higher likelihood\")\n",
        "    print(\"  → Model may be misspecified or data insufficient\")"
      ],
      "id": "likelihood-check",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnostics"
      ],
      "id": "41cf94d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: diagnostics\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DIAGNOSTICS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if estimates are within 2 standard errors of true values\n",
        "within_2se = []\n",
        "for i, (name, true_val, est_val, se) in enumerate([\n",
        "    ('β', true_beta, beta_hat, se_beta),\n",
        "    ('γ', true_gamma, gamma_hat, se_gamma),\n",
        "    ('δ', true_delta, delta_hat, se_delta),\n",
        "]):\n",
        "    if np.isnan(se):\n",
        "        status = \"SE not available\"\n",
        "        within = None\n",
        "    else:\n",
        "        diff = abs(est_val - true_val)\n",
        "        within = diff <= 2 * se\n",
        "        status = \"✓ Yes\" if within else \"✗ No\"\n",
        "    within_2se.append(within)\n",
        "    print(f\"  {name}: |{est_val:.4f} - {true_val:.4f}| = {abs(est_val - true_val):.4f} \"\n",
        "          f\"{'<=' if within else '>'} 2×SE={2*se:.4f} → {status}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Overall assessment\n",
        "n_covered = sum(1 for w in within_2se if w is True)\n",
        "n_total = sum(1 for w in within_2se if w is not None)\n",
        "\n",
        "if n_total > 0:\n",
        "    print(f\"Coverage: {n_covered}/{n_total} parameters within 2 SE of true value\")\n",
        "    if n_covered == n_total:\n",
        "        print(\"✓ All parameters recovered successfully!\")\n",
        "    else:\n",
        "        print(\"⚠ Some parameters may have estimation issues.\")\n",
        "else:\n",
        "    print(\"⚠ Standard errors not available for coverage assessment.\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"COVARIANCE MATRIX\")\n",
        "print(\"=\" * 60)\n",
        "print(result.cov_matrix)"
      ],
      "id": "diagnostics",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/kissshot894/Anaconda/miniconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
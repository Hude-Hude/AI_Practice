---
title: "Solving a Dynamic Discrete Choice Model"
subtitle: "MDP with Type-I Extreme Value Shocks"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
jupyter: python3
---

## Model Setup

We consider a dynamic discrete choice model with:

- **State**: $s \in \mathbb{R}$ (continuous, discretized on a grid)
- **Action**: $a \in \{0, 1\}$ (binary choice)
- **Mean reward**: $u(s, a) = \beta s - a$
- **State transition**: $s' = (1 - \gamma) s + a$ (deterministic)
- **Discount factor**: $\delta$
- **Shocks**: $\varepsilon_a \sim$ Type-I Extreme Value (iid across actions and time)

### Interpretation

- Higher state $s$ yields higher flow payoff (coefficient $\beta > 0$)
- Action $a = 1$ costs 1 unit but boosts next period's state by 1
- Action $a = 0$ is free but state decays toward zero at rate $\gamma$

## Optimal Markov Decision Policy

### Bellman Equation with Extreme Value Shocks

Each period, the agent observes state $s$ and receives action-specific shocks $(\varepsilon_0, \varepsilon_1)$ drawn iid from the Type-I Extreme Value (Gumbel) distribution:
$$
F(\varepsilon) = \exp(-\exp(-\varepsilon)), \quad f(\varepsilon) = \exp(-\varepsilon - \exp(-\varepsilon))
$$

The agent chooses action $a \in \{0, 1\}$ to maximize:
$$
u(s, a) + \varepsilon_a + \delta V(s')
$$
where $s' = (1 - \gamma)s + a$ is the next state.

The **Bellman equation** is:
$$
V(s, \varepsilon_0, \varepsilon_1) = \max_{a \in \{0,1\}} \left\{ u(s, a) + \varepsilon_a + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon_0', \varepsilon_1') \right] \right\}
$$

### Choice-Specific Value Functions

Define the **choice-specific value function** (the deterministic part, excluding the current shock):
$$
v(s, a) = u(s, a) + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon') \right]
$$

Substituting the reward and transition:
$$
v(s, a) = \beta s - a + \delta \bar{V}\bigl((1 - \gamma) s + a\bigr)
$$
where $\bar{V}(s) = \mathbb{E}_\varepsilon[V(s, \varepsilon)]$ is the **integrated (ex-ante) value function**.

### Optimal Policy

The optimal decision rule is:
$$
a^*(s, \varepsilon_0, \varepsilon_1) = \arg\max_{a \in \{0,1\}} \left\{ v(s, a) + \varepsilon_a \right\} = \mathbf{1}\left\{ v(s, 1) + \varepsilon_1 > v(s, 0) + \varepsilon_0 \right\}
$$

The agent chooses $a = 1$ if and only if:
$$
v(s, 1) - v(s, 0) > \varepsilon_0 - \varepsilon_1
$$

### Integrated Value Function

The **integrated value function** is the expected value before observing shocks:
$$
\bar{V}(s) = \mathbb{E}_{\varepsilon_0, \varepsilon_1} \left[ \max_{a \in \{0,1\}} \left\{ v(s, a) + \varepsilon_a \right\} \right]
$$

With Type-I EV shocks, this has a closed-form (the "log-sum-exp" formula):
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right) + \gamma_E
$$
where $\gamma_E \approx 0.5772$ is Euler's constant. Since $\gamma_E$ is a constant, it does not affect the policy and is often omitted.

### Choice Probabilities

The probability of choosing action $a = 1$ given state $s$ follows the **logit formula**:
$$
P(a = 1 | s) = P(\varepsilon_0 - \varepsilon_1 < v(s, 1) - v(s, 0)) = \frac{\exp(v(s, 1))}{\exp(v(s, 0)) + \exp(v(s, 1))}
$$

Equivalently:
$$
P(a = 1 | s) = \frac{1}{1 + \exp(v(s, 0) - v(s, 1))} = \Lambda(v(s, 1) - v(s, 0))
$$
where $\Lambda(x) = \frac{1}{1 + e^{-x}}$ is the logistic function.

The optimal policy is a **stochastic Markov policy**: the choice probability depends only on the current state $s$ through the value difference $v(s, 1) - v(s, 0)$.

## Neural Network Architecture

We approximate each action's choice-specific value function $v(s, a)$ using a **monotonic neural network**. Since higher states yield higher rewards ($\beta > 0$), we expect $v(s, a)$ to be increasing in $s$. Enforcing monotonicity as an architectural constraint improves extrapolation and ensures economically sensible behavior.

### Monotonic Neural Networks

A feed-forward neural network is **monotonically increasing** in its input if:

1. All weights are **non-negative**
2. All activation functions are **monotonically increasing**

For a single hidden layer network:
$$
v_\theta(s, a) = W_2^{(a)} \cdot \sigma\left( W_1^{(a)} \cdot s + b_1^{(a)} \right) + b_2^{(a)}
$$

where $W_1^{(a)} \geq 0$, $W_2^{(a)} \geq 0$, and $\sigma$ is a monotonic activation.

### Softplus Activation

We use the **softplus** activation function:
$$
\sigma(x) = \log(1 + e^x)
$$

Properties:

- Monotonically increasing: $\sigma'(x) = \frac{e^x}{1 + e^x} = \text{sigmoid}(x) > 0$
- Smooth approximation to ReLU: $\sigma(x) \approx \max(0, x)$ for large $|x|$
- Differentiable everywhere (unlike ReLU)

### Weight Parameterization

To enforce non-negative weights, we parameterize the weights using softplus of unconstrained parameters:
$$
W = \log(1 + e^{\tilde{W}})
$$

where $\tilde{W} \in \mathbb{R}$ is the learnable parameter. This ensures $W > 0$ while allowing gradient-based optimization.

### Type Definitions

```
TYPE ALIASES:
  Scalar    = Float
  Vector    = Array[Float, n]           # 1D array of length n
  Matrix    = Array[Float, m, n]        # 2D array of shape (m, n)
  StateGrid = Vector                    # Discretized state space
  Action    = Int ∈ {0, 1}              # Binary action
```

### Architecture Pseudo Code

```
CLASS: MonotonicLayer
─────────────────────
  # A linear layer with non-negative weights

  ATTRIBUTES:
    weight_raw : Matrix[out_features, in_features]   # Unconstrained parameters
    bias       : Vector[out_features]                # Unconstrained

  METHOD: FORWARD(x: Vector[in_features]) → Vector[out_features]
    weight : Matrix[out_features, in_features] ← softplus(weight_raw)
    RETURN weight @ x + bias
```

```
CLASS: MonotonicValueNetwork
────────────────────────────
  # Feed-forward network ensuring monotonicity in input

  ATTRIBUTES:
    layers       : List[MonotonicLayer | Softplus]
    output_layer : MonotonicLayer

  CONSTRUCTOR(hidden_sizes: List[Int])
    in_size : Int ← 1  # Single state input
    FOR hidden_size : Int IN hidden_sizes:
      ADD MonotonicLayer(in_features=in_size, out_features=hidden_size)
      ADD Softplus activation
      in_size ← hidden_size
    ADD MonotonicLayer(in_features=in_size, out_features=1)

  METHOD: FORWARD(s: Vector[n]) → Vector[n]
    x : Matrix[n, 1] ← reshape(s, shape=(-1, 1))
    FOR layer IN layers:
      x ← layer(x)
    RETURN squeeze(x)
```

```
CLASS: ValueFunctionApproximator
────────────────────────────────
  # Approximates v(s, 0) and v(s, 1) with separate monotonic networks

  ATTRIBUTES:
    v0_net : MonotonicValueNetwork
    v1_net : MonotonicValueNetwork

  CONSTRUCTOR(hidden_sizes: List[Int])
    v0_net ← MonotonicValueNetwork(hidden_sizes=hidden_sizes)
    v1_net ← MonotonicValueNetwork(hidden_sizes=hidden_sizes)

  METHOD: FORWARD(s: Vector[n]) → Tuple[Vector[n], Vector[n]]
    v0 : Vector[n] ← v0_net.FORWARD(s)
    v1 : Vector[n] ← v1_net.FORWARD(s)
    RETURN (v0, v1)

  METHOD: COMPUTE_INTEGRATED_VALUE(s: Vector[n]) → Vector[n]
    v0 : Vector[n], v1 : Vector[n] ← FORWARD(s)
    RETURN logsumexp(stack(v0, v1), axis=1)

  METHOD: COMPUTE_CHOICE_PROBABILITY(s: Vector[n]) → Vector[n]
    v0 : Vector[n], v1 : Vector[n] ← FORWARD(s)
    RETURN sigmoid(v1 - v0)
```

## Value Iteration Algorithm

### Pseudo Code

```
ALGORITHM: SolveValueFunction
─────────────────────────────
INPUT:
  state_grid     : StateGrid        # Discretized state space, length n
  β              : Scalar           # Reward coefficient on state
  γ              : Scalar           # State decay rate
  δ              : Scalar           # Discount factor
  tolerance      : Scalar           # Convergence tolerance
  max_iterations : Int              # Maximum iterations

OUTPUT:
  V  : Vector[n]                    # Integrated value function
  v₀ : Vector[n]                    # Choice-specific value for action 0
  v₁ : Vector[n]                    # Choice-specific value for action 1

PROCEDURE:
  1. V : Vector[n] ← INITIALIZE_VALUE_FUNCTION(state_grid)

  2. FOR iteration : Int = 1 TO max_iterations:
     
     a. v₀ : Vector[n] ← COMPUTE_CHOICE_VALUE(V, state_grid, action=0, β, γ, δ)
     b. v₁ : Vector[n] ← COMPUTE_CHOICE_VALUE(V, state_grid, action=1, β, γ, δ)
     
     c. V_new : Vector[n] ← COMPUTE_INTEGRATED_VALUE(v₀, v₁)
     
     d. IF CHECK_CONVERGENCE(V, V_new, tolerance):
          RETURN (V_new, v₀, v₁)
     
     e. V ← V_new

  3. WARN("Did not converge")
     RETURN (V, v₀, v₁)
```

### Subroutines

```
SUBROUTINE: INITIALIZE_VALUE_FUNCTION
─────────────────────────────────────
  INPUT:  state_grid : StateGrid
  OUTPUT: V : Vector[n]

  n : Int ← length(state_grid)
  RETURN zeros(n)
```

```
SUBROUTINE: COMPUTE_CHOICE_VALUE
────────────────────────────────
  INPUT:
    V          : Vector[n]      # Current value function
    state_grid : StateGrid      # State grid
    action     : Action         # Action (0 or 1)
    β          : Scalar         # Reward coefficient
    γ          : Scalar         # Decay rate
    δ          : Scalar         # Discount factor
  OUTPUT:
    v : Vector[n]               # Choice-specific value

  # Step 1: Compute flow reward
  reward : Vector[n] ← COMPUTE_REWARD(s=state_grid, action=action, β=β)
  
  # Step 2: Compute next states
  next_states : Vector[n] ← COMPUTE_NEXT_STATE(s=state_grid, action=action, γ=γ)
  
  # Step 3: Interpolate continuation value at next states
  continuation : Vector[n] ← INTERPOLATE_VALUE(V=V, state_grid=state_grid, query_states=next_states)
  
  # Step 4: Combine flow reward and discounted continuation
  RETURN reward + δ * continuation
```

```
SUBROUTINE: COMPUTE_REWARD
──────────────────────────
  INPUT:
    s      : Vector[n]    # State(s)
    action : Action       # Action (0 or 1)
    β      : Scalar       # Reward coefficient
  OUTPUT:
    reward : Vector[n]    # Flow reward

  RETURN β * s - action
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
──────────────────────────────
  INPUT:
    s      : Vector[n]    # Current state(s)
    action : Action       # Action (0 or 1)
    γ      : Scalar       # Decay rate
  OUTPUT:
    s_next : Vector[n]    # Next state(s)

  RETURN (1 - γ) * s + action
```

```
SUBROUTINE: INTERPOLATE_VALUE
─────────────────────────────
  INPUT:
    V            : Vector[n]    # Value function on grid
    state_grid   : StateGrid    # Grid points
    query_states : Vector[m]    # Points to interpolate
  OUTPUT:
    V_interp : Vector[m]        # Interpolated values

  # Linear interpolation with boundary extrapolation
  RETURN linear_interpolate(
    x=state_grid,
    y=V,
    query=query_states,
    extrapolate="boundary"
  )
```

```
SUBROUTINE: COMPUTE_INTEGRATED_VALUE
────────────────────────────────────
  INPUT:
    v₀ : Vector[n]    # Choice-specific value for action 0
    v₁ : Vector[n]    # Choice-specific value for action 1
  OUTPUT:
    V : Vector[n]     # Integrated value function

  # Log-sum-exp formula (Emax with Type-I EV shocks)
  RETURN log(exp(v₀) + exp(v₁))
```

```
SUBROUTINE: CHECK_CONVERGENCE
─────────────────────────────
  INPUT:
    V_old     : Vector[n]    # Previous value function
    V_new     : Vector[n]    # Updated value function
    tolerance : Scalar       # Convergence threshold
  OUTPUT:
    converged : Bool         # True if converged

  diff : Scalar ← max(|V_new - V_old|)
  RETURN diff < tolerance
```

### Derived Quantities

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
──────────────────────────────────────
  INPUT:
    v₀ : Vector[n]    # Choice-specific value for action 0
    v₁ : Vector[n]    # Choice-specific value for action 1
  OUTPUT:
    p : Vector[n]     # Probability of choosing action 1

  # Logit formula: P(a=1|s)
  RETURN exp(v₁) / (exp(v₀) + exp(v₁))
  
  # Equivalently: sigmoid(v₁ - v₀)
```

## Implementation

We implement the MDP solver using the `mdp_solver` package with the following parameters:

- $\beta = 0.5$ (reward coefficient)
- $\gamma = 0.1$ (state decay rate)
- $\delta = 0.95$ (discount factor)

```{python}
import numpy as np
import matplotlib.pyplot as plt
from mdp_solver import (
    solve_value_function,
    compute_choice_probability,
)
from mdp_solver.value_iteration import (
    initialize_value_function,
    compute_choice_value,
    compute_integrated_value,
)
```

### Parameters

```{python}
# Model parameters
beta = 0.5      # Reward coefficient on state
gamma = 0.1     # State decay rate
delta = 0.95    # Discount factor

# State space
n_states = 200
s_min, s_max = 0.0, 15.0
state_grid = np.linspace(start=s_min, stop=s_max, num=n_states)

print(f"Parameters: β = {beta}, γ = {gamma}, δ = {delta}")
print(f"State grid: [{s_min}, {s_max}] with {n_states} points")
```

### Solve the Model with Convergence Tracking

```{python}
# Solve with convergence tracking
tolerance = 1e-10
max_iterations = 1000

# Initialize
V = initialize_value_function(state_grid=state_grid)
errors = []

for iteration in range(max_iterations):
    # Compute choice-specific values
    v0_iter = compute_choice_value(
        V=V, state_grid=state_grid, action=0,
        beta=beta, gamma=gamma, delta=delta
    )
    v1_iter = compute_choice_value(
        V=V, state_grid=state_grid, action=1,
        beta=beta, gamma=gamma, delta=delta
    )
    
    # Compute integrated value
    V_new = compute_integrated_value(v0=v0_iter, v1=v1_iter)
    
    # Track error
    error = np.max(np.abs(V_new - V))
    errors.append(error)
    
    # Check convergence
    if error < tolerance:
        n_iter = iteration + 1
        break
    
    V = V_new
else:
    n_iter = max_iterations

# Final values
V = V_new
v0 = compute_choice_value(
    V=V, state_grid=state_grid, action=0,
    beta=beta, gamma=gamma, delta=delta
)
v1 = compute_choice_value(
    V=V, state_grid=state_grid, action=1,
    beta=beta, gamma=gamma, delta=delta
)

print(f"Converged in {n_iter} iterations")
print(f"Final error: {errors[-1]:.2e}")
```

### Convergence Plot

```{python}
#| label: fig-convergence
#| fig-cap: "Convergence of value function iteration"

fig, ax = plt.subplots(figsize=(8, 5))

ax.semilogy(range(1, len(errors) + 1), errors, linewidth=2, color="darkgreen")
ax.axhline(y=tolerance, color="red", linestyle="--", alpha=0.7, label=f"Tolerance = {tolerance:.0e}")
ax.set_xlabel("Iteration")
ax.set_ylabel("Error (max |V_new - V_old|)")
ax.set_title("Convergence of Value Function Iteration")
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Error at iteration 1: {errors[0]:.4f}")
print(f"Error at iteration 10: {errors[9]:.4f}")
print(f"Error at iteration 50: {errors[49]:.4f}")
print(f"Error at iteration 100: {errors[99]:.6f}")
```

### Compute Choice Probabilities

```{python}
prob_a1 = compute_choice_probability(v0=v0, v1=v1)
```

### Results

#### Value Functions

```{python}
#| label: fig-value-functions
#| fig-cap: "Value functions by state"

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))

# Plot choice-specific values
ax = axes[0]
ax.plot(state_grid, v0, label="$v(s, 0)$", linewidth=2)
ax.plot(state_grid, v1, label="$v(s, 1)$", linewidth=2)
ax.set_xlabel("State $s$")
ax.set_ylabel("Choice-specific value")
ax.set_title("Choice-Specific Value Functions")
ax.legend()
ax.grid(alpha=0.3)

# Plot integrated value
ax = axes[1]
ax.plot(state_grid, V, color="black", linewidth=2)
ax.set_xlabel("State $s$")
ax.set_ylabel("$V(s)$")
ax.set_title("Integrated Value Function (Emax)")
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Choice Probabilities

```{python}
#| label: fig-choice-prob
#| fig-cap: "Probability of choosing action 1 by state"

fig, ax = plt.subplots(figsize=(8, 5))

ax.plot(state_grid, prob_a1, color="darkred", linewidth=2)
ax.axhline(y=0.5, color="gray", linestyle="--", alpha=0.5)
ax.set_xlabel("State $s$")
ax.set_ylabel("$P(a = 1 | s)$")
ax.set_title("Probability of Action 1")
ax.set_ylim([-0.05, 1.05])
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Policy Interpretation

```{python}
# Find threshold state where choice probability crosses 50%
threshold_idx = np.argmin(np.abs(prob_a1 - 0.5))
threshold_state = state_grid[threshold_idx]

print(f"Model parameters: β = {beta}, γ = {gamma}, δ = {delta}")
print(f"Threshold state (P(a=1) ≈ 0.5): s ≈ {threshold_state:.2f}")
print(f"At s = {s_min:.1f}: P(a=1) = {prob_a1[0]:.3f}")
print(f"At s = {s_max:.1f}: P(a=1) = {prob_a1[-1]:.3f}")
```

#### Value Function Difference

The choice depends on $v(s, 1) - v(s, 0)$:

```{python}
#| label: fig-value-diff
#| fig-cap: "Difference in choice-specific values"

fig, ax = plt.subplots(figsize=(8, 5))

v_diff = v1 - v0
ax.plot(state_grid, v_diff, color="darkblue", linewidth=2)
ax.axhline(y=0, color="gray", linestyle="--", alpha=0.5)
ax.set_xlabel("State $s$")
ax.set_ylabel("$v(s, 1) - v(s, 0)$")
ax.set_title("Value Difference (Action 1 minus Action 0)")
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

## Summary

This document describes a dynamic discrete choice problem with:

- **State dynamics**: $s' = (1 - \gamma) s + a$ — state decays naturally, action 1 boosts it
- **Trade-off**: Action 1 costs 1 today but increases future state (and hence future rewards)
- **Type-I EV shocks**: Provide smooth choice probabilities via logit formula

The solution method:

1. **Discretize** the continuous state space
2. **Interpolate** to handle off-grid continuation values
3. **Iterate** on the Bellman equation using log-sum-exp
4. **Recover** choice probabilities from the logit formula

The neural network approximation:

1. Use **monotonic networks** (non-negative weights + softplus activation)
2. Separate networks for each action's value function
3. Enforce economic structure (monotonicity) as architectural constraint

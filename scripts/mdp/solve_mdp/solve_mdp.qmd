---
title: "Solving a Dynamic Discrete Choice Model"
subtitle: "MDP with Type-I Extreme Value Shocks"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Model Setup

We consider a dynamic discrete choice model with:

- **State**: $s \in \mathbb{R}_{\geq 0}$ (continuous, non-negative)
- **Action**: $a \in \{0, 1\}$ (binary choice)
- **Mean reward**: $u(s, a) = \beta \log(1 + s) - a$
- **State transition**: $s' = (1 - \gamma) s + a$ (deterministic)
- **Discount factor**: $\delta$
- **Shocks**: $\varepsilon_a \sim$ Type-I Extreme Value (iid across actions and time)

### Interpretation

- Higher state $s$ yields higher flow payoff, but with **diminishing marginal returns**
- The $\log(1 + s)$ specification ensures:
  - Marginal utility $\frac{\partial u}{\partial s} = \frac{\beta}{1 + s} > 0$ (increasing in $s$)
  - Diminishing: $\frac{\partial^2 u}{\partial s^2} = -\frac{\beta}{(1 + s)^2} < 0$ (concave)
  - Well-defined at $s = 0$: $u(0, a) = -a$
- Action $a = 1$ costs 1 unit but boosts next period's state by 1
- Action $a = 0$ is free but state decays toward zero at rate $\gamma$

## Optimal Markov Decision Policy

### Bellman Equation with Extreme Value Shocks

Each period, the agent observes state $s$ and receives action-specific shocks $(\varepsilon_0, \varepsilon_1)$ drawn iid from the Type-I Extreme Value (Gumbel) distribution:
$$
F(\varepsilon) = \exp(-\exp(-\varepsilon)), \quad f(\varepsilon) = \exp(-\varepsilon - \exp(-\varepsilon))
$$

The agent chooses action $a \in \{0, 1\}$ to maximize:
$$
u(s, a) + \varepsilon_a + \delta V(s')
$$
where $s' = (1 - \gamma)s + a$ is the next state.

The **Bellman equation** is:
$$
V(s, \varepsilon_0, \varepsilon_1) = \max_{a \in \{0,1\}} \left\{ u(s, a) + \varepsilon_a + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon_0', \varepsilon_1') \right] \right\}
$$

### Choice-Specific Value Functions

Define the **choice-specific value function** (the deterministic part, excluding the current shock):
$$
v(s, a) = u(s, a) + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon') \right]
$$

Substituting the reward and transition:
$$
v(s, a) = \beta \log(1 + s) - a + \delta \bar{V}\bigl((1 - \gamma) s + a\bigr)
$$
where $\bar{V}(s) = \mathbb{E}_\varepsilon[V(s, \varepsilon)]$ is the **integrated (ex-ante) value function**.

### Integrated Value Function

The **integrated value function** is the expected value before observing shocks:
$$
\bar{V}(s) = \mathbb{E}_{\varepsilon_0, \varepsilon_1} \left[ \max_{a \in \{0,1\}} \left\{ v(s, a) + \varepsilon_a \right\} \right]
$$

With Type-I EV shocks, this has a closed-form (the "log-sum-exp" formula):
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right) + \gamma_E
$$
where $\gamma_E \approx 0.5772$ is Euler's constant. Since $\gamma_E$ is a constant, it does not affect the policy and is often omitted.

### Optimal Policy and Choice Probabilities

The optimal decision rule is:
$$
a^*(s, \varepsilon_0, \varepsilon_1) = \mathbf{1}\left\{ v(s, 1) + \varepsilon_1 > v(s, 0) + \varepsilon_0 \right\}
$$

The probability of choosing action $a = 1$ given state $s$ follows the **logit formula**:
$$
P(a = 1 | s) = \frac{\exp(v(s, 1))}{\exp(v(s, 0)) + \exp(v(s, 1))} = \frac{1}{1 + \exp(v(s, 0) - v(s, 1))}
$$

The optimal policy is a **stochastic Markov policy**: the choice probability depends only on the current state $s$ through the value difference $v(s, 1) - v(s, 0)$.

## Solution Method: Neural Network Value Iteration

### Bellman Fixed Point

The integrated value function $\bar{V}(s)$ satisfies the **fixed point equation**:
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right)
$$
where each choice-specific value depends on $\bar{V}$:
$$
v(s, a) = \beta \log(1 + s) - a + \delta \bar{V}((1 - \gamma)s + a)
$$

By Blackwell's sufficient conditions, this defines a contraction mapping with modulus $\delta < 1$, guaranteeing a unique fixed point $\bar{V}^*$.

### Monotonic Neural Network Architecture

We approximate each action's choice-specific value function $v(s, a)$ using a **monotonic neural network**. Since higher states yield higher rewards ($\beta > 0$), we expect $v(s, a)$ to be increasing in $s$. Enforcing monotonicity as an architectural constraint improves extrapolation and ensures economically sensible behavior.

A feed-forward neural network is **monotonically increasing** in its input if:

1. All weights are **non-negative**
2. All activation functions are **monotonically increasing**

For a network with $L$ hidden layers:
$$
v_\theta(s) = W_L \cdot \sigma\left( W_{L-1} \cdot \sigma\left( \cdots \sigma(W_1 \cdot s + b_1) \cdots \right) + b_{L-1} \right) + b_L
$$
where $W_\ell \geq 0$ for all layers and $\sigma$ is a monotonically increasing activation function.

### Tanh Activation

We use the **tanh** (hyperbolic tangent) activation function:
$$
\sigma(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

Properties:

- **Monotonically increasing**: $\sigma'(x) = 1 - \tanh^2(x) > 0$
- **Bounded output**: $\sigma(x) \in (-1, 1)$ prevents explosive growth
- **Zero-centered**: Allows both positive and negative intermediate representations
- **Better gradient flow**: Bounded outputs help maintain stable gradients through deep networks

### Weight Parameterization

To enforce non-negative weights during optimization, we parameterize the weights using softplus of unconstrained parameters:
$$
W = \text{softplus}(\tilde{W}) = \log(1 + e^{\tilde{W}})
$$
where $\tilde{W} \in \mathbb{R}$ is the learnable parameter. This ensures $W > 0$ while allowing standard gradient-based optimization.

### Value Function Approximator

We use **separate monotonic networks** for each action:

- $v_{\theta_0}(s)$: Monotonic network for action $a = 0$
- $v_{\theta_1}(s)$: Monotonic network for action $a = 1$

The integrated value function is computed as:
$$
\bar{V}_\theta(s) = \log\left( \exp(v_{\theta_0}(s)) + \exp(v_{\theta_1}(s)) \right)
$$

Choice probabilities follow the logit formula:
$$
P_\theta(a = 1 | s) = \frac{1}{1 + \exp(v_{\theta_0}(s) - v_{\theta_1}(s))}
$$

### Training Objective

The networks are trained to satisfy the Bellman equation by minimizing the **Bellman residual**:
$$
\mathcal{L}(\theta) = \mathbb{E}_s\left[ \sum_{a \in \{0,1\}} \left( v_{\theta_a}(s) - u(s, a) - \delta \bar{V}_\theta(s'_a) \right)^2 \right]
$$
where $s'_a = (1 - \gamma)s + a$ is the next state under action $a$.

### Type Definitions

```
TYPE DEFINITIONS
────────────────
Scalar      = Float                      # Single real number
Vector[n]   = Array[Float, n]            # 1D array of n floats
Matrix[m,n] = Array[Float, m, n]         # 2D array of shape (m, n)
Action      = Int ∈ {0, 1}               # Binary action
Network     = MonotonicNeuralNetwork     # Neural network with non-negative weights
```

### Algorithm

```
ALGORITHM: SOLVE_VALUE_FUNCTION
───────────────────────────────
INPUT:
  β            : Scalar              # Reward coefficient
  γ            : Scalar              # State decay rate
  δ            : Scalar              # Discount factor
  s_min        : Scalar              # Minimum state
  s_max        : Scalar              # Maximum state
  hidden_sizes : List[Int]           # Hidden layer sizes
  α            : Scalar              # Learning rate
  N            : Int                 # Batch size
  ε            : Scalar              # Convergence tolerance
  K            : Int                 # Maximum iterations
  T            : Int                 # Target network update frequency

OUTPUT:
  v₀_net       : Network             # Trained network for action 0
  v₁_net       : Network             # Trained network for action 1

PROCEDURE:
  # Initialize policy networks (updated every iteration)
  v₀_net : Network, v₁_net : Network ← INITIALIZE_NETWORKS(hidden_sizes)
  
  # Initialize target networks (frozen copies, updated every T iterations)
  v₀_target : Network ← COPY_NETWORK(v₀_net)
  v₁_target : Network ← COPY_NETWORK(v₁_net)
  
  # Generate fixed state grid (same states used every iteration)
  s_grid : Vector[N] ← GENERATE_STATE_GRID(N, s_min, s_max)
  
  FOR k : Int = 1 TO K:
    # Predictions from policy networks (using fixed grid)
    v₀_pred : Vector[N] ← EVALUATE_NETWORK(v₀_net, s_grid)
    v₁_pred : Vector[N] ← EVALUATE_NETWORK(v₁_net, s_grid)
    
    # Targets computed from TARGET networks (stable, not moving)
    target₀ : Vector[N], target₁ : Vector[N] ← COMPUTE_BELLMAN_TARGETS(
      v₀_target, v₁_target, s_grid, β, γ, δ
    )
    
    loss : Scalar ← COMPUTE_BELLMAN_LOSS(v₀_pred, v₁_pred, target₀, target₁)
    
    # Update policy networks
    UPDATE_NETWORK_WEIGHTS(v₀_net, v₁_net, loss, α)
    
    # Periodically sync target networks with policy networks
    IF k MOD T = 0:
      v₀_target ← COPY_NETWORK(v₀_net)
      v₁_target ← COPY_NETWORK(v₁_net)
    
    IF CHECK_CONVERGENCE(loss, ε):
      RETURN (v₀_net, v₁_net)
  
  RETURN (v₀_net, v₁_net)
```

### Subroutines

```
SUBROUTINE: INITIALIZE_NETWORKS
───────────────────────────────
INPUT:
  hidden_sizes : List[Int]           # Hidden layer sizes
OUTPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1

  v₀_net : Network ← BUILD_MONOTONIC_NETWORK(hidden_sizes)
  v₁_net : Network ← BUILD_MONOTONIC_NETWORK(hidden_sizes)
  RETURN (v₀_net, v₁_net)
```

```
SUBROUTINE: COPY_NETWORK
────────────────────────
INPUT:
  source : Network                   # Network to copy from
OUTPUT:
  target : Network                   # Deep copy of source network

  # Create a new network with identical architecture and weights
  target : Network ← DEEP_COPY(source)
  
  # Freeze the target network (no gradient computation)
  SET_REQUIRES_GRAD(target, false)
  
  RETURN target
```

```
SUBROUTINE: BUILD_MONOTONIC_NETWORK
───────────────────────────────────
INPUT:
  hidden_sizes : List[Int]           # Hidden layer sizes
OUTPUT:
  network : Network                  # Monotonic neural network

  layers : List[Layer] ← empty list
  in_features : Int ← 1
  
  FOR i : Int = 0 TO length(hidden_sizes) - 1:
    out_features : Int ← hidden_sizes[i]
    
    # Create monotonic linear layer
    W_raw : Matrix[out_features, in_features] ← random_normal(0, 0.1)
    b : Vector[out_features] ← zeros(out_features)
    layers.append(MonotonicLinear(W_raw, b))
    
    # Add tanh activation (bounded, zero-centered)
    layers.append(TanhActivation())
    
    in_features ← out_features
  
  # Output layer (no activation)
  W_raw_out : Matrix[1, in_features] ← random_normal(0, 0.1)
  b_out : Vector[1] ← zeros(1)
  layers.append(MonotonicLinear(W_raw_out, b_out))
  
  RETURN Network(layers)
```

```
SUBROUTINE: APPLY_MONOTONIC_LINEAR
──────────────────────────────────
INPUT:
  W_raw : Matrix[m, n]               # Unconstrained weight parameters
  b     : Vector[m]                  # Bias vector
  x     : Vector[n]                  # Input vector
OUTPUT:
  y     : Vector[m]                  # Output vector

  # Enforce non-negative weights via softplus
  W : Matrix[m, n] ← log(1 + exp(W_raw))
  
  # Linear transformation
  y : Vector[m] ← W @ x + b
  RETURN y
```

```
SUBROUTINE: APPLY_TANH
──────────────────────
INPUT:
  x : Vector[n]                      # Input vector
OUTPUT:
  y : Vector[n]                      # Output vector

  FOR i : Int = 0 TO n - 1:
    y[i] : Scalar ← tanh(x[i])       # Bounded to (-1, 1)
  RETURN y
```

```
SUBROUTINE: EVALUATE_NETWORK
────────────────────────────
INPUT:
  network : Network                  # Monotonic neural network
  s       : Vector[N]                # Input states
OUTPUT:
  values  : Vector[N]                # Output values

  x : Matrix[N, 1] ← reshape(s, (N, 1))
  
  FOR layer IN network.layers:
    IF layer IS MonotonicLinear:
      x ← APPLY_MONOTONIC_LINEAR(layer.W_raw, layer.b, x)
    ELSE IF layer IS TanhActivation:
      x ← APPLY_TANH(x)
  
  values : Vector[N] ← reshape(x, (N,))
  RETURN values
```

```
SUBROUTINE: GENERATE_STATE_GRID
───────────────────────────────
INPUT:
  N     : Int                        # Number of grid points
  s_min : Scalar                     # Minimum state
  s_max : Scalar                     # Maximum state
OUTPUT:
  s     : Vector[N]                  # Fixed state grid (evenly spaced)

  # Generate evenly spaced points from s_min to s_max
  step : Scalar ← (s_max - s_min) / (N - 1)
  
  FOR i : Int = 0 TO N - 1:
    s[i] : Scalar ← s_min + i * step
  RETURN s
```

```
SUBROUTINE: COMPUTE_REWARD
──────────────────────────
INPUT:
  s      : Vector[N]                 # States
  action : Action                    # Action (0 or 1)
  β      : Scalar                    # Reward coefficient
OUTPUT:
  r      : Vector[N]                 # Rewards

  FOR i : Int = 0 TO N - 1:
    r[i] : Scalar ← β * log(1 + s[i]) - action
  RETURN r
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
──────────────────────────────
INPUT:
  s      : Vector[N]                 # Current states
  action : Action                    # Action (0 or 1)
  γ      : Scalar                    # Decay rate
OUTPUT:
  s_next : Vector[N]                 # Next states

  FOR i : Int = 0 TO N - 1:
    s_next[i] : Scalar ← (1 - γ) * s[i] + action
  RETURN s_next
```

```
SUBROUTINE: COMPUTE_INTEGRATED_VALUE
────────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # States
OUTPUT:
  V_bar  : Vector[N]                 # Integrated value

  v₀ : Vector[N] ← EVALUATE_NETWORK(v₀_net, s)
  v₁ : Vector[N] ← EVALUATE_NETWORK(v₁_net, s)
  
  FOR i : Int = 0 TO N - 1:
    v_max : Scalar ← max(v₀[i], v₁[i])
    V_bar[i] : Scalar ← v_max + log(exp(v₀[i] - v_max) + exp(v₁[i] - v_max))
  RETURN V_bar
```

```
SUBROUTINE: COMPUTE_BELLMAN_TARGETS
───────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # Current states
  β      : Scalar                    # Reward coefficient
  γ      : Scalar                    # Decay rate
  δ      : Scalar                    # Discount factor
OUTPUT:
  target₀ : Vector[N]                # Target for action 0
  target₁ : Vector[N]                # Target for action 1

  # Compute targets for action 0 (no clamping - unbounded state space)
  r₀ : Vector[N] ← COMPUTE_REWARD(s, action=0, β)
  s_next₀ : Vector[N] ← COMPUTE_NEXT_STATE(s, action=0, γ)
  V_next₀ : Vector[N] ← COMPUTE_INTEGRATED_VALUE(v₀_net, v₁_net, s_next₀)
  
  FOR i : Int = 0 TO N - 1:
    target₀[i] : Scalar ← r₀[i] + δ * V_next₀[i]
  
  # Compute targets for action 1 (no clamping - unbounded state space)
  r₁ : Vector[N] ← COMPUTE_REWARD(s, action=1, β)
  s_next₁ : Vector[N] ← COMPUTE_NEXT_STATE(s, action=1, γ)
  V_next₁ : Vector[N] ← COMPUTE_INTEGRATED_VALUE(v₀_net, v₁_net, s_next₁)
  
  FOR i : Int = 0 TO N - 1:
    target₁[i] : Scalar ← r₁[i] + δ * V_next₁[i]
  
  RETURN (target₀, target₁)
```

```
SUBROUTINE: COMPUTE_BELLMAN_LOSS
────────────────────────────────
INPUT:
  v₀_pred : Vector[N]                # Predicted values for action 0
  v₁_pred : Vector[N]                # Predicted values for action 1
  target₀ : Vector[N]                # Target values for action 0
  target₁ : Vector[N]                # Target values for action 1
OUTPUT:
  loss    : Scalar                   # Mean squared Bellman residual

  sum_sq : Scalar ← 0.0
  
  FOR i : Int = 0 TO N - 1:
    error₀ : Scalar ← v₀_pred[i] - target₀[i]
    error₁ : Scalar ← v₁_pred[i] - target₁[i]
    sum_sq ← sum_sq + error₀ * error₀ + error₁ * error₁
  
  loss : Scalar ← sum_sq / (2 * N)
  RETURN loss
```

```
SUBROUTINE: UPDATE_NETWORK_WEIGHTS
──────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  loss   : Scalar                    # Loss value
  α      : Scalar                    # Learning rate
OUTPUT:
  (none, updates networks in place)

  # Compute gradients via backpropagation
  grad₀ : List[Matrix] ← BACKPROPAGATE(v₀_net, loss)
  grad₁ : List[Matrix] ← BACKPROPAGATE(v₁_net, loss)
  
  # Update weights for network 0
  FOR j : Int = 0 TO length(v₀_net.layers) - 1:
    IF v₀_net.layers[j] IS MonotonicLinear:
      v₀_net.layers[j].W_raw ← v₀_net.layers[j].W_raw - α * grad₀[j].W
      v₀_net.layers[j].b ← v₀_net.layers[j].b - α * grad₀[j].b
  
  # Update weights for network 1
  FOR j : Int = 0 TO length(v₁_net.layers) - 1:
    IF v₁_net.layers[j] IS MonotonicLinear:
      v₁_net.layers[j].W_raw ← v₁_net.layers[j].W_raw - α * grad₁[j].W
      v₁_net.layers[j].b ← v₁_net.layers[j].b - α * grad₁[j].b
```

```
SUBROUTINE: CHECK_CONVERGENCE
─────────────────────────────
INPUT:
  loss : Scalar                      # Current loss
  ε    : Scalar                      # Tolerance
OUTPUT:
  converged : Bool                   # True if converged

  rmse : Scalar ← sqrt(loss)
  converged : Bool ← (rmse < ε)
  RETURN converged
```

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
──────────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # States
OUTPUT:
  prob₀  : Vector[N]                 # P(a=0|s)
  prob₁  : Vector[N]                 # P(a=1|s)

  v₀ : Vector[N] ← EVALUATE_NETWORK(v₀_net, s)
  v₁ : Vector[N] ← EVALUATE_NETWORK(v₁_net, s)
  
  FOR i : Int = 0 TO N - 1:
    # Numerically stable softmax
    v_max : Scalar ← max(v₀[i], v₁[i])
    exp₀ : Scalar ← exp(v₀[i] - v_max)
    exp₁ : Scalar ← exp(v₁[i] - v_max)
    sum_exp : Scalar ← exp₀ + exp₁
    
    prob₀[i] : Scalar ← exp₀ / sum_exp
    prob₁[i] : Scalar ← exp₁ / sum_exp
  
  RETURN (prob₀, prob₁)
```

### Convergence

The algorithm minimizes the Bellman residual, driving the neural network approximation toward the true fixed point. Convergence is monitored via the root mean squared Bellman error $\sqrt{\mathcal{L}^{(k)}}$.

Key properties:

- **Continuous approximation**: No grid discretization or interpolation required
- **Scalability**: Extends naturally to high-dimensional state spaces
- **Economic structure**: Monotonicity enforced architecturally via non-negative weights

## Implementation

### Setup

```{python}
#| label: setup
#| cache: false

# Standard libraries
import sys
import numpy as np
import torch

# Add project paths
sys.path.insert(0, "../../../src")
sys.path.insert(0, "../config_mdp")
sys.path.insert(0, "../../utils")

# Import MDP solver
from mdp.solver import (
    solve_value_function,
    compute_choice_probability,
    evaluate_network,
    compute_integrated_value,
    compute_bellman_targets,
)

# Import configuration
import config

# Import plotting utilities
from plotting import (
    plot_convergence,
    plot_value_functions,
    plot_choice_probabilities,
    plot_value_difference,
    plot_bellman_residuals,
    plot_comparative_statics_values,
    plot_comparative_statics_probs,
)

print("Modules loaded successfully")
```

### Model Parameters

```{python}
# Load parameters from central config
beta = config.beta
gamma = config.gamma
delta = config.delta
s_min = config.s_min
s_max = config.s_max
hidden_sizes = config.hidden_sizes
learning_rate = config.learning_rate
batch_size = config.batch_size
tolerance = config.tolerance
max_iterations = config.max_iterations
target_update_freq = config.target_update_freq

print(f"Model: u(s,a) = {beta} * log(1+s) - a")
print(f"Transition: s' = {1-gamma:.1f} * s + a")
print(f"Discount factor: δ = {delta}")
print(f"\nLoaded from: scripts/config_mdp/config.py")
```

### Solve the MDP

```{python}
#| cache: true
#| label: solve-baseline

# Set random seed for reproducibility
torch.manual_seed(42)

# Solve for value functions
v0_net, v1_net, losses, n_iter = solve_value_function(
    beta=beta,
    gamma=gamma,
    delta=delta,
    s_min=s_min,
    s_max=s_max,
    hidden_sizes=hidden_sizes,
    learning_rate=learning_rate,
    batch_size=batch_size,
    tolerance=tolerance,
    max_iterations=max_iterations,
    target_update_freq=target_update_freq,
)

print(f"Converged in {n_iter} iterations")
print(f"Final RMSE: {losses[-1]**0.5:.2e}")

# Save results to output/solve_mdp/
import json
from pathlib import Path

output_dir = Path("../../../output/mdp/solve")
output_dir.mkdir(parents=True, exist_ok=True)

# Save config (model parameters)
config = {
    "beta": beta,
    "gamma": gamma,
    "delta": delta,
    "s_min": s_min,
    "s_max": s_max,
    "hidden_sizes": hidden_sizes,
}
with open(output_dir / "config.json", "w") as f:
    json.dump(config, f, indent=2)

# Save trained networks
torch.save(v0_net.state_dict(), output_dir / "v0_net.pt")
torch.save(v1_net.state_dict(), output_dir / "v1_net.pt")

# Save training losses
np.save(output_dir / "losses.npy", np.array(losses))

print(f"\nResults saved to {output_dir.resolve()}:")
print(f"  - config.json (model parameters)")
print(f"  - v0_net.pt (action 0 network)")
print(f"  - v1_net.pt (action 1 network)")
print(f"  - losses.npy ({len(losses)} iterations)")
```

### Results

#### Convergence Plot

```{python}
#| fig-cap: "Convergence of the Bellman residual over training iterations"

plot_convergence(losses=np.array(losses))
```

#### Value Functions

```{python}
#| fig-cap: "Choice-specific value functions v(s, a) for each action"

# Create evaluation grid
s_eval = torch.linspace(s_min, s_max, 200)

# Evaluate value functions
with torch.no_grad():
    v0_values = evaluate_network(v0_net, s_eval).numpy()
    v1_values = evaluate_network(v1_net, s_eval).numpy()
    V_bar = compute_integrated_value(v0_net, v1_net, s_eval).numpy()

s_np = s_eval.numpy()

plot_value_functions(s=s_np, v0_values=v0_values, v1_values=v1_values, V_bar=V_bar)
```

#### Choice Probabilities

```{python}
#| fig-cap: "Probability of choosing each action as a function of state"

# Compute choice probabilities
with torch.no_grad():
    prob_a0, prob_a1 = compute_choice_probability(v0_net, v1_net, s_eval)
    prob_a0 = prob_a0.numpy()
    prob_a1 = prob_a1.numpy()

plot_choice_probabilities(s=s_np, prob_a0=prob_a0, prob_a1=prob_a1)
```

#### Value Difference

```{python}
#| fig-cap: "Value difference v(s,1) - v(s,0) determines the choice probability"

plot_value_difference(s=s_np, v0_values=v0_values, v1_values=v1_values)
```

#### Bellman Residuals

```{python}
#| fig-cap: "Bellman residual v(s,a) - target(s,a) across states. A well-converged solution should have near-zero residuals."

# Compute Bellman targets at evaluation points
with torch.no_grad():
    target0, target1 = compute_bellman_targets(
        v0_net, v1_net, s_eval,
        beta=beta, gamma=gamma, delta=delta
    )
    target0_np = target0.numpy()
    target1_np = target1.numpy()

# Compute residuals
residual0 = v0_values - target0_np
residual1 = v1_values - target1_np

plot_bellman_residuals(s=s_np, residual0=residual0, residual1=residual1)
```

### Interpretation

```{python}
# Find threshold state where P(a=1) ≈ 0.5
threshold_idx = np.argmin(np.abs(prob_a1 - 0.5))
threshold_state = s_np[threshold_idx]

print(f"Policy Interpretation (β={beta}, γ={gamma}, δ={delta}):")
print(f"- At state s ≈ {threshold_state:.2f}, the agent is indifferent between actions")
print(f"- For s < {threshold_state:.2f}, investment (a=1) is more likely")
print(f"- For s > {threshold_state:.2f}, no investment (a=0) is more likely")
print(f"\nThis reflects the trade-off:")
print(f"- Low state: Investment pays off (builds future value)")
print(f"- High state: Already earning good rewards, investment cost outweighs benefit")
```

## Comparative Statics: Effect of β on Value Functions

This section examines how the value functions change as we vary the reward coefficient β from 0 to 2.

```{python}
#| label: comparative-statics-setup
#| cache: true

def solve_and_extract_values(beta_val):
    """Solve MDP and extract value functions and choice probabilities for given beta."""
    torch.manual_seed(42)
    v0_net_cs, v1_net_cs, _, _ = solve_value_function(
        beta=beta_val,
        gamma=gamma,
        delta=delta,
        s_min=s_min,
        s_max=s_max,
        hidden_sizes=hidden_sizes,
        learning_rate=learning_rate,
        batch_size=batch_size,
        tolerance=tolerance,
        max_iterations=max_iterations,
    )
    with torch.no_grad():
        v0_vals = evaluate_network(v0_net_cs, s_eval).numpy()
        v1_vals = evaluate_network(v1_net_cs, s_eval).numpy()
        p0_vals, p1_vals = compute_choice_probability(v0_net_cs, v1_net_cs, s_eval)
        p0_vals = p0_vals.numpy()
        p1_vals = p1_vals.numpy()
    return v0_vals, v1_vals, p0_vals, p1_vals

# Beta values from 0 to 2 in steps of 0.25
beta_values = np.arange(0, 2.25, 0.25)
n_beta = len(beta_values)

# Solve for all beta values
print("Solving for different β values...")
results_beta = {}
for b in beta_values:
    v0_vals, v1_vals, p0_vals, p1_vals = solve_and_extract_values(b)
    results_beta[b] = {'v0': v0_vals, 'v1': v1_vals, 'p0': p0_vals, 'p1': p1_vals}
    print(f"  β = {b:.2f} done")
print("Complete!")
```

### Value Functions by Action

```{python}
#| fig-cap: "Value functions v(s,a) as β varies from 0 to 2. Left: a=0 (black→blue). Right: a=1 (black→red)."

plot_comparative_statics_values(
    s=s_np,
    results=results_beta,
    param_values=beta_values,
    param_name="β",
    param_min=0,
    param_max=2,
)
```

### Choice Probabilities by Action

```{python}
#| fig-cap: "Choice probabilities P(a|s) as β varies from 0 to 2. Left: P(a=0|s) (black→blue). Right: P(a=1|s) (black→red)."

plot_comparative_statics_probs(
    s=s_np,
    results=results_beta,
    param_values=beta_values,
    param_name="β",
    param_min=0,
    param_max=2,
)
```

### Summary (β)

```{python}
print("Comparative Statics Summary: Effect of β on Value Functions")
print("=" * 60)
print(f"\nFixed parameters: γ={gamma}, δ={delta}")
print(f"β varied from 0 to 2 in steps of 0.25")
print(f"\nKey observations:")
print("• As β increases, both value functions increase (higher rewards)")
print("• v(s,0) and v(s,1) both become steeper (more sensitive to state)")
print("• At β=0, values are purely from discounted future option value")
print("• At high β, current-period rewards dominate the value functions")
```

## Comparative Statics: Effect of γ on Value Functions

This section examines how the value functions change as we vary the decay rate γ from 0 to 0.1.

```{python}
#| label: comparative-statics-gamma
#| cache: true

def solve_and_extract_values_gamma(gamma_val):
    """Solve MDP and extract value functions and choice probabilities for given gamma."""
    torch.manual_seed(42)
    v0_net_cs, v1_net_cs, _, _ = solve_value_function(
        beta=beta,
        gamma=gamma_val,
        delta=delta,
        s_min=s_min,
        s_max=s_max,
        hidden_sizes=hidden_sizes,
        learning_rate=learning_rate,
        batch_size=batch_size,
        tolerance=tolerance,
        max_iterations=max_iterations,
    )
    with torch.no_grad():
        v0_vals = evaluate_network(v0_net_cs, s_eval).numpy()
        v1_vals = evaluate_network(v1_net_cs, s_eval).numpy()
        p0_vals, p1_vals = compute_choice_probability(v0_net_cs, v1_net_cs, s_eval)
        p0_vals = p0_vals.numpy()
        p1_vals = p1_vals.numpy()
    return v0_vals, v1_vals, p0_vals, p1_vals

# Gamma values from 0 to 0.1 in steps of 0.025
gamma_values = np.arange(0, 0.125, 0.025)
n_gamma = len(gamma_values)

# Solve for all gamma values
print("Solving for different γ values...")
results_gamma = {}
for g in gamma_values:
    v0_vals, v1_vals, p0_vals, p1_vals = solve_and_extract_values_gamma(g)
    results_gamma[g] = {'v0': v0_vals, 'v1': v1_vals, 'p0': p0_vals, 'p1': p1_vals}
    print(f"  γ = {g:.3f} done")
print("Complete!")
```

### Value Functions by Action (γ)

```{python}
#| fig-cap: "Value functions v(s,a) as γ varies from 0 to 0.1. Left: a=0 (black→blue). Right: a=1 (black→red)."

plot_comparative_statics_values(
    s=s_np,
    results=results_gamma,
    param_values=gamma_values,
    param_name="γ",
    param_min=0,
    param_max=0.1,
)
```

### Choice Probabilities by Action (γ)

```{python}
#| fig-cap: "Choice probabilities P(a|s) as γ varies from 0 to 0.1. Left: P(a=0|s) (black→blue). Right: P(a=1|s) (black→red)."

plot_comparative_statics_probs(
    s=s_np,
    results=results_gamma,
    param_values=gamma_values,
    param_name="γ",
    param_min=0,
    param_max=0.1,
)
```

### Summary (γ)

```{python}
print("Comparative Statics Summary: Effect of γ on Value Functions")
print("=" * 60)
print(f"\nFixed parameters: β={beta}, δ={delta}")
print(f"γ varied from 0 to 0.1 in steps of 0.025")
print(f"\nKey observations:")
print("• As γ increases, state decays faster (s' = (1-γ)s + a)")
print("• Higher γ reduces value of being in high states (they depreciate quickly)")
print("• At γ=0, state persists forever")
print("• Investment (a=1) becomes relatively more valuable at high γ to maintain state")
```

## Conclusion

### Interpretation of Comparative Statics

The comparative statics analysis reveals how the model parameters shape the agent's value functions and optimal behavior in this dynamic discrete choice problem.

#### Effect of the Reward Coefficient (β)

The parameter β governs the **marginal value of the state** through the reward function $u(s, a) = \beta \log(1 + s) - a$:

- **Higher β amplifies the reward from being in a high state.** As β increases from 0 to 2, both value functions $v(s, 0)$ and $v(s, 1)$ shift upward and become steeper. This reflects the increased payoff from accumulating state.

- **At β = 0**, the flow reward is simply $-a$ (just the cost of investment). The value functions are relatively flat because the state provides no direct reward—value comes only from the option to invest in the future.

- **At high β**, the value functions become strongly increasing in $s$, reflecting the substantial benefit of being in a high state. The curvature from $\log(1 + s)$ ensures diminishing marginal returns, preventing unbounded values.

#### Effect of the Decay Rate (γ)

The parameter γ controls **how quickly the state depreciates** via the transition $s' = (1 - \gamma)s + a$:

- **Higher γ means faster depreciation.** When γ is larger, the state decays more rapidly toward zero without investment. This reduces the long-run value of being in any given state.

- **At γ = 0**, the state is perfectly persistent—once accumulated, it never decays. This makes the value of high states very large, as rewards persist indefinitely.

- **At higher γ**, investment becomes relatively more valuable because it is the only way to counteract the natural decay. The agent must invest more frequently to maintain a desirable state level.

- **The value functions compress** as γ increases: the difference between high and low states diminishes because high states quickly depreciate back toward lower levels.

#### Economic Intuition

Together, these comparative statics illustrate a fundamental trade-off in dynamic optimization:

1. **β determines the stakes**: Higher β makes the state more valuable, increasing both the reward from being in a good state and the incentive to invest.

2. **γ determines persistence**: Higher γ erodes the benefits of past investments, making the problem more "short-run" in character and requiring ongoing investment to maintain value.

The interaction between these parameters shapes the optimal policy: when β is high and γ is low, the agent enjoys large, persistent rewards from accumulating state. When β is low or γ is high, the benefits of investment are either small or short-lived, leading to less aggressive investment behavior.

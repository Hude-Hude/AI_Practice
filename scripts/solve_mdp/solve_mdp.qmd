---
title: "Solving a Dynamic Discrete Choice Model"
subtitle: "MDP with Type-I Extreme Value Shocks"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Model Setup

We consider a dynamic discrete choice model with:

- **State**: $s \in \mathbb{R}$ (continuous, discretized on a grid)
- **Action**: $a \in \{0, 1\}$ (binary choice)
- **Mean reward**: $u(s, a) = \beta s - a$
- **State transition**: $s' = (1 - \gamma) s + a$ (deterministic)
- **Discount factor**: $\delta$
- **Shocks**: $\varepsilon_a \sim$ Type-I Extreme Value (iid across actions and time)

### Interpretation

- Higher state $s$ yields higher flow payoff (coefficient $\beta > 0$)
- Action $a = 1$ costs 1 unit but boosts next period's state by 1
- Action $a = 0$ is free but state decays toward zero at rate $\gamma$

## Optimal Markov Decision Policy

### Bellman Equation with Extreme Value Shocks

Each period, the agent observes state $s$ and receives action-specific shocks $(\varepsilon_0, \varepsilon_1)$ drawn iid from the Type-I Extreme Value (Gumbel) distribution:
$$
F(\varepsilon) = \exp(-\exp(-\varepsilon)), \quad f(\varepsilon) = \exp(-\varepsilon - \exp(-\varepsilon))
$$

The agent chooses action $a \in \{0, 1\}$ to maximize:
$$
u(s, a) + \varepsilon_a + \delta V(s')
$$
where $s' = (1 - \gamma)s + a$ is the next state.

The **Bellman equation** is:
$$
V(s, \varepsilon_0, \varepsilon_1) = \max_{a \in \{0,1\}} \left\{ u(s, a) + \varepsilon_a + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon_0', \varepsilon_1') \right] \right\}
$$

### Choice-Specific Value Functions

Define the **choice-specific value function** (the deterministic part, excluding the current shock):
$$
v(s, a) = u(s, a) + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon') \right]
$$

Substituting the reward and transition:
$$
v(s, a) = \beta s - a + \delta \bar{V}\bigl((1 - \gamma) s + a\bigr)
$$
where $\bar{V}(s) = \mathbb{E}_\varepsilon[V(s, \varepsilon)]$ is the **integrated (ex-ante) value function**.

### Optimal Policy

The optimal decision rule is:
$$
a^*(s, \varepsilon_0, \varepsilon_1) = \arg\max_{a \in \{0,1\}} \left\{ v(s, a) + \varepsilon_a \right\} = \mathbf{1}\left\{ v(s, 1) + \varepsilon_1 > v(s, 0) + \varepsilon_0 \right\}
$$

The agent chooses $a = 1$ if and only if:
$$
v(s, 1) - v(s, 0) > \varepsilon_0 - \varepsilon_1
$$

### Integrated Value Function

The **integrated value function** is the expected value before observing shocks:
$$
\bar{V}(s) = \mathbb{E}_{\varepsilon_0, \varepsilon_1} \left[ \max_{a \in \{0,1\}} \left\{ v(s, a) + \varepsilon_a \right\} \right]
$$

With Type-I EV shocks, this has a closed-form (the "log-sum-exp" formula):
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right) + \gamma_E
$$
where $\gamma_E \approx 0.5772$ is Euler's constant. Since $\gamma_E$ is a constant, it does not affect the policy and is often omitted.

### Choice Probabilities

The probability of choosing action $a = 1$ given state $s$ follows the **logit formula**:
$$
P(a = 1 | s) = P(\varepsilon_0 - \varepsilon_1 < v(s, 1) - v(s, 0)) = \frac{\exp(v(s, 1))}{\exp(v(s, 0)) + \exp(v(s, 1))}
$$

Equivalently:
$$
P(a = 1 | s) = \frac{1}{1 + \exp(v(s, 0) - v(s, 1))} = \Lambda(v(s, 1) - v(s, 0))
$$
where $\Lambda(x) = \frac{1}{1 + e^{-x}}$ is the logistic function.

The optimal policy is a **stochastic Markov policy**: the choice probability depends only on the current state $s$ through the value difference $v(s, 1) - v(s, 0)$.

## Neural Network Architecture

We approximate each action's choice-specific value function $v(s, a)$ using a **monotonic neural network**. Since higher states yield higher rewards ($\beta > 0$), we expect $v(s, a)$ to be increasing in $s$. Enforcing monotonicity as an architectural constraint improves extrapolation and ensures economically sensible behavior.

### Monotonic Neural Networks

A feed-forward neural network is **monotonically increasing** in its input if:

1. All weights are **non-negative**
2. All activation functions are **monotonically increasing**

For a single hidden layer network:
$$
v_\theta(s, a) = W_2^{(a)} \cdot \sigma\left( W_1^{(a)} \cdot s + b_1^{(a)} \right) + b_2^{(a)}
$$

where $W_1^{(a)} \geq 0$, $W_2^{(a)} \geq 0$, and $\sigma$ is a monotonic activation.

### Softplus Activation

We use the **softplus** activation function:
$$
\sigma(x) = \log(1 + e^x)
$$

Properties:

- Monotonically increasing: $\sigma'(x) = \frac{e^x}{1 + e^x} = \text{sigmoid}(x) > 0$
- Smooth approximation to ReLU: $\sigma(x) \approx \max(0, x)$ for large $|x|$
- Differentiable everywhere (unlike ReLU)

### Weight Parameterization

To enforce non-negative weights, we parameterize the weights using softplus of unconstrained parameters:
$$
W = \log(1 + e^{\tilde{W}})
$$

where $\tilde{W} \in \mathbb{R}$ is the learnable parameter. This ensures $W > 0$ while allowing gradient-based optimization.

### Architecture Pseudo Code

```
CLASS: MonotonicLayer
─────────────────────
  # A linear layer with non-negative weights

  ATTRIBUTES:
    weight_raw : matrix of unconstrained parameters
    bias : vector (unconstrained)

  METHOD: FORWARD(x)
    weight ← softplus(weight_raw)    # Ensure W > 0
    RETURN weight @ x + bias
```

```
CLASS: MonotonicValueNetwork
────────────────────────────
  # Feed-forward network ensuring monotonicity in input

  ATTRIBUTES:
    layers : list of (MonotonicLayer, Softplus) pairs
    output_layer : MonotonicLayer

  CONSTRUCTOR(hidden_sizes):
    in_size ← 1  # Single state input
    FOR each hidden_size IN hidden_sizes:
      ADD MonotonicLayer(in_size → hidden_size)
      ADD Softplus activation
      in_size ← hidden_size
    ADD MonotonicLayer(in_size → 1)  # Output layer

  METHOD: FORWARD(s)
    x ← reshape(s, (-1, 1))
    FOR each layer IN layers:
      x ← layer(x)
    RETURN squeeze(x)
```

```
CLASS: ValueFunctionApproximator
────────────────────────────────
  # Approximates v(s, 0) and v(s, 1) with separate monotonic networks

  ATTRIBUTES:
    v0_net : MonotonicValueNetwork
    v1_net : MonotonicValueNetwork

  CONSTRUCTOR(hidden_sizes):
    v0_net ← MonotonicValueNetwork(hidden_sizes)
    v1_net ← MonotonicValueNetwork(hidden_sizes)

  METHOD: FORWARD(s)
    v0 ← v0_net(s)
    v1 ← v1_net(s)
    RETURN v0, v1

  METHOD: COMPUTE_INTEGRATED_VALUE(s)
    v0, v1 ← FORWARD(s)
    RETURN logsumexp(stack(v0, v1))

  METHOD: COMPUTE_CHOICE_PROBABILITY(s)
    v0, v1 ← FORWARD(s)
    RETURN sigmoid(v1 - v0)
```

## Value Iteration Algorithm

### Pseudo Code

```
ALGORITHM: SolveValueFunction
INPUT: state_grid, β, γ, δ, tolerance, max_iterations
OUTPUT: V (integrated value function), v₀, v₁ (choice-specific values)

1. V ← INITIALIZE_VALUE_FUNCTION(state_grid)

2. FOR iteration = 1 TO max_iterations:
   
   a. v₀ ← COMPUTE_CHOICE_VALUE(V, state_grid, action=0, β, γ, δ)
   b. v₁ ← COMPUTE_CHOICE_VALUE(V, state_grid, action=1, β, γ, δ)
   
   c. V_new ← COMPUTE_INTEGRATED_VALUE(v₀, v₁)
   
   d. IF CHECK_CONVERGENCE(V, V_new, tolerance):
        RETURN V_new, v₀, v₁
   
   e. V ← V_new

3. WARN("Did not converge")
   RETURN V, v₀, v₁
```

### Subroutines

```
SUBROUTINE: INITIALIZE_VALUE_FUNCTION(state_grid)
────────────────────────────────────────────────
  RETURN zeros(length(state_grid))
```

```
SUBROUTINE: COMPUTE_CHOICE_VALUE(V, state_grid, action, β, γ, δ)
────────────────────────────────────────────────────────────────
  # Step 1: Compute flow reward
  reward ← COMPUTE_REWARD(state_grid, action, β)
  
  # Step 2: Compute next states
  next_states ← COMPUTE_NEXT_STATE(state_grid, action, γ)
  
  # Step 3: Interpolate continuation value at next states
  continuation ← INTERPOLATE_VALUE(V, state_grid, next_states)
  
  # Step 4: Combine flow reward and discounted continuation
  RETURN reward + δ * continuation
```

```
SUBROUTINE: COMPUTE_REWARD(s, action, β)
────────────────────────────────────────
  RETURN β * s - action
```

```
SUBROUTINE: COMPUTE_NEXT_STATE(s, action, γ)
────────────────────────────────────────────
  RETURN (1 - γ) * s + action
```

```
SUBROUTINE: INTERPOLATE_VALUE(V, state_grid, query_states)
──────────────────────────────────────────────────────────
  # Linear interpolation with boundary extrapolation
  RETURN linear_interpolate(
    x=state_grid,
    y=V,
    query=query_states,
    extrapolate=boundary_values
  )
```

```
SUBROUTINE: COMPUTE_INTEGRATED_VALUE(v₀, v₁)
────────────────────────────────────────────
  # Log-sum-exp formula (Emax with Type-I EV shocks)
  RETURN log(exp(v₀) + exp(v₁))
```

```
SUBROUTINE: CHECK_CONVERGENCE(V_old, V_new, tolerance)
──────────────────────────────────────────────────────
  diff ← max(|V_new - V_old|)
  RETURN diff < tolerance
```

### Derived Quantities

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY(v₀, v₁)
──────────────────────────────────────────────
  # Logit formula: P(a=1|s)
  RETURN exp(v₁) / (exp(v₀) + exp(v₁))
  
  # Equivalently: sigmoid(v₁ - v₀)
```

## Summary

This document describes a dynamic discrete choice problem with:

- **State dynamics**: $s' = (1 - \gamma) s + a$ — state decays naturally, action 1 boosts it
- **Trade-off**: Action 1 costs 1 today but increases future state (and hence future rewards)
- **Type-I EV shocks**: Provide smooth choice probabilities via logit formula

The solution method:

1. **Discretize** the continuous state space
2. **Interpolate** to handle off-grid continuation values
3. **Iterate** on the Bellman equation using log-sum-exp
4. **Recover** choice probabilities from the logit formula

The neural network approximation:

1. Use **monotonic networks** (non-negative weights + softplus activation)
2. Separate networks for each action's value function
3. Enforce economic structure (monotonicity) as architectural constraint

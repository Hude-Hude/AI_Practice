---
title: "Solving a Dynamic Discrete Choice Model"
subtitle: "MDP with Type-I Extreme Value Shocks"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Model Setup

We consider a dynamic discrete choice model with:

- **State**: $s \in \mathbb{R}_{\geq 0}$ (continuous, non-negative)
- **Action**: $a \in \{0, 1\}$ (binary choice)
- **Mean reward**: $u(s, a) = \beta \log(1 + s) - a$
- **State transition**: $s' = (1 - \gamma) s + a$ (deterministic)
- **Discount factor**: $\delta$
- **Shocks**: $\varepsilon_a \sim$ Type-I Extreme Value (iid across actions and time)

### Interpretation

- Higher state $s$ yields higher flow payoff, but with **diminishing marginal returns**
- The $\log(1 + s)$ specification ensures:
  - Marginal utility $\frac{\partial u}{\partial s} = \frac{\beta}{1 + s} > 0$ (increasing in $s$)
  - Diminishing: $\frac{\partial^2 u}{\partial s^2} = -\frac{\beta}{(1 + s)^2} < 0$ (concave)
  - Well-defined at $s = 0$: $u(0, a) = -a$
- Action $a = 1$ costs 1 unit but boosts next period's state by 1
- Action $a = 0$ is free but state decays toward zero at rate $\gamma$

## Optimal Markov Decision Policy

### Bellman Equation with Extreme Value Shocks

Each period, the agent observes state $s$ and receives action-specific shocks $(\varepsilon_0, \varepsilon_1)$ drawn iid from the Type-I Extreme Value (Gumbel) distribution:
$$
F(\varepsilon) = \exp(-\exp(-\varepsilon)), \quad f(\varepsilon) = \exp(-\varepsilon - \exp(-\varepsilon))
$$

The agent chooses action $a \in \{0, 1\}$ to maximize:
$$
u(s, a) + \varepsilon_a + \delta V(s')
$$
where $s' = (1 - \gamma)s + a$ is the next state.

The **Bellman equation** is:
$$
V(s, \varepsilon_0, \varepsilon_1) = \max_{a \in \{0,1\}} \left\{ u(s, a) + \varepsilon_a + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon_0', \varepsilon_1') \right] \right\}
$$

### Choice-Specific Value Functions

Define the **choice-specific value function** (the deterministic part, excluding the current shock):
$$
v(s, a) = u(s, a) + \delta \mathbb{E}_{\varepsilon'} \left[ V(s', \varepsilon') \right]
$$

Substituting the reward and transition:
$$
v(s, a) = \beta \log(1 + s) - a + \delta \bar{V}\bigl((1 - \gamma) s + a\bigr)
$$
where $\bar{V}(s) = \mathbb{E}_\varepsilon[V(s, \varepsilon)]$ is the **integrated (ex-ante) value function**.

### Integrated Value Function

The **integrated value function** is the expected value before observing shocks:
$$
\bar{V}(s) = \mathbb{E}_{\varepsilon_0, \varepsilon_1} \left[ \max_{a \in \{0,1\}} \left\{ v(s, a) + \varepsilon_a \right\} \right]
$$

With Type-I EV shocks, this has a closed-form (the "log-sum-exp" formula):
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right) + \gamma_E
$$
where $\gamma_E \approx 0.5772$ is Euler's constant. Since $\gamma_E$ is a constant, it does not affect the policy and is often omitted.

### Optimal Policy and Choice Probabilities

The optimal decision rule is:
$$
a^*(s, \varepsilon_0, \varepsilon_1) = \mathbf{1}\left\{ v(s, 1) + \varepsilon_1 > v(s, 0) + \varepsilon_0 \right\}
$$

The probability of choosing action $a = 1$ given state $s$ follows the **logit formula**:
$$
P(a = 1 | s) = \frac{\exp(v(s, 1))}{\exp(v(s, 0)) + \exp(v(s, 1))} = \frac{1}{1 + \exp(v(s, 0) - v(s, 1))}
$$

The optimal policy is a **stochastic Markov policy**: the choice probability depends only on the current state $s$ through the value difference $v(s, 1) - v(s, 0)$.

## Solution Method: Neural Network Value Iteration

### Bellman Fixed Point

The integrated value function $\bar{V}(s)$ satisfies the **fixed point equation**:
$$
\bar{V}(s) = \log\left( \exp(v(s, 0)) + \exp(v(s, 1)) \right)
$$
where each choice-specific value depends on $\bar{V}$:
$$
v(s, a) = \beta \log(1 + s) - a + \delta \bar{V}((1 - \gamma)s + a)
$$

By Blackwell's sufficient conditions, this defines a contraction mapping with modulus $\delta < 1$, guaranteeing a unique fixed point $\bar{V}^*$.

### Monotonic Neural Network Architecture

We approximate each action's choice-specific value function $v(s, a)$ using a **monotonic neural network**. Since higher states yield higher rewards ($\beta > 0$), we expect $v(s, a)$ to be increasing in $s$. Enforcing monotonicity as an architectural constraint improves extrapolation and ensures economically sensible behavior.

A feed-forward neural network is **monotonically increasing** in its input if:

1. All weights are **non-negative**
2. All activation functions are **monotonically increasing**

For a network with $L$ hidden layers:
$$
v_\theta(s) = W_L \cdot \sigma\left( W_{L-1} \cdot \sigma\left( \cdots \sigma(W_1 \cdot s + b_1) \cdots \right) + b_{L-1} \right) + b_L
$$
where $W_\ell \geq 0$ for all layers and $\sigma$ is a monotonically increasing activation function.

### Softplus Activation

We use the **softplus** activation function:
$$
\sigma(x) = \log(1 + e^x)
$$

Properties:

- **Monotonically increasing**: $\sigma'(x) = \frac{e^x}{1 + e^x} = \text{sigmoid}(x) > 0$
- **Smooth approximation to ReLU**: $\sigma(x) \approx \max(0, x)$ for large $|x|$
- **Differentiable everywhere** (unlike ReLU)

### Weight Parameterization

To enforce non-negative weights during optimization, we parameterize the weights using softplus of unconstrained parameters:
$$
W = \text{softplus}(\tilde{W}) = \log(1 + e^{\tilde{W}})
$$
where $\tilde{W} \in \mathbb{R}$ is the learnable parameter. This ensures $W > 0$ while allowing standard gradient-based optimization.

### Value Function Approximator

We use **separate monotonic networks** for each action:

- $v_{\theta_0}(s)$: Monotonic network for action $a = 0$
- $v_{\theta_1}(s)$: Monotonic network for action $a = 1$

The integrated value function is computed as:
$$
\bar{V}_\theta(s) = \log\left( \exp(v_{\theta_0}(s)) + \exp(v_{\theta_1}(s)) \right)
$$

Choice probabilities follow the logit formula:
$$
P_\theta(a = 1 | s) = \frac{1}{1 + \exp(v_{\theta_0}(s) - v_{\theta_1}(s))}
$$

### Training Objective

The networks are trained to satisfy the Bellman equation by minimizing the **Bellman residual**:
$$
\mathcal{L}(\theta) = \mathbb{E}_s\left[ \sum_{a \in \{0,1\}} \left( v_{\theta_a}(s) - u(s, a) - \delta \bar{V}_\theta(s'_a) \right)^2 \right]
$$
where $s'_a = (1 - \gamma)s + a$ is the next state under action $a$.

### Type Definitions

```
TYPE DEFINITIONS
────────────────
Scalar      = Float                      # Single real number
Vector[n]   = Array[Float, n]            # 1D array of n floats
Matrix[m,n] = Array[Float, m, n]         # 2D array of shape (m, n)
Action      = Int ∈ {0, 1}               # Binary action
Network     = MonotonicNeuralNetwork     # Neural network with non-negative weights
```

### Algorithm

```
ALGORITHM: SOLVE_VALUE_FUNCTION
───────────────────────────────
INPUT:
  β            : Scalar              # Reward coefficient
  γ            : Scalar              # State decay rate
  δ            : Scalar              # Discount factor
  s_min        : Scalar              # Minimum state
  s_max        : Scalar              # Maximum state
  hidden_sizes : List[Int]           # Hidden layer sizes
  α            : Scalar              # Learning rate
  N            : Int                 # Batch size
  ε            : Scalar              # Convergence tolerance
  K            : Int                 # Maximum iterations

OUTPUT:
  v₀_net       : Network             # Trained network for action 0
  v₁_net       : Network             # Trained network for action 1

PROCEDURE:
  v₀_net : Network, v₁_net : Network ← INITIALIZE_NETWORKS(hidden_sizes)
  
  FOR k : Int = 1 TO K:
    s_batch : Vector[N] ← SAMPLE_STATES(N, s_min, s_max)
    
    v₀_pred : Vector[N] ← EVALUATE_NETWORK(v₀_net, s_batch)
    v₁_pred : Vector[N] ← EVALUATE_NETWORK(v₁_net, s_batch)
    
    target₀ : Vector[N], target₁ : Vector[N] ← COMPUTE_BELLMAN_TARGETS(
      v₀_net, v₁_net, s_batch, β, γ, δ
    )
    
    loss : Scalar ← COMPUTE_BELLMAN_LOSS(v₀_pred, v₁_pred, target₀, target₁)
    
    UPDATE_NETWORK_WEIGHTS(v₀_net, v₁_net, loss, α)
    
    IF CHECK_CONVERGENCE(loss, ε):
      RETURN (v₀_net, v₁_net)
  
  RETURN (v₀_net, v₁_net)
```

### Subroutines

```
SUBROUTINE: INITIALIZE_NETWORKS
───────────────────────────────
INPUT:
  hidden_sizes : List[Int]           # Hidden layer sizes
OUTPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1

  v₀_net : Network ← BUILD_MONOTONIC_NETWORK(hidden_sizes)
  v₁_net : Network ← BUILD_MONOTONIC_NETWORK(hidden_sizes)
  RETURN (v₀_net, v₁_net)
```

```
SUBROUTINE: BUILD_MONOTONIC_NETWORK
───────────────────────────────────
INPUT:
  hidden_sizes : List[Int]           # Hidden layer sizes
OUTPUT:
  network : Network                  # Monotonic neural network

  layers : List[Layer] ← empty list
  in_features : Int ← 1
  
  FOR i : Int = 0 TO length(hidden_sizes) - 1:
    out_features : Int ← hidden_sizes[i]
    
    # Create monotonic linear layer
    W_raw : Matrix[out_features, in_features] ← random_normal(0, 0.1)
    b : Vector[out_features] ← zeros(out_features)
    layers.append(MonotonicLinear(W_raw, b))
    
    # Add softplus activation
    layers.append(SoftplusActivation())
    
    in_features ← out_features
  
  # Output layer (no activation)
  W_raw_out : Matrix[1, in_features] ← random_normal(0, 0.1)
  b_out : Vector[1] ← zeros(1)
  layers.append(MonotonicLinear(W_raw_out, b_out))
  
  RETURN Network(layers)
```

```
SUBROUTINE: APPLY_MONOTONIC_LINEAR
──────────────────────────────────
INPUT:
  W_raw : Matrix[m, n]               # Unconstrained weight parameters
  b     : Vector[m]                  # Bias vector
  x     : Vector[n]                  # Input vector
OUTPUT:
  y     : Vector[m]                  # Output vector

  # Enforce non-negative weights via softplus
  W : Matrix[m, n] ← log(1 + exp(W_raw))
  
  # Linear transformation
  y : Vector[m] ← W @ x + b
  RETURN y
```

```
SUBROUTINE: APPLY_SOFTPLUS
──────────────────────────
INPUT:
  x : Vector[n]                      # Input vector
OUTPUT:
  y : Vector[n]                      # Output vector

  FOR i : Int = 0 TO n - 1:
    y[i] : Scalar ← log(1 + exp(x[i]))
  RETURN y
```

```
SUBROUTINE: EVALUATE_NETWORK
────────────────────────────
INPUT:
  network : Network                  # Monotonic neural network
  s       : Vector[N]                # Input states
OUTPUT:
  values  : Vector[N]                # Output values

  x : Matrix[N, 1] ← reshape(s, (N, 1))
  
  FOR layer IN network.layers:
    IF layer IS MonotonicLinear:
      x ← APPLY_MONOTONIC_LINEAR(layer.W_raw, layer.b, x)
    ELSE IF layer IS SoftplusActivation:
      x ← APPLY_SOFTPLUS(x)
  
  values : Vector[N] ← reshape(x, (N,))
  RETURN values
```

```
SUBROUTINE: SAMPLE_STATES
─────────────────────────
INPUT:
  N     : Int                        # Number of samples
  s_min : Scalar                     # Minimum state
  s_max : Scalar                     # Maximum state
OUTPUT:
  s     : Vector[N]                  # Sampled states

  FOR i : Int = 0 TO N - 1:
    u : Scalar ← random_uniform(0, 1)
    s[i] : Scalar ← s_min + u * (s_max - s_min)
  RETURN s
```

```
SUBROUTINE: COMPUTE_REWARD
──────────────────────────
INPUT:
  s      : Vector[N]                 # States
  action : Action                    # Action (0 or 1)
  β      : Scalar                    # Reward coefficient
OUTPUT:
  r      : Vector[N]                 # Rewards

  FOR i : Int = 0 TO N - 1:
    r[i] : Scalar ← β * log(1 + s[i]) - action
  RETURN r
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
──────────────────────────────
INPUT:
  s      : Vector[N]                 # Current states
  action : Action                    # Action (0 or 1)
  γ      : Scalar                    # Decay rate
OUTPUT:
  s_next : Vector[N]                 # Next states

  FOR i : Int = 0 TO N - 1:
    s_next[i] : Scalar ← (1 - γ) * s[i] + action
  RETURN s_next
```

```
SUBROUTINE: COMPUTE_INTEGRATED_VALUE
────────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # States
OUTPUT:
  V_bar  : Vector[N]                 # Integrated value

  v₀ : Vector[N] ← EVALUATE_NETWORK(v₀_net, s)
  v₁ : Vector[N] ← EVALUATE_NETWORK(v₁_net, s)
  
  FOR i : Int = 0 TO N - 1:
    v_max : Scalar ← max(v₀[i], v₁[i])
    V_bar[i] : Scalar ← v_max + log(exp(v₀[i] - v_max) + exp(v₁[i] - v_max))
  RETURN V_bar
```

```
SUBROUTINE: COMPUTE_BELLMAN_TARGETS
───────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # Current states
  β      : Scalar                    # Reward coefficient
  γ      : Scalar                    # Decay rate
  δ      : Scalar                    # Discount factor
OUTPUT:
  target₀ : Vector[N]                # Target for action 0
  target₁ : Vector[N]                # Target for action 1

  # Compute targets for action 0 (no clamping - unbounded state space)
  r₀ : Vector[N] ← COMPUTE_REWARD(s, action=0, β)
  s_next₀ : Vector[N] ← COMPUTE_NEXT_STATE(s, action=0, γ)
  V_next₀ : Vector[N] ← COMPUTE_INTEGRATED_VALUE(v₀_net, v₁_net, s_next₀)
  
  FOR i : Int = 0 TO N - 1:
    target₀[i] : Scalar ← r₀[i] + δ * V_next₀[i]
  
  # Compute targets for action 1 (no clamping - unbounded state space)
  r₁ : Vector[N] ← COMPUTE_REWARD(s, action=1, β)
  s_next₁ : Vector[N] ← COMPUTE_NEXT_STATE(s, action=1, γ)
  V_next₁ : Vector[N] ← COMPUTE_INTEGRATED_VALUE(v₀_net, v₁_net, s_next₁)
  
  FOR i : Int = 0 TO N - 1:
    target₁[i] : Scalar ← r₁[i] + δ * V_next₁[i]
  
  RETURN (target₀, target₁)
```

```
SUBROUTINE: COMPUTE_BELLMAN_LOSS
────────────────────────────────
INPUT:
  v₀_pred : Vector[N]                # Predicted values for action 0
  v₁_pred : Vector[N]                # Predicted values for action 1
  target₀ : Vector[N]                # Target values for action 0
  target₁ : Vector[N]                # Target values for action 1
OUTPUT:
  loss    : Scalar                   # Mean squared Bellman residual

  sum_sq : Scalar ← 0.0
  
  FOR i : Int = 0 TO N - 1:
    error₀ : Scalar ← v₀_pred[i] - target₀[i]
    error₁ : Scalar ← v₁_pred[i] - target₁[i]
    sum_sq ← sum_sq + error₀ * error₀ + error₁ * error₁
  
  loss : Scalar ← sum_sq / (2 * N)
  RETURN loss
```

```
SUBROUTINE: UPDATE_NETWORK_WEIGHTS
──────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  loss   : Scalar                    # Loss value
  α      : Scalar                    # Learning rate
OUTPUT:
  (none, updates networks in place)

  # Compute gradients via backpropagation
  grad₀ : List[Matrix] ← BACKPROPAGATE(v₀_net, loss)
  grad₁ : List[Matrix] ← BACKPROPAGATE(v₁_net, loss)
  
  # Update weights for network 0
  FOR j : Int = 0 TO length(v₀_net.layers) - 1:
    IF v₀_net.layers[j] IS MonotonicLinear:
      v₀_net.layers[j].W_raw ← v₀_net.layers[j].W_raw - α * grad₀[j].W
      v₀_net.layers[j].b ← v₀_net.layers[j].b - α * grad₀[j].b
  
  # Update weights for network 1
  FOR j : Int = 0 TO length(v₁_net.layers) - 1:
    IF v₁_net.layers[j] IS MonotonicLinear:
      v₁_net.layers[j].W_raw ← v₁_net.layers[j].W_raw - α * grad₁[j].W
      v₁_net.layers[j].b ← v₁_net.layers[j].b - α * grad₁[j].b
```

```
SUBROUTINE: CHECK_CONVERGENCE
─────────────────────────────
INPUT:
  loss : Scalar                      # Current loss
  ε    : Scalar                      # Tolerance
OUTPUT:
  converged : Bool                   # True if converged

  rmse : Scalar ← sqrt(loss)
  converged : Bool ← (rmse < ε)
  RETURN converged
```

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
──────────────────────────────────────
INPUT:
  v₀_net : Network                   # Network for action 0
  v₁_net : Network                   # Network for action 1
  s      : Vector[N]                 # States
OUTPUT:
  prob₀  : Vector[N]                 # P(a=0|s)
  prob₁  : Vector[N]                 # P(a=1|s)

  v₀ : Vector[N] ← EVALUATE_NETWORK(v₀_net, s)
  v₁ : Vector[N] ← EVALUATE_NETWORK(v₁_net, s)
  
  FOR i : Int = 0 TO N - 1:
    # Numerically stable softmax
    v_max : Scalar ← max(v₀[i], v₁[i])
    exp₀ : Scalar ← exp(v₀[i] - v_max)
    exp₁ : Scalar ← exp(v₁[i] - v_max)
    sum_exp : Scalar ← exp₀ + exp₁
    
    prob₀[i] : Scalar ← exp₀ / sum_exp
    prob₁[i] : Scalar ← exp₁ / sum_exp
  
  RETURN (prob₀, prob₁)
```

### Convergence

The algorithm minimizes the Bellman residual, driving the neural network approximation toward the true fixed point. Convergence is monitored via the root mean squared Bellman error $\sqrt{\mathcal{L}^{(k)}}$.

Key properties:

- **Continuous approximation**: No grid discretization or interpolation required
- **Scalability**: Extends naturally to high-dimensional state spaces
- **Economic structure**: Monotonicity enforced architecturally via non-negative weights

## Implementation

```{python}
import torch
import numpy as np
import matplotlib.pyplot as plt

# Add project root to path
import sys
sys.path.insert(0, "../../src")

from mdp_solver import solve_value_function, compute_choice_probability, evaluate_network, compute_integrated_value
```

### Model Parameters

```{python}
# Model parameters
beta = 0.1      # Reward coefficient (diminishing returns)
gamma = 0.1     # State decay rate
delta = 0.95    # Discount factor

# State space bounds
s_min = 0.0
s_max = 10.0

# Neural network architecture
hidden_sizes = [32, 32]

# Training parameters
learning_rate = 0.01
batch_size = 128
tolerance = 1e-4
max_iterations = 2000

print(f"Model: u(s,a) = {beta} * log(1+s) - a")
print(f"Transition: s' = {1-gamma:.1f} * s + a")
print(f"Discount factor: δ = {delta}")
```

### Solve the MDP

```{python}
#| cache: true
#| label: solve-baseline

# Set random seed for reproducibility
torch.manual_seed(42)

# Solve for value functions
v0_net, v1_net, losses, n_iter = solve_value_function(
    beta=beta,
    gamma=gamma,
    delta=delta,
    s_min=s_min,
    s_max=s_max,
    hidden_sizes=hidden_sizes,
    learning_rate=learning_rate,
    batch_size=batch_size,
    tolerance=tolerance,
    max_iterations=max_iterations,
)

print(f"Converged in {n_iter} iterations")
print(f"Final RMSE: {losses[-1]**0.5:.2e}")
```

### Results

#### Convergence Plot

```{python}
#| fig-cap: "Convergence of the Bellman residual over training iterations"

plt.figure(figsize=(10, 4))
plt.semilogy(losses, 'b-', linewidth=0.5)
plt.xlabel('Iteration')
plt.ylabel('MSE Loss')
plt.title('Training Convergence')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

#### Value Functions

```{python}
#| fig-cap: "Choice-specific value functions v(s, a) for each action"

# Create evaluation grid
s_eval = torch.linspace(s_min, s_max, 200)

# Evaluate value functions
with torch.no_grad():
    v0_values = evaluate_network(v0_net, s_eval).numpy()
    v1_values = evaluate_network(v1_net, s_eval).numpy()
    V_bar = compute_integrated_value(v0_net, v1_net, s_eval).numpy()

s_np = s_eval.numpy()

plt.figure(figsize=(10, 5))
plt.plot(s_np, v0_values, 'b-', label='v(s, 0) - No investment', linewidth=2)
plt.plot(s_np, v1_values, 'r-', label='v(s, 1) - Invest', linewidth=2)
plt.plot(s_np, V_bar, 'k--', label='V̄(s) - Integrated value', linewidth=2)
plt.xlabel('State s')
plt.ylabel('Value')
plt.title('Choice-Specific and Integrated Value Functions')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

#### Choice Probabilities

```{python}
#| fig-cap: "Probability of choosing each action as a function of state"

# Compute choice probabilities
with torch.no_grad():
    prob_a0, prob_a1 = compute_choice_probability(v0_net, v1_net, s_eval)
    prob_a0 = prob_a0.numpy()
    prob_a1 = prob_a1.numpy()

plt.figure(figsize=(10, 5))
plt.plot(s_np, prob_a0, 'b-', label='P(a=0|s) - No investment', linewidth=2)
plt.plot(s_np, prob_a1, 'r-', label='P(a=1|s) - Invest', linewidth=2)
plt.xlabel('State s')
plt.ylabel('Probability')
plt.title('Choice Probabilities')
plt.ylim(-0.05, 1.05)
plt.legend()
plt.grid(True, alpha=0.3)
plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
```

#### Value Difference

```{python}
#| fig-cap: "Value difference v(s,1) - v(s,0) determines the choice probability"

plt.figure(figsize=(10, 5))
value_diff = v1_values - v0_values
plt.plot(s_np, value_diff, 'm-', linewidth=2)
plt.xlabel('State s')
plt.ylabel('v(s, 1) - v(s, 0)')
plt.title('Value Difference: Benefit of Investment')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
```

### Interpretation

```{python}
# Find threshold state where P(a=1) ≈ 0.5
threshold_idx = np.argmin(np.abs(prob_a1 - 0.5))
threshold_state = s_np[threshold_idx]

print(f"Policy Interpretation (β={beta}, γ={gamma}, δ={delta}):")
print(f"- At state s ≈ {threshold_state:.2f}, the agent is indifferent between actions")
print(f"- For s < {threshold_state:.2f}, investment (a=1) is more likely")
print(f"- For s > {threshold_state:.2f}, no investment (a=0) is more likely")
print(f"\nThis reflects the trade-off:")
print(f"- Low state: Investment pays off (builds future value)")
print(f"- High state: Already earning good rewards, investment cost outweighs benefit")
```

## Comparative Statics: Effect of β on Value Functions

This section examines how the value functions change as we vary the reward coefficient β from 0 to 2.

```{python}
#| label: comparative-statics-setup

import matplotlib.colors as mcolors

def solve_and_extract_values(beta_val):
    """Solve MDP and extract value functions for given beta."""
    torch.manual_seed(42)
    v0_net_cs, v1_net_cs, _, _ = solve_value_function(
        beta=beta_val,
        gamma=gamma,
        delta=delta,
        s_min=s_min,
        s_max=s_max,
        hidden_sizes=hidden_sizes,
        learning_rate=learning_rate,
        batch_size=batch_size,
        tolerance=tolerance,
        max_iterations=max_iterations,
    )
    with torch.no_grad():
        v0_vals = evaluate_network(v0_net_cs, s_eval).numpy()
        v1_vals = evaluate_network(v1_net_cs, s_eval).numpy()
    return v0_vals, v1_vals

# Beta values from 0 to 2 in steps of 0.25
beta_values = np.arange(0, 2.25, 0.25)
n_beta = len(beta_values)

# Solve for all beta values
print("Solving for different β values...")
results_beta = {}
for b in beta_values:
    v0_vals, v1_vals = solve_and_extract_values(b)
    results_beta[b] = {'v0': v0_vals, 'v1': v1_vals}
    print(f"  β = {b:.2f} done")
print("Complete!")

# Define color gradients
cmap_blue = mcolors.LinearSegmentedColormap.from_list('black_blue', ['black', 'blue'])
cmap_red = mcolors.LinearSegmentedColormap.from_list('black_red', ['black', 'red'])
```

### Value Functions by Action

```{python}
#| fig-cap: "Value functions v(s,a) as β varies from 0 to 2. Left: a=0 (black→blue). Right: a=1 (black→red)."

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot v(s, 0) - No investment (black to blue)
ax0 = axes[0]
for i, b in enumerate(beta_values):
    color = cmap_blue(i / (n_beta - 1))
    ax0.plot(s_np, results_beta[b]['v0'], color=color, linewidth=1.5, alpha=0.8)
ax0.set_xlabel('State s')
ax0.set_ylabel('v(s, 0)')
ax0.set_title('Value Function: No Investment (a=0)')
ax0.grid(True, alpha=0.3)

# Add colorbar
sm0 = plt.cm.ScalarMappable(cmap=cmap_blue, norm=plt.Normalize(vmin=0, vmax=2))
sm0.set_array([])
cbar0 = plt.colorbar(sm0, ax=ax0)
cbar0.set_label('β')

# Plot v(s, 1) - Investment (black to red)
ax1 = axes[1]
for i, b in enumerate(beta_values):
    color = cmap_red(i / (n_beta - 1))
    ax1.plot(s_np, results_beta[b]['v1'], color=color, linewidth=1.5, alpha=0.8)
ax1.set_xlabel('State s')
ax1.set_ylabel('v(s, 1)')
ax1.set_title('Value Function: Investment (a=1)')
ax1.grid(True, alpha=0.3)

# Add colorbar
sm1 = plt.cm.ScalarMappable(cmap=cmap_red, norm=plt.Normalize(vmin=0, vmax=2))
sm1.set_array([])
cbar1 = plt.colorbar(sm1, ax=ax1)
cbar1.set_label('β')

plt.tight_layout()
plt.show()
```

### Summary

```{python}
print("Comparative Statics Summary: Effect of β on Value Functions")
print("=" * 60)
print(f"\nFixed parameters: γ={gamma}, δ={delta}")
print(f"β varied from 0 to 2 in steps of 0.25")
print(f"\nKey observations:")
print("• As β increases, both value functions increase (higher rewards)")
print("• v(s,0) and v(s,1) both become steeper (more sensitive to state)")
print("• At β=0, values are purely from discounted future option value")
print("• At high β, current-period rewards dominate the value functions")
```

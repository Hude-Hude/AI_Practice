---
title: "Monte Carlo Simulation for MDP"
subtitle: "Simulator and Structural Parameter Estimation"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

## Setup

```{python}
#| label: setup

import sys
from pathlib import Path

import numpy as np
import torch

# Add project paths
sys.path.insert(0, "../../src")
sys.path.insert(0, "../config_mdp")
sys.path.insert(0, "../mdp_utils")

# Import MDP solver
from mdp_solver import (
    build_monotonic_network,
    evaluate_network,
    compute_choice_probability,
    compute_integrated_value,
)

# Import MDP simulator
from mdp_simulator import simulate_mdp_panel

# Import configuration
import config

# Import plotting utilities
from plotting import (
    plot_convergence,
    plot_value_functions,
    plot_choice_probabilities,
    plot_value_difference,
    plot_bellman_residuals,
    plot_state_evolution,
    plot_trajectories,
    plot_choice_validation,
    plot_calibration,
    plot_state_transitions,
    plot_state_distribution,
    plot_reward_distribution,
)

print("Modules loaded successfully")
```

## Load Configuration and Trained Model

```{python}
#| label: load-model

# Load parameters from central config
beta = config.beta
gamma = config.gamma
delta = config.delta
s_min = config.s_min
s_max = config.s_max
hidden_sizes = config.hidden_sizes

print("Model parameters (from scripts/config_mdp/config.py):")
print(f"  beta = {beta}")
print(f"  gamma = {gamma}")
print(f"  delta = {delta}")
print(f"  s_min = {s_min}")
print(f"  s_max = {s_max}")
print(f"  hidden_sizes = {hidden_sizes}")

# Load trained networks from solver output
output_dir = Path("../../output/solve_mdp")

v0_net = build_monotonic_network(hidden_sizes=hidden_sizes)
v1_net = build_monotonic_network(hidden_sizes=hidden_sizes)

v0_net.load_state_dict(torch.load(output_dir / "v0_net.pt", weights_only=True))
v1_net.load_state_dict(torch.load(output_dir / "v1_net.pt", weights_only=True))

v0_net.eval()
v1_net.eval()

# Load training losses
losses = np.load(output_dir / "losses.npy")

print(f"\nLoaded trained networks from {output_dir.resolve()}")
print(f"  Training iterations: {len(losses)}")
print(f"  Final RMSE: {losses[-1]**0.5:.2e}")
```

## Simulation Algorithm

### Overview

The Monte Carlo simulator generates panel data by forward-simulating agents who follow the optimal policy derived from the trained value functions. Each period:

1. Agent observes current state $s_t$
2. Agent draws action from the closed-form logit choice probability:
   $$a_t \sim \text{Bernoulli}\left( P(a=1|s_t) \right) \quad \text{where} \quad P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
3. State transitions: $s_{t+1} = (1 - \gamma) s_t + a_t$

Since Type-I Extreme Value shocks yield closed-form logit probabilities, we use the analytical choice probabilities directly rather than drawing shocks.

### Type Definitions

```
TYPE DEFINITIONS
────────────────
Scalar      = Float                      # Single real number
Vector[n]   = Array[Float, n]            # 1D array of n floats
Matrix[m,n] = Array[Float, m, n]         # 2D array of shape (m, n)
Action      = Int ∈ {0, 1}               # Binary action
Network     = MonotonicNeuralNetwork     # Trained value function network
```

### Main Algorithm

```
ALGORITHM: SIMULATE_MDP_PANEL
─────────────────────────────
INPUT:
  v₀_net       : Network       # Trained network for action 0
  v₁_net       : Network       # Trained network for action 1
  n_agents     : Int           # Number of agents
  n_periods    : Int           # Number of time periods
  s_init       : Vector[n] | Scalar  # Initial states
  β            : Scalar        # Reward coefficient
  γ            : Scalar        # State decay rate
  seed         : Int           # Random seed for reproducibility

OUTPUT:
  states       : Matrix[n, T]  # State history for each agent
  actions      : Matrix[n, T]  # Action history for each agent
  rewards      : Matrix[n, T]  # Reward history for each agent

PROCEDURE:
  SET_SEED(seed)
  
  # Initialize storage matrices
  states  ← Matrix[n_agents, n_periods]
  actions ← Matrix[n_agents, n_periods]
  rewards ← Matrix[n_agents, n_periods]
  
  # Set initial states
  states[*, 0] ← s_init
  
  FOR t = 0 TO n_periods - 1:
    # Get current states for all agents
    s_t ← states[*, t]
    
    # Compute choice probabilities using closed-form logit formula
    _, p₁ ← COMPUTE_CHOICE_PROBABILITY(v₀_net, v₁_net, s_t)
    
    # Draw actions from Bernoulli distribution
    u ← UNIFORM(0, 1, n_agents)
    actions[*, t] ← (u < p₁) ? 1 : 0
    
    # Compute flow rewards
    rewards[*, t] ← COMPUTE_REWARD(s_t, actions[*, t], β)
    
    # State transition (if not last period)
    IF t < n_periods - 1:
      states[*, t+1] ← COMPUTE_NEXT_STATE(s_t, actions[*, t], γ)
  
  RETURN (states, actions, rewards)
```

### Subroutines

```
SUBROUTINE: COMPUTE_CHOICE_PROBABILITY
──────────────────────────────────────
INPUT:
  v₀_net : Network           # Trained network for action 0
  v₁_net : Network           # Trained network for action 1
  s      : Vector[n]         # States
OUTPUT:
  p₀     : Vector[n]         # P(a=0|s)
  p₁     : Vector[n]         # P(a=1|s)

  # Evaluate value functions
  v₀ ← EVALUATE_NETWORK(v₀_net, s)
  v₁ ← EVALUATE_NETWORK(v₁_net, s)
  
  # Closed-form logit probabilities (from Type-I EV distribution)
  # Using numerically stable softmax
  v_max ← MAX(v₀, v₁)
  exp₀ ← exp(v₀ - v_max)
  exp₁ ← exp(v₁ - v_max)
  sum_exp ← exp₀ + exp₁
  
  p₀ ← exp₀ / sum_exp
  p₁ ← exp₁ / sum_exp
  
  RETURN (p₀, p₁)
```

```
SUBROUTINE: COMPUTE_REWARD
──────────────────────────
INPUT:
  s      : Vector[n]         # States
  a      : Vector[n]         # Actions (0 or 1)
  β      : Scalar            # Reward coefficient
OUTPUT:
  r      : Vector[n]         # Flow rewards

  FOR i = 0 TO n - 1:
    r[i] ← β * log(1 + s[i]) - a[i]
  RETURN r
```

```
SUBROUTINE: COMPUTE_NEXT_STATE
──────────────────────────────
INPUT:
  s      : Vector[n]         # Current states
  a      : Vector[n]         # Actions (0 or 1)
  γ      : Scalar            # Decay rate
OUTPUT:
  s'     : Vector[n]         # Next states

  FOR i = 0 TO n - 1:
    s'[i] ← (1 - γ) * s[i] + a[i]
  RETURN s'
```

### Properties

The simulation algorithm satisfies the following properties:

1. **Exactness**: Actions are drawn directly from the closed-form logit probabilities:
   $$P(a=1|s) = \frac{\exp(v(s,1))}{\exp(v(s,0)) + \exp(v(s,1))}$$
   This uses the analytical result from Type-I Extreme Value shocks rather than simulating the shocks.

2. **Ergodicity**: Under the optimal policy, the state process converges to a stationary distribution (given $0 < \gamma < 1$).

3. **Reproducibility**: Given the same seed, the algorithm produces identical panel data.

## Run Simulation

```{python}
#| label: run-simulation

# Simulation parameters
n_agents = 100       # Number of agents (paths)
n_periods = 100      # Number of time periods
simulation_seed = 42 # Random seed

# Randomize initial states: draw from Uniform[s_min, s_max]
np.random.seed(simulation_seed)
s_init = np.random.uniform(s_min, s_max, size=n_agents)

print(f"Initial state distribution:")
print(f"  Distribution: Uniform[{s_min}, {s_max}]")
print(f"  Mean: {s_init.mean():.2f}")
print(f"  Std:  {s_init.std():.2f}")
print(f"  Min:  {s_init.min():.2f}")
print(f"  Max:  {s_init.max():.2f}")

# Run simulation using the implemented algorithm
panel = simulate_mdp_panel(
    v0_net=v0_net,
    v1_net=v1_net,
    n_agents=n_agents,
    n_periods=n_periods,
    s_init=s_init,  # Vector of different initial states per agent
    beta=beta,
    gamma=gamma,
    seed=simulation_seed,
)

print(f"\nSimulation complete:")
print(f"  Agents: {panel.n_agents}")
print(f"  Periods: {panel.n_periods}")
print(f"  Total observations: {panel.n_agents * panel.n_periods:,}")
```

## Simulation Results

### Summary Statistics

```{python}
#| label: summary-stats

print("=" * 60)
print("PANEL DATA SUMMARY STATISTICS")
print("=" * 60)

# State statistics
print("\nState Distribution:")
print(f"  Mean:   {panel.states.mean():.3f}")
print(f"  Std:    {panel.states.std():.3f}")
print(f"  Min:    {panel.states.min():.3f}")
print(f"  Max:    {panel.states.max():.3f}")

# Action statistics
action_rate = panel.actions.mean()
print(f"\nAction Statistics:")
print(f"  P(a=1): {action_rate:.3f} (investment rate)")
print(f"  P(a=0): {1 - action_rate:.3f} (no investment rate)")

# Reward statistics
print(f"\nReward Distribution:")
print(f"  Mean:   {panel.rewards.mean():.3f}")
print(f"  Std:    {panel.rewards.std():.3f}")
print(f"  Min:    {panel.rewards.min():.3f}")
print(f"  Max:    {panel.rewards.max():.3f}")

# Steady state analysis
print(f"\nSteady State Analysis (last 10 periods):")
late_states = panel.states[:, -10:]
late_actions = panel.actions[:, -10:]
print(f"  Mean state:  {late_states.mean():.3f}")
print(f"  P(a=1):      {late_actions.mean():.3f}")

# Note: Total rewards and final states statistics are reported 
# in dedicated sections below with histograms
```

### State Evolution

```{python}
#| label: plot-state-evolution
#| fig-cap: "Evolution of state distribution over time"

plot_state_evolution(panel.states, panel.actions, s_init.mean())
```

### Sample Trajectories

```{python}
#| label: plot-trajectories
#| fig-cap: "Sample agent trajectories showing state evolution"

plot_trajectories(panel.states, s_init.mean(), n_samples=10)
```

### Choice Probability Validation

```{python}
#| label: plot-choice-validation
#| fig-cap: "Comparison of simulated choice frequencies with theoretical choice probabilities"

# Flatten data for plotting
state_flat = panel.states.flatten()
action_flat = panel.actions.flatten()

# Compute theoretical probabilities
s_theory = torch.linspace(state_flat.min(), state_flat.max(), 200)
with torch.no_grad():
    _, prob_a1_theory = compute_choice_probability(v0_net, v1_net, s_theory)
    prob_a1_theory = prob_a1_theory.numpy()

rmse = plot_choice_validation(
    state_flat, action_flat, s_theory.numpy(), prob_a1_theory
)
```

### Choice Probability Calibration

```{python}
#| label: plot-calibration
#| fig-cap: "Calibration plot: empirical vs theoretical choice probabilities"

# Compute theoretical P(a=1|s) for all observations
s_all = torch.tensor(state_flat, dtype=torch.float32)
with torch.no_grad():
    _, p1_theory_all = compute_choice_probability(v0_net, v1_net, s_all)
    p1_theory_all = p1_theory_all.numpy()

cal_rmse = plot_calibration(action_flat, p1_theory_all, n_bins=50)
```

### State Transition Dynamics

```{python}
#| label: plot-transitions
#| fig-cap: "State transition dynamics: current state vs next state by action"

plot_state_transitions(panel.states, panel.actions, gamma)
```

### State Distribution (Initial vs Final)

```{python}
#| label: plot-state-distribution
#| fig-cap: "Distribution of states: initial (t=0) vs final (t=T-1)"

final_states = panel.states[:, -1]
plot_state_distribution(s_init, final_states, s_min, s_max, n_periods)
```

### Reward Distribution (Cumulative vs Discounted)

```{python}
#| label: plot-reward-distribution
#| fig-cap: "Distribution of total rewards: cumulative vs discounted"

cumulative_rewards, discounted_rewards = plot_reward_distribution(panel.rewards, delta)
```

## Summary

```{python}
#| label: summary

print("=" * 60)
print("SIMULATION SUMMARY")
print("=" * 60)

print(f"\nModel Parameters:")
print(f"  β = {beta} (reward coefficient)")
print(f"  γ = {gamma} (state decay rate)")
print(f"  δ = {delta} (discount factor)")

print(f"\nSimulation Setup:")
print(f"  Agents: {panel.n_agents}")
print(f"  Periods: {panel.n_periods}")
print(f"  Initial states: Uniform[{s_min}, {s_max}], mean={s_init.mean():.1f}")
print(f"  Random seed: {simulation_seed}")

print(f"\nKey Results:")
print(f"  Overall investment rate: {panel.actions.mean():.1%}")
print(f"  Stationary mean state: {panel.states[:, -20:].mean():.2f}")
print(f"  Choice probability RMSE: {rmse:.4f}")

print(f"\nThe simulation validates that:")
print(f"  ✓ State transitions follow s' = (1-γ)s + a")
print(f"  ✓ Empirical choice frequencies match theoretical logit probabilities")
print(f"  ✓ State distribution converges to a stationary distribution")
```
